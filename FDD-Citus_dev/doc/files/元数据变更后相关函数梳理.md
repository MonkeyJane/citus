## 一 、UDF函数兼容性

**本节测试用例：**

```sql
--在主CN、cn_tp以及cn_rtap上都执行：
select * from citus_worker_stat_activity;
select * from citus_dist_stat_activity;
select * from citus_lock_waits;
select * from get_global_active_transactions();
select * from get_all_active_transactions();
```

### 1.1、citus_worker_stat_activity 

**函数功能**：显示对worker的查询，包括针对各个分片查询，在不同类型的CN上执行，采集到的节点不同：

cn_rtap节点：通过采集本节点以及所有secondary worker节点数据，获取所有对worker的查询

CN节点：收集本节点、cn_tp、cn_rtap、primary worker节点数据，获取所有对worker的查询

cn_tp、cn_mx节点：收集本节点、cn_tp、cn_rtap、primary worker节点数据，获取所有对worker的查询

**结论：**需要新增适配修改

**适配修改方式**：

```c
static List *
CitusStatActivity(const char *statQuery)
{
	List *citusStatsList = NIL;
#ifdef FDD
	/* 因为只有cn_rtap节点会和secondary节点通信，所以在cn_rtap节点，需要获取到所有secondary worker节点和本机的查询情况。
	 * 而CN和cn_tp节点，也希望获取本机以及其他CN节点的统计情况*/
	List *workerNodeList = (cn_rtap_role == FDD_GetLocalNodeRole()) ? FDD_ActiveSecondaryNodeList() : FDD_ActivePrimaryAndCNNodeList();
#else	
	List *workerNodeList = ActivePrimaryNodeList();
#endif
。。。。。。
}
```

### 1.2、citus_dist_stat_activity

**函数功能**：显示在所有节点上执行的分布式查询，在不同类型的CN上执行，采集到的节点不同：

cn_rtap节点：会采集本节点统计数据，以及所有secondary worker节点的统计数据

CN节点：收集所有CN、cn_tp、cn_rtap、primary worker节点的分布式 查询

cn_tp、cn_mx节点：收集所有cn_tp、cn_rtap、primary worker节点的分布式 查询

**结论**：需要进行兼容性修改，具体修改参见1.1节

### 1.3、citus_lock_waits； 

这是一个视图，用于获取当前节点的锁等待关系，需要待分析

**结论：**待讨论和分析

### 1.4、get_global_active_transactions 

**函数功能**：从群集的每个节点中，分别采集所有活动后端的所有可用信息，然后统一显示出来，而由于CN的类型不一样，和其连接的worker也不一样，所以调用此函数时，涉及的节点也不一样：

cn_rtap节点：会采集本节点事务信息，以及所有secondary worker节点的事务信息

CN节点：收集所有CN、cn_tp、cn_rtap、primary worker节点的事务信息

cn_tp、cn_mx节点：收集所有cn_tp、cn_rtap、primary worker节点的事务信息

**结论**：需要进行兼容性修改

**适配修改方式**：

```c
Datum get_global_active_transactions(PG_FUNCTION_ARGS)
{
	ReturnSetInfo *returnSetInfo = (ReturnSetInfo *) fcinfo->resultinfo;
	TupleDesc tupleDescriptor = NULL;
	Tuplestorestate *tupleStore = NULL;
	MemoryContext perQueryContext = NULL;
	MemoryContext oldContext = NULL;
#ifdef FDD	
	/* 因为只有cn_rtap节点会和secondary节点通信，所以在cn_rtap节点，需要获取到所有secondary worker节点和本机的后端进程的事务信息。
	 * 而CN和cn_tp节点，也希望获取本机以及其他CN节点的后端进程事务信息*/
	List *workerNodeList = (cn_rtap_role == FDD_GetLocalNodeRole()) ? FDD_ActiveSecondaryNodeList() : FDD_ActivePrimaryAndCNNodeList();
#else
	List *workerNodeList = ActivePrimaryNodeList();
#endif
	。。。。。。
}
```

### 1.5、get_all_active_transactions 

该函数是用于获取本节点的活动的事务，不需要修改

**结论：**不需要进行兼容性修改

## 二  函数调用排查

### 2.1 ActivePrimaryNodeList

#### 1、 AcquireDistributedLockOnRelations <- 

​         LockTruncatedRelationMetadataInWorkers  <-ProcessTruncateStatement  

**结论**：需要增加兼容性

**修改原因**：只会在TRUNCATE TABLE时触发，并且清空的表是在其他CN上有元数据的表，也就是带流复制的hash表和参考表，所以需要在所有其他CN上也加锁

```c
//原因：注意该函数的注释：
/*Notice that the locking functions is sent to all workers regardless of if it has metadata or not. This is because a worker node only knows itself and previous workers that has metadata sync turned on. The node does not know about other nodes that have metadata sync turned on afterwards.*/
```

表明是发送到所有work，因为不知道对应worker，在该表创建时是否是一个MX，然后后面又移除了，所以加锁用的字段是“LOCK_RELATION_IF_EXISTS”，也就是当前节点，存在该表的relationId就加锁，不管是不是以前创建的。

**修改方式为**：

```c
static void
AcquireDistributedLockOnRelations(List *relationIdList, LOCKMODE lockMode)
{
	ListCell *relationIdCell = NULL;
#ifdef FDD
	/* 只会在TRUNCATE TABLE时触发，并且清空的表是在其他CN上有元数据的表，也就是带流复制的hash表和参考表，
	 * 所以需要在所有其他CN上也加锁*/
	List *workerNodeList = FDD_ActivePrimaryAndCNNodeList();
#else
	List *workerNodeList = ActivePrimaryNodeList();
#endif
	。。。。。。
}
```

**测试用例**：

```sql
--用例：tabl2是带流复制标志的hash分布表、tbl3是复制表
TRUNCATE TABLE tbl2;
TRUNCATE TABLE tbl3;
```

#### 2、ActivePrimaryNodeCount  <-

​	 master_create_empty_shard  （不需增加兼容性）
​	CollectBasicUsageStatistics CitusMaintenanceDaemonMain（后续需要新增集群信息采集）

- master_create_empty_shard  （UDF）函数：

  不需要增加兼容性，因为该函数是给append类型的表，新增一个空的shard，由于该类型的表是不会同步到其他CN的，并且在此处，他也是为了获取集群中的所有活动worker节点，以便决定新增加的shard放在哪个worker上面

- CollectBasicUsageStatistics 函数

  后续需要新增集群信息采集，该接口是为了便于将集群信息，定时发送到配置的信息采集器

**结论：**不需要增加兼容性

#### 3、broadcast_intermediate_result （UDF）

**结论：**不需要增加兼容性

因为该函数是为了把执行计划的中间结果存储在以分布式事务ID命名的目录中，而此处获取到的worker节点列表是为了保存到文件中，以便知道后续发送data时，要发送到哪

参见函数注释：execute a query and write its results to an result file on all workers

#### 4、CitusStatActivity 

​	4.1 citus_dist_stat_activity （UDF）：显示在所有节点上执行的分布式查询
​	4.2 citus_worker_stat_activity （UDF）：显示对worker的Query语句，包括针对各个分片Query语句

**结论**：需要增加兼容性

因为只有cn_rtap节点会和secondary节点通信，所以在cn_rtap节点，需要获取到所有secondary worker节点和本机的查询情况。

而CN和cn_tp节点，也希望获取本机以及其他CN节点的统计情况

修改方式参见1.1节

#### 5、 CreateReferenceTableColocationId <-  

​	ColocationIdForNewTable  <- CreateDistributedTable （不需要新增兼容性）
​	 ConvertToReferenceTableMetadata <- ReplicateSingleShardTableToAllWorkers <-upgrade_to_reference_table （UDF）  （不需要新增兼容性）

**CreateReferenceTableColocationId 函数功能**：用于为复制表创建一个新的co-location id并将其写入pg_dist_colocation，然后返回新生成的co-location id，由于对于所有类型的引用表只能有一个亲和组，如果已经为引用表创建了co-location id，则此函数将返回它而不创建任何内容。

而对于复制表，决定一个co-location id，只由所有活的worker primary节点的个数决定，和shardCount（固定为1）、distributionColumnType（固定为InvalidOid）无关，所以不需要为新增的CN新增兼容性

**结论：**不需要新增兼容性

#### 6、CreateReferenceTableShard <-

​        CreateDistributedTable

**CreateReferenceTableShard 函数功能**：用于创建一个参考表，而此处ActivePrimaryNodeList函数是用于查询所有活的worker primary节点，以便决定需要往哪些节点上插入参考表

**结论：**不需要新增兼容性

#### 7、CreateShardsWithRoundRobinPolicy  <-

​        master_create_worker_shards（官网建议用 [create_distributed_table](http://docs.citusdata.com/en/v8.1/develop/api_udf.html?highlight=master_create_worker_shards#create-distributed-table) 取代）

​        CreateHashDistributedTableShards <- CreateDistributedTable

**CreateShardsWithRoundRobinPolicy 函数功能**：用于根据指定的初始分片数为给定表创建空的shard，而此处ActivePrimaryNodeList函数是用于查询所有活的worker primary节点，以便决定生成的分片shard，将落在哪些节点上

**结论：**不需要新增兼容性

#### 8、create_reference_table （UDF）

该函数是创建复制表时，调用ActivePrimaryNodeList函数用于查询所有活的worker primary节点个数是否为0，若为0，则表明集群没有可用worker，故创建复制表失败，与新增的两类CN节点存在与否无关

**结论：**不需要新增兼容性

#### 9、EnsureSchemaExistsOnAllNodes <-

​	 CreateHashDistributedTableShards （不需要新增兼容性）
​	 create_reference_table （不需要新增兼容性）
**结论：**不需要新增兼容性

**原因：**如果在创建分布表时，没有指明SCHEMA，那么默认的就是“public”，此时不会发送“CREATE SCHEMA”的SQL到所有活的worker节点；反之，若指定了自定义的SCHEMA，那么在往其他所有活的worker节点创建分布表时，为了保证涉及的worker节点上，创建该SCHEMA下的分布表成功，则先会调用该函数，发送“CREATE SCHEMA IF NOT EXISTS ”的SQL。

同时，在确所涉及的每个worker节点上分片表创建成功后，会同步相应的元数据到所有CN节点，此时也会同样发送“CREATE SCHEMA IF NOT EXISTS ”的SQL到新增的两类CN节点上（参见CreateTableMetadataOnWorkers函数），故此处的函数不需要新增兼容性。

```sql
--测试用例
SET citus.replication_model TO streaming;
set citus.shard_count = 3; 

CREATE SCHEMA IF NOT EXISTS myschema AUTHORIZATION postgres;
create table myschema.tbl7(id int,name int);
SELECT create_distributed_table('myschema.tbl7', 'id');

--查看该myschema下的表
SET search_path TO myschema,public;
```

#### 10、GetLocalNodeCitusDistStat  <- 

​          CitusStatActivity 

**结论：**需要新增兼容性

**原因：**因为该函数是在收集节点Query语句时使用，主要是获取当前节点的分布式查询语句，其中主CN有特殊的途径获取，而cn_tp和cn_rtap的获取，则需要做适配，所以在该函数中，需要查询出所有活的primary worker节点以及所有的cn_rtap和cn_tp节点。

**修改方式为：**

```c
static List *
GetLocalNodeCitusDistStat(const char *statQuery)
{
	。。。。。。
	if (IsCoordinator())
	{
		/*
		 * Coordinator's nodename and nodeport doesn't show-up in the metadata,
		 * so mark it manually as executing from the coordinator.
		 */
		citusStatsList = LocalNodeCitusDistStat(statQuery, coordinator_host_name,
												PostPortNumber);
		return citusStatsList;
	}
	localGroupId = GetLocalGroupId();

	/* get the current worker's node stats */
#ifdef FDD
	workerNodeList = FDD_ActivePrimaryAndCNNodeList();
#else
	workerNodeList = ActivePrimaryNodeList();
#endif
   。。。。。。。
}
```

#### 11、get_global_active_transactions （UDF）

**结论：**需要新增兼容性

具体修改原因和方式，参见1.4节

#### 12、HasMetadataWorkers

**结论：**需要新增兼容性

具体修改原因和方式，参见2.3节	
​											 

#### 13、IsFirstWorkerNode  <-

​          SerializeNonCommutativeWrites 
​	 该函数功能：在给定分片上获取所需的锁，以防止进行并发写入。依据分片类型和执行该操作的节点位置，存在以下限制：

-  如果修改的分片是复制表的分片，并且是MX群集，则需要在第一个worker节点上获取分片资源锁（通过调用LockShardListResourcesOnFirstWorker函数），以防止在参考表的不同节点上去获取锁而引起死锁。

- 如果修改操作是在非第一个worker节点，则通过通过获取对第一个worker上该参考表的锁，来进行顺序的修改操作。如果修改操作恰好是第一个worker节点（即**IsFirstWorkerNode** 函数返回true，并且该节点必定是MX节点），则不使用远程获取锁以避免额外的往返或自死锁，而是终使用LockShardListResources()。

- 另外，如果不是处理MX集群上的复制表，将始终使用LockShardListResources()函数来获取锁。

从上分析可知：

**IsFirstWorkerNode 函数功能：**判断当前节点是否是经过排序后的，所有活的primary worker节点的第一个节点，并且该节点含有元数据（即GetLocalGroupId等于pg_dist_node中的groupId）。

该函数只有SerializeNonCommutativeWrites会调用，以便在进行对参考表的修改操作时，是否在其上使用远程获取锁，因为cn_tp和cn_rtap上不会存在复制表，所以此处不应该增加兼容性修改

**结论：**不需要新增兼容性	

测试方式：		

```sql
--创建复制表tbl5
create table tbl5(id int,name int);
SELECT create_reference_table('tbl5');

--往tbl5插入数据即可触发
insert into tbl5 values(1232,'11'),(2222,'33');
```

​			

#### 14、master_create_empty_shard (udf) 

**函数功能**：为给定的append分布式表创建一个空分片。 该函数首先根据配置项计算分片落在哪些worker上以及顺序，同时更新主CN节点上的元数据，以使此分片可见。 然后，它在worker节点上创建空分片，最后，将分片存在位置（placement）添加到元数据表（**pg_dist_node**）。

在该函数计算分片落在位置时，有两个地方用到了***ActivePrimaryNodeList***函数

第一个位置：在获取所有活的primary worker节点个数的函数ActivePrimaryNodeCount()中会调用，它主要根据副本个数配置参数citus.shard_replication_factor，来决定最终需要新创建的分片个数，当副本个数配置值比集群 活的worker个数小时，会额外多增加一个分片，用作备份，代码中的位置如下：

```c
Datum master_create_empty_shard(PG_FUNCTION_ARGS)
{
	。。。。。。
	/* if enough live groups, add an extra candidate node as backup */
	{
        //获取出所有活的primary worker节点的个数
		uint32 primaryNodeCount = ActivePrimaryNodeCount();
		attemptableNodeCount = ShardReplicationFactor;
		if (primaryNodeCount > ShardReplicationFactor)
		{
			attemptableNodeCount = ShardReplicationFactor + 1;
		}
	}
	。。。。。。
}
```

第二个位置：当配置参数citus.shard_placement_policy配置为‘round-robin’时，在决定新增的某个分片落在那个具体worker节点上时用到，也就是根据(shardId + placementIndex) % workerNodeCount获取索引值，然后从 ***ActivePrimaryNodeList()***获取到的worker节点列表中选择worker。

```c
Datum master_create_empty_shard(PG_FUNCTION_ARGS)
{
	。。。。。。
	/* first retrieve a list of random nodes for shard placements */
	while (candidateNodeIndex < attemptableNodeCount)
	{
		。。。。。。
		else if (ShardPlacementPolicy == SHARD_PLACEMENT_ROUND_ROBIN)
		{
            //获取所有活的primary worker节点列表，决定当前新增分片落在那个worker上
			List *workerNodeList = ActivePrimaryNodeList();
			candidateNode = WorkerGetRoundRobinCandidateNode(workerNodeList, shardId,
															 candidateNodeIndex);
		}
		。。。。。。
	}
	。。。。。。
}
```

综合前面的两点分析可知，此处涉及的都是活的primary worker节点，和新增的cn_rtap、cn_tp节点无关，所以不用新增适配，同时对于ppend分布表，是不需要把元数据同步到cn_rtap、cn_tp节点

**结论：**不需要新增兼容性

#### 15、MultiTaskTrackerExecute 

**函数功能：**在设置为采用task-tracker执行器后，执行具体的SQL时会调用。

在该函数中调用***ActivePrimaryNodeList***的用途是:获取活的primary worker节点列表，然后创建两个哈希表来管理与这些worker节点的连接，其中 第一个哈希表用于管理每个分配的连接并且检查任务状态。 第二个（临时）哈希表协助从每个worker节点获取查询结果到主节点上。

因为此处是为了发送查询计划到每个worker节点，并且从中获取查询结果，所以不涉及新增的cn_rtap、cn_tp节点，故不用新增适配。

**结论：**不需要新增兼容性

#### 16、OpenConnectionsToAllNodes  <- 

​           citus_create_restore_point (udf)

​	   citus_create_restore_point 函数功能：首先暂时阻止写入操作，然后，通过在所有节点上运行pg_create_restore_point，以便在所有节点上创建命名还原点。 当集群除主CN之外没有其他节点进行写入操作的情况下，这会创建一致性还原点。

​	    所以此处的“所有节点”应该包括新增的cn_rtap、cn_tp节点，以便在这些节点上也会创建同样的命名还原点，保证集群还原时，两种新增CN节点，数据也会还原。

​	  OpenConnectionsToAllNodes函数，是唯一被citus_create_restore_point 函数所调用，用于获取集群中所有活的primarily worker节点，并且和每个节点新建一个独占连接，从前面分析可知，此函数应该新增兼容性，需要也和cn_rtap、cn_tp节点新建连接。

**注意：**是否需要强制阻塞其他含有元数据的节点进行写入操作

**结论：**需要新增兼容性

**修改方式为：**

```c
static List * OpenConnectionsToAllNodes(void)
{
	。。。。。。
#ifdef FDD
	//需要往所有primary节点和所有CN节点发送创建命名还原点命令
	workerNodeList = FDD_ActivePrimaryAndCNNodeList();
#else
	workerNodeList = ActivePrimaryNodeList();
#endif
	。。。。。。
}
```

**测试方式：**

```sql
--在主CN上执行
select citus_create_restore_point('foo');
```



#### 17、RecoverTwoPhaseCommits  <-

​	  recover_prepared_transactions (udf)（需要新增兼容性）

​	 CitusMaintenanceDaemonMain（需要新增兼容性）

​         其中：recover_prepared_transactions 函数是一个UDF函数，用于恢复此节点在其他节点上启动的所有待处理的prepared事务。因为cn_rtap节点，只会和secondary节点进行通信，从中获取数据，所以该类型的节点，在调用此函数时，应该是恢复此节点在所有secondary节点上启动的所有待处理的prepared事务。

​	CitusMaintenanceDaemonMain函数，是citus后台维护进程的main函数，该函数当配置了2PC自动恢复检查间隔参数citus.recover_2pc_interval（默认60000毫秒）时，将会间隔这么多毫秒自动执行一次恢复2pc流程，具体执行流程，和上面那个函数一样，所以也需要新增同样的兼容性。

前面两个函数，实际上都是调用***RecoverTwoPhaseCommits*** 函数，来恢复此节点在其他节点上启动的所有待处理的prepared事务。所以需要在该函数中新增适配，即若当前节点是cn_rtap节点时，需要获取的是所有secondary节点的列表，以便恢复其上的prepared事务

**结论：**需要新增兼容性

**修改方式为：**

```c
int RecoverTwoPhaseCommits(void)
{
	。。。。。。
#ifdef FDD
	if(cn_rtap_role == FDD_GetLocalNodeRole())
	{
		workerList = ActiveReadableNodeList();
	}	
	else
#endif
	{
		workerList = ActivePrimaryNodeList();
	}
}
```

**测试方式：**

```sql
--在各种CN上先创建一下2pc事务，然后执行
select recover_prepared_transactions();
```

#### 18、RemoveNodeFromCluster <-

​           master_remove_node(udf) 

​           master_remove_node函数是用于把一个节点从集群中移除，其功能完全由***RemoveNodeFromCluster***函数来实现。 

**RemoveNodeFromCluster 函数功能**：若某个节点存在于集群，则把其从集群中移除，并且还会同步修改所有涉及到的元数据，同时也会把修改的元数据同步到其他所有CN节点，具体操作步骤如下：

1、如果带移除的节点是worker节点，则在主CN节点和具有元数据的所有节点中，从pg_dist_placement中删除属于给定节点的所有参考表的位置信息，但不会删除对应节点上的实际数据。

2、如果移除的节点上有活动的分片数据，则移除失败。

3、在主CN上，从pg_dist_node表中删除该节点信息。

4、如果待移除的节点是worker节点，则修改参考表的亲和组的复制因子（即pg_dist_colocation表中的对应行），因为该复制因子等于集群中活的primary worker节点个数（注意步骤3已经移除了待删除的节点，所以此处的节点个数是移除后的个数）。

5、在其他具有元数据的所有节点中，从pg_dist_node表中删除该节点信息。

在这个函数中，有一个地方调用了***ActivePrimaryNodeList***函数，也就是上面的步骤3，用于获取移除待删除的worker节点后，当前集群中剩下的活的primary worker节点列表，以便同步修改复制表的亲和组的复制因子，而新增的两种CN上面不会存放数据，故不影响复制表的亲和组的复制因子计算。

**结论：**不需要新增兼容性

**注意：**目前的实现，对于第4步，只会修改主CN上pg_dist_colocation表中的记录，而不会同步修改其他所有CN上的记录，并且pg_dist_colocation表中的数据不会被缓存，故也不会涉及缓存失效机制。实际上pg_dist_colocation表中所有元数据都不会同步到其他所有CN中，因为所有的DDL操作都是在主CN进行，故其他CN不关心这个。

**测试方式：**

```sql
--在主CN上通过master_remove_node移除掉一个primary worker后，在所有CN节点执行
select * from pg_dist_colocation;
```

#### 19、ReplicateAllReferenceTablesToNode <-

​        ActivateNode  <- 

​       master_activate_node（UDF）

​       master_add_coordinator（UDF）

​       master_add_node（UDF）

ActivateNode  函数功能：用于激活一个节点，也就是在pg_dist_node表中设置该节点对应行中，isactive字段为“true”，同时把该修改同步到所有含有元数据的节点上；若待激活的节点是primary worker节点，将会调用ReplicateAllReferenceTablesToNode 函数，把集群中所有的复制表复制一份到该节点中。

**ReplicateAllReferenceTablesToNode 函数功能**：查找集群中的所有复制表，然后把这些复制表包括其中的数据都复制一份到目标节点中，同时也会更新pg_dist_colocation表中，复制表所在亲和组行中replicationfactor（复制因子）列的值（也就是当前集群活的primary worker节点个数）。

**注意：**若某个复制表的placement在目标节点存在并且可用（即shardstate为FILE_FINALIZED，也就是为1），则跳过该placement的复制，这种情形一般出现在把分片个数为1的分布表转换为复制表的场景。

在该函数中使用***ActivePrimaryNodeList***函数，是用于获取当前集群活的primary worker节点个数，以便更新pg_dist_colocation表中，复制表所在亲和组行中replicationfactor（复制因子）列的值，因为不需要统计新增两种CN节点的个数，所以此处不用进行兼容性修改。

**结论：**不需要新增兼容性

#### 20、ReplicateShardToAllWorkers   <-

​        ReplicateSingleShardTableToAllWorkers  <-  upgrade_to_reference_table(UDF)

**upgrade_to_reference_table函数功能**：它是一个UDF函数，用于把一个只具有一个分片的分布表（并且该表不能够采用流复制）转换为复制表，并且把该表同步到所有worker节点，同时修改元数据，标记该分布表变成了复制表，而这些全部都是通过调用下面的ReplicateSingleShardTableToAllWorkers函数实现。

**ReplicateSingleShardTableToAllWorkers函数功能**：实现把一个分片表变成复制表，主要流程如下：

1、把一个分片表复制到所有worker节点，如果对应的worker节点上已经存在该分片（实际上就是该分片原始所在节点），避免重复操作就跳过复制，该步由***ReplicateShardToAllWorkers***函数实现；

2、在主CN上先在pg_dist_partition、pg_dist_colocation 和pg_dist_shard表中删除该分片的记录，然后在pg_dist_partition和pg_dist_shard中新增行，即把该分片表变为了复制表，shardId保持不变。

3、在所有节点（primary workers和两种新增CN）上新增该复制表，包括创建该复制表、插入对应的元数据、设置对应的触发器等操作。

**注意：**在步骤2中，只是在主CN上删除三个表中原分布表对应的元数据，而不会同步到其他含有元数据的节点，这是因为只有非流复制的分布表，才允许转换为复制表，而非流复制表的元数据，是不会同步到其他含有元数据的节点，所以此处也就只需要在主CN上删除即可。

**ReplicateShardToAllWorkers   函数功能**：用于把一个分片表复制到所有worker节点，如果对应的worker节点上已经存在该分片（实际上就是该分片原始所在节点），避免重复操作就跳过复制；而在该函数中用于获取所有worker节点函数就是ActivePrimaryNodeList，因为两种CN节点上不存放实际数据，所以此处不用新增兼容性。

**结论：**不需要新增兼容性

测试用例：

```sql
--创建hash分布表，分片个数为1
create table tbl8(id int primary key,name int);
set citus.shard_count = 1;
SELECT create_distributed_table('tbl8', 'id');

--将分布表tbl8提升为复制表
SELECT upgrade_to_reference_table('tbl8');
```



#### 21、SendBareCommandListToWorkers  <- 

​          ExecuteDistributedDDLJob

**ExecuteDistributedDDLJob函数功能**：用于在分布式事务中执行DDLJob，包括元数据同步（如果需要）。在提交协议中，在连接到每个分片所在节点后发送BEGIN，并且CoordinatedTransactionCallback函数处理COMMIT / ROLLBACK消息。

> 需要注意的是：对于在线创建索引（CREATE INDEX CONCURRENTLY），为了不堵塞其他会话对被创建索引表的DML（INSERT,UPDATE,DELETE）操作，以便适合于在线业务。在该函数中会对这种场景进行特殊处理，也就是采用‘bare’类型的方式发送命令，以便该命令不是在一个事务块中进行发送。

**SendBareCommandListToWorkers 函数功能：**用于将一组命令以串行方式发送给一组目标worker（有元数据）节点。这些命令会立即提交：始终使用新连接，并且不使用事务块（因此是‘bare’方式）。这些连接是以citus管理者身份创建的，以确保有对Citus元数据表的写访问权限。这对于使用CREATE INDEX CONCURRENTLY命令非常有用。

**注意1：**这一组命令是先逐一发送到一个目标节点，每个命令操作成功后才会发送下一条命令，只有在把这些命令在一个节点上都发送完，才会在另一个节点上进行发送。

**注意2：**这些命令的发送，都不是在一个事务块中进行的。

修改方式：citus的原有思路是，只有目标worker节点上含有元数据，也就是mx节点，才会把这一组命令发送到其上，而这个又是为执行CREATE INDEX CONCURRENTLY命令特地兼容的，那其思路是不是就是目的节点不仅仅含有元数据同样也得含有实际数据，才可以执行该命令了？需要观察主CN是否也会有该命令的执行。

**结论：**是否需要进行兼容性修改，需要进一步考虑

> **UpdateNodeLocation函数功能**：该函数是用于根据nodeid来更新pg_dist_node表中的nodename和nodeport字段，并且会把该修改同步到其他含有元数据的CN节点上，在更新前，若发送对应CN节点的pg_dist_node表上存在待修改的nodename和nodeport，则会先修改，然后在同步。
>
> 在之前的修改方案中，是采用SendBareCommandListToWorkers 来发送命令到目的节点上，应该将其改为采用SendCommandToWorkers函数来发送，保证走分布式事务。

测试用例：

```sql
--tbl6为分布表
create index CONCURRENTLY idx_tbl6 on tbl6 (id);
```



#### 22、SendCommandToFirstworker  <-

​          LockShardListResourcesOnFirstWorker <- SerializeNonCommutativeWrites

**LockShardListResourcesOnFirstWorker 函数功能**：该函数在本节的13小点中介绍过，用于获取第一个worker上指定分片的资源锁，该函数不对分片列表进行排序，因此调用者应对分片列表进行排序以避免死锁。

**SendCommandToFirstworker 函数功能**：先对所有的活动primary worker节点列表按主机名和端口号进行排序，选择出第一个worker节点，然后使用SendCommandToWorker将给定命令发送到该节点上，因为新增的两类CN上不存放实际的分片数据，所以在排序的节点列表中，不应该包括他们。

**结论：**不需要新增兼容性

#### 23、SendCommandToWorkersParams <-

​          SendCommandToWorkers

**SendCommandToWorkers函数功能**：该函数是直接调用SendCommandToWorkersParams函数来实现的，只是没有指定PQexecParams参数，也就是采用默认的。

**SendCommandToWorkersParams函数功能**：并行向所有Active 且符合一定条件的Primary Worker发送命令。 当本地事务提交时，将在worker上也会提交命令这些。 这些连接是以Citus管理者身份创建的，以确保有对Citus元数据表的写访问权限。因为有时候也需要只向含有原始的节点发送命令，即指定参数targetWorkerSet为WORKERS_WITH_METADATA，所以在这需要为新增的cn_rtap和cn_tp节点新增兼容性，以便在这种情况下，也会将命令发送到这些节点。

**结论：**需要新增兼容性

**修改方式**：

```c
void
SendCommandToWorkersParams(TargetWorkerSet targetWorkerSet, char *command,
						   int parameterCount, const Oid *parameterTypes,
						   const char *const *parameterValues)
{
	List *connectionList = NIL;
	ListCell *connectionCell = NULL;
#ifdef FDD
    List *workerNodeList = FDD_ActivePrimaryAndCNNodeList();
#else
    List *workerNodeList = ActivePrimaryNodeList();
#endif
}
```

### 2.2 SendCommandToWorkersParams <-

​	  SendCommandToWorkers

从2.1节第23条可以知道，该函数需要进行兼容性修改。以下是我们对其调用关系的排查，确认修改后是否将引入问题。重点关注是否有针对于MX节点的特殊场景

#### 1、 CreateTableMetadataOnWorkers <- 

​         CreateDistributedTable

​	 ReplicateSingleShardTableToAllWorkers <- upgrade_to_reference_table(UDF)

**CreateTableMetadataOnWorkers函数功能：**生成一套完整的在Worker节点上创建指定分布表所必须的DDL命令行集（相关代码如下），以及 `SET enable_ddl_propagation TO off`的命令，按顺序依次通过 `SendCommandToWorkers` 下发给Worker节点。

```c
void CreateTableMetadataOnWorkers(Oid relationId)
{
	List *commandList = GetDistributedTableDDLEvents(relationId);
	ListCell *commandCell = NULL;

	/* prevent recursive propagation */
	SendCommandToWorkers(WORKERS_WITH_METADATA, DISABLE_DDL_PROPAGATION);

	/* send the commands one by one */
	foreach(commandCell, commandList)
	{
		char *command = (char *) lfirst(commandCell);

		SendCommandToWorkers(WORKERS_WITH_METADATA, command);
	}
}
List *GetDistributedTableDDLEvents(Oid relationId)
{
	DistTableCacheEntry *cacheEntry = DistributedTableCacheEntry(relationId);
	。。。。。。
	bool includeSequenceDefaults = true;

	/* commands to create sequences */
	sequenceDDLCommands = SequenceDDLCommandsForTable(relationId);
	commandList = list_concat(commandList, sequenceDDLCommands);

	/* commands to create the table */
	tableDDLCommands = GetTableDDLEvents(relationId, includeSequenceDefaults);
	commandList = list_concat(commandList, tableDDLCommands);

	/* command to reset the table owner */
	tableOwnerResetCommand = TableOwnerResetCommand(relationId);
	commandList = lappend(commandList, tableOwnerResetCommand);

	/* command to insert pg_dist_partition entry */
	metadataCommand = DistributionCreateCommand(cacheEntry);
	commandList = lappend(commandList, metadataCommand);

	/* commands to create the truncate trigger of the table */
	truncateTriggerCreateCommand = TruncateTriggerCreateCommand(relationId);
	commandList = lappend(commandList, truncateTriggerCreateCommand);

	/* commands to insert pg_dist_shard & pg_dist_placement entries */
	shardIntervalList = LoadShardIntervalList(relationId);
	shardMetadataInsertCommandList = ShardListInsertCommand(shardIntervalList);
	commandList = list_concat(commandList, shardMetadataInsertCommandList);

	/* commands to create foreign key constraints */
	foreignConstraintCommands = GetTableForeignConstraintCommands(relationId);
	commandList = list_concat(commandList, foreignConstraintCommands);

	/* commands to create partitioning hierarchy */
	if (PartitionTable(relationId))
	{
		char *alterTableAttachPartitionCommands =
			GenerateAlterTableAttachPartitionCommand(relationId);
		commandList = lappend(commandList, alterTableAttachPartitionCommands);
	}

	return commandList;
}
```

仅当创建流复制hash表、参考表或使用UDF函数`upgrade_to_reference_table`时将被调用，只支持主CN调用。所以此处兼容新增的cn_rtap和cn_tp节点新增兼容性，可以使这些命令发送新增CN节点，确保这些节点上与该表相关的元数据与集群保持一致。

**结论：**兼容性修改可行

测试用例：

```sql
-- 参考2.1节第9条的用例
SET citus.replication_model TO streaming;
set citus.shard_count = 3; 

CREATE SCHEMA IF NOT EXISTS myschema AUTHORIZATION postgres;
create table myschema.tbl7(id int,name int);
SELECT create_distributed_table('myschema.tbl7', 'id');

--在cn_tp、cn_rtap上查看tbl7表相关信息应与集群主CN以及MX节点一致
\dt
select * from pg_dist_partition;
select * from pg_dist_shard;
select * from pg_dist_placement;
```

#### 2、MasterRemoveDistributedTableMetadataFromWorkers <-

​	master_remove_distributed_table_metadata_from_workers(UDF)

**此函数功能：**删除分布表，且该表在MX节点上有元数据（也即流复制hash表、参考表）时，通知集群拥有元数据的Worker节点删除该表的元数据。

> 需要注意的是：此函数只支持主CN（也即localGroupId=0）调用。而且当环境变量enable_ddl_propagation 为off时，将不执行具体流程，直接退出。通过在Worker执行 'SELECT worker_drop_distributed_table(<shemaName.tableName>)'来删除表及其元数据。

此函数由上述UDF函数唯一调用。而在`citus--8.0-2--8.0-3.sql` 中创建触发器 pg_catalog.citus_drop_trigger，对通过 pg_event_trigger_dropped_objects 获取的待删除的表，依次执行上述UDF函数以及其他UDF函数（master_drop_all_shards、master_remove_partition_metadata），完成删除表相关元数据以及分片数据的系列操作。

建表时需要同步相关元数据给新CN角色节点，同样删表时也需要同步。

**结论：**兼容性修改可行

测试用例：

```sql
--在主CN删除1个参考表后，在cn_tp、cn_rtap上查看该表相关信息，应与集群主CN以及MX节点一致
\dt
select * from pg_dist_partition;
select * from pg_dist_shard;
select * from pg_dist_placement;
```

#### 3、UpdateRelationColocationGroup <-

​	MarkTablesColocated <- mark_tables_colocated(UDF)

**UpdateRelationColocationGroup函数功能：**修改 pg_dist_partition 表中指定 distributedRelationId 记录的属性 colocationid为指定值。如果distributedRelationId是流复制hash表或者参考表，则将此修改命令通过 `SendCommandToWorkers` 下发给Worker节点。

在MarkTablesColocated中，先后有两处调用此函数。

1. 如果源表的亲和表id为默认值，则为之创建1条新记录插入 pg_dist_colocation 表，并将源表的亲和表id修改为新生成的亲和表id
2.  将目的表的亲和id修改为源表的亲和表id

> 需要注意的是：标记互为亲和表的双方，也即源表、目的表双方需符合一定条件，才能亲和成功。
>
> 1. 表的流复制模式相同
> 2. 分布key值类型相同（复制表的key值为InvalidOid）
> 3. 分片数量相同
> 4. 分片的分区方式相同
> 5. 若分区方式为HASH，则要求双方分片的区间值（最小值、最大值）相同
> 6. 分片的placement数相同
> 7. 分片的placement所在Worker节点相同
> 8. 分片的placement状态相同

**结论：**兼容性修改可行

#### 4、master_drop_sequences(UDF)

**此函数功能：**删除Worker节点上的指定一系列sequence。当入参sequence有效时，将先后两次调用`SendCommandToWorkers`将指令下发给Worker节点。

1. 发送`SET citus.enable_ddl_propagation TO off`指令。
2. 发送`DROP SEQUENCE IF EXISTS %s, %s,..,%s CASCADE`指令，其中`%s`为sequence 值。

2.2节第1条说明建表或升级到参考表时可能同步创建Sequence相关指令给新CN角色节点，因此删除Sequence时也需要同步。而且，`DROP SEQUENCE IF EXISTS`保障即使节点上并不存在该Sequence也不会出错。

**结论：**兼容性修改可行

#### 5、RemoveNodeFromCluster <-

​	master_remove_node

**此函数功能：**见2.1节第18条。此函数中调用`SendCommandToWorkers`通知所有具有元数据的节点，删除pg_dist_node表中相应节点信息。

**结论：**兼容性修改可行

#### 6、ProcessDropTableStmt <-

​	multi_ProcessUtility

**此函数功能：**对传入的一系列表，按顺序依次执行删表操作。

1. 根据表名获取relationId，若无效或不是分布表（pg_dist_partition表中可见），则跳过不处理
2. 若该表曾经或正在被其他表通过外键关联，则标记外键图失效`shouldInvalidateForeignKeyGraph = true`
3. 若该表不是在元数据节点上可见的表（参考表、流复制hash表），或不是PG分区表（PG10支持relkind=RELKIND_PARTITIONED_TABLE）；实际上分区表只支持hash分布，因此若表不是流复制的hash PG分区表，则跳过不处理
4. 若当前节点不是主CN（localGroupId=0），则报错退出
5. 获取该表的所有子表，若不存在，则跳过不处理
6. 调用`SendCommandToWorkers`通知所有具有元数据的节点执行 `SET citus.enable_ddl_propagation TO off`
7. 遍历步骤5获取的所有子表，若该表为PG分区表，则生成SQL指令`ALTER TABLE IF EXISTS %s DETACH PARTITION %s;`参数依次为：子表名、父表名，调用`SendCommandToWorkers`通知所有具有元数据的节点执行

2.2节第1条说明创建上述类型的表时，将同步相关数据给元数据节点，因此删除时也需要同步。而且，`ALTER TABLE IF EXISTS`保障即使节点上并不存在该表也不会出错。

**结论：**兼容性修改可行

#### 7、ExecuteDistributedDDLJob <-

​	multi_ProcessUtility

**此函数功能：**见2.1节第21条。有3处调用`SendCommandToWorkers`同步命令到有元数据的节点，当DDL操作的是参考表、流复制hash表时：

1. 通知所有具有元数据的节点执行 `SET citus.enable_ddl_propagation TO off`
2. 通知所有具有元数据的节点执行 `SET search_path TO %s;`
3. 通知所有具有元数据的节点执行 DDL 指令

**结论：**兼容性修改可行

#### 8、AddNodeMetadata <-

​	master_add_node

​	master_add_secondary_node

​	master_add_inactive_node

​	master_initialize_node_metadata

​	master_add_coordinator

**此函数功能：**结合传入的节点信息，生成1条新的pg_dist_node记录，并通知所有拥有元数据的节点同步更新pg_dist_node表。此函数限制只能由主CN（localGroupId=0）调用，有2处调用`SendCommandToWorkers`同步命令到有元数据的节点：

1. 通知所有具有元数据的节点执行 `DELETE FROM pg_dist_node WHERE nodeid = %u`，删除pg_dist_node表中与新增节点的nodeId相同的节点记录
2. 通知所有具有元数据的节点执行 `INSERT INTO pg_dist_node (nodeid, groupid, nodename, nodeport, noderack, hasmetadata, isactive, noderole, nodecluster) VALUES (%d, %d, %s, %d, %s, %s, %s, '%s'::noderole, %s)`，往pg_dist_node表中插入新增的节点记录

新增CN角色需与主CN保持元数据一致，因此集群新增节点时也应该将数据同步给新增CN角色。

**结论：**兼容性修改可行

#### 9、SetNodeState <-

​	master_disable_node

​	ActivateNode <- (调用关系见2.1节第19条)

**此函数功能：**修改传入节点在pg_dist_node表中的isactive属性，并生成SQL指令 `UPDATE pg_dist_node SET isactive = %s WHERE nodeid = %u`，调用`SendCommandToWorkers`通知所有元数据节点同步该节点的属性变更。

**结论：**兼容性修改可行

#### 10、ReplicateShardToNode <-

​	ReplicateAllReferenceTablesToNode <- (调用关系见2.1节第19条)

​	ReplicateShardToAllWorkers <- ReplicateSingleShardTableToAllWorkers <- upgrade_to_reference_table

**此函数功能：**开启一个独立的事务，将指定分片拷贝到指定节点；拷贝成功后，刷新本地pg_dist_placement表相关信息。如果该shardId所属表为元数据节点上可见的表（参考表、流复制hash表），则生成SQL指令`INSERT INTO pg_dist_placement (shardid, shardstate, shardlength, groupid, placementid) VALUES (UINT64_FORMAT, %d, UINT64_FORMAT, %d, UINT64_FORMAT) ON CONFLICT (placementid) DO UPDATE SET shardid = EXCLUDED.shardid, shardstate = EXCLUDED.shardstate, shardlength = EXCLUDED.shardlength, groupid = EXCLUDED.groupid`，调用`SendCommandToWorkers`通知所有元数据节点刷新pg_dist_placement表。若指定节点已存在状态正常的该shardId分片，则不进行拷贝直接退出。

**结论：**兼容性修改可行

#### 11、DeleteAllReferenceTablePlacementsFromNodeGroup <-

​	RemoveNodeFromCluster <- master_remove_node

​	master_disable_node

**此函数功能：**删除pg_dist_placement表中所有参考表的placement记录，并生成SQL指令`DELETE FROM pg_dist_placement WHERE placementid = %u`，调用`SendCommandToWorkers`通知所有元数据节点刷新pg_dist_placement表。

**结论：**兼容性修改可行

#### 12、LockShardListMetadataOnWorkers <- 

​	LockReferencedReferenceShardDistributionMetadata <- BlockWritesToShardList

​	BlockWritesToShardList

​		<-

​		RepairShardPlacement <- master_copy_shard_placement

​		DeleteAllReferenceTablePlacementsFromNodeGroup <- (调用关系见本节第11条)

​		ReplicateAllReferenceTablesToNode <-  (调用关系见2.1节第19条)

**此函数功能：**生成SQL指令`SELECT lock_shard_metadata(%d, ARRAY[%lu, %lu, ..., %lu])` ，调用`SendCommandToWorkers`通知所有元数据节点对分片元数据上锁。其中 %d 为锁类型，%lu 为 shardId。

通过对分片元数据上锁，避免在上述调用流程中，有元数据节点对分片进行DML以及DDL操作。新增CN角色也能操作分片，因此也需要同步上锁。

**结论：**兼容性修改可行

### 2.3 HasMetadataWorkers <-

​          ClusterHasKnownMetadataWorkers

​          EnsureSupportedSequenceColumnType

 1、ClusterHasKnownMetadataWorkers
​	该函数用于返回，集群中是否有worker上存在元数据（即MX节点），cn_tp和cn_rtap可以看成是一种特殊的MX节点，所以此处需要新增兼容性

2、EnsureSupportedSequenceColumnType

​	该函数在创建分布表时调用，用于查看是否 存在某个列是“序列”，若是则进一步判断其类型是否适合以分布式方式使用。任何依赖于该序列但不是bigserial类型的列都不能用于mx表，因为没有足够的值来确保生成的数字是全局唯一的。

> 参见官方文档： http://docs.citusdata.com/en/v8.1/arch/mx.html?highlight=SEQUENCE 
>
> 在文档最后面有：Serial columns must have type “bigserial.” Globally in the cluster the **sequence** values will not be monotonically increasing because the sixteen most significant bits hold the worker node id.
>
> 即：在整个集群中，序列值不会单调增加，因为最高十六个有效位保存工作节点id。

​	而在该函数中，调用***HasMetadataWorkers***，是用于判断集群是否是MX，而我们新增的两种CN类型，也是多CN架构，所以也存在若序列作为一个列值，那么其类型也必须为bigserial类型，故需要新增兼容性。

**HasMetadataWorkers函数功能**：用于判断集群中，是否存在worker节点含有元数据

**结论：**需要新增兼容性

**修改方式**：

```c
static bool HasMetadataWorkers(void)
{
#ifdef FDD
	List *workerNodeList = FDD_ActivePrimaryAndCNNodeList();
#else
    List *workerNodeList = ActivePrimaryNodeList();
#endif
	。。。。。。
}
```

### 2.4 RemoveNodeFromCluster

参考上面2.1节的第18小节 

**结论：**不需要新增兼容性



