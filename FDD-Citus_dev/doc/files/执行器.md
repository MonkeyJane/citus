# 1. 执行器选择

1. Citus 向PgSQL注册自定义的执行器相关结构体

```
void RegisterCitusCustomScanMethods(void)
{
	RegisterCustomScanMethods(&RealTimeCustomScanMethods);
	RegisterCustomScanMethods(&TaskTrackerCustomScanMethods);
	RegisterCustomScanMethods(&RouterCustomScanMethods);
	RegisterCustomScanMethods(&CoordinatorInsertSelectCustomScanMethods);
	RegisterCustomScanMethods(&DelayedErrorCustomScanMethods);
}

# 执行器方法结构体（src\backend\distributed\executor\citus_custom_scan.c）
CustomScanMethods RealTimeCustomScanMethods = {
	"Citus Real-Time",
	RealTimeCreateScan
};

CustomScanMethods TaskTrackerCustomScanMethods = {
	"Citus Task-Tracker",
	TaskTrackerCreateScan
};

CustomScanMethods RouterCustomScanMethods = {
	"Citus Router",
	RouterCreateScan
};

CustomScanMethods CoordinatorInsertSelectCustomScanMethods = {
	"Citus INSERT ... SELECT via coordinator",
	CoordinatorInsertSelectCreateScan
};

# 执行器规则结构体（src\backend\distributed\executor\citus_custom_scan.c）
static CustomExecMethods RealTimeCustomExecMethods = {
	.CustomName = "RealTimeScan",
	.BeginCustomScan = CitusSelectBeginScan,
	.ExecCustomScan = RealTimeExecScan,
	.EndCustomScan = CitusEndScan,
	.ReScanCustomScan = CitusReScan,
	.ExplainCustomScan = CitusExplainScan
};

static CustomExecMethods TaskTrackerCustomExecMethods = {
	.CustomName = "TaskTrackerScan",
	.BeginCustomScan = CitusSelectBeginScan,
	.ExecCustomScan = TaskTrackerExecScan,
	.EndCustomScan = CitusEndScan,
	.ReScanCustomScan = CitusReScan,
	.ExplainCustomScan = CitusExplainScan
};

static CustomExecMethods RouterSequentialModifyCustomExecMethods = {
	.CustomName = "RouterSequentialModifyScan",
	.BeginCustomScan = CitusModifyBeginScan,
	.ExecCustomScan = RouterSequentialModifyExecScan,
	.EndCustomScan = CitusEndScan,
	.ReScanCustomScan = CitusReScan,
	.ExplainCustomScan = CitusExplainScan
};

static CustomExecMethods RouterMultiModifyCustomExecMethods = {
	.CustomName = "RouterMultiModifyScan",
	.BeginCustomScan = CitusModifyBeginScan,
	.ExecCustomScan = RouterMultiModifyExecScan,
	.EndCustomScan = CitusEndScan,
	.ReScanCustomScan = CitusReScan,
	.ExplainCustomScan = CitusExplainScan
};

static CustomExecMethods RouterSelectCustomExecMethods = {
	.CustomName = "RouterSelectScan",
	.BeginCustomScan = CitusSelectBeginScan,
	.ExecCustomScan = RouterSelectExecScan,
	.EndCustomScan = CitusEndScan,
	.ReScanCustomScan = CitusReScan,
	.ExplainCustomScan = CitusExplainScan
};

static CustomExecMethods CoordinatorInsertSelectCustomExecMethods = {
	.CustomName = "CoordinatorInsertSelectScan",
	.BeginCustomScan = CitusSelectBeginScan,
	.ExecCustomScan = CoordinatorInsertSelectExecScan,
	.EndCustomScan = CitusEndScan,
	.ReScanCustomScan = CitusReScan,
	.ExplainCustomScan = CoordinatorInsertSelectExplainScan
};
```

2. Citus 根据计划确定执行器类型

```
shared_library_init::_PG_init --> distributed_planner::distributed_planner --> distributed_planner::CreateDistributedPlan --> distributed_planner::FinalizePlan

MultiExecutorType JobExecutorType(DistributedPlan *distributedPlan)
{
	Job *job = distributedPlan->workerJob;
	List *workerNodeList = NIL;
	int workerNodeCount = 0;
	int taskCount = 0;
	double tasksPerNode = 0.;
	MultiExecutorType executorType = TaskExecutorType;
	bool routerExecutablePlan = distributedPlan->routerExecutable;

	/* check if can switch to router executor */
	if (routerExecutablePlan)
	{
		ereport(DEBUG2, (errmsg("Plan is router executable")));
		return MULTI_EXECUTOR_ROUTER;
	}

	if (distributedPlan->insertSelectSubquery != NULL)
	{
		return MULTI_EXECUTOR_COORDINATOR_INSERT_SELECT;
	}

	Assert(distributedPlan->operation == CMD_SELECT);

	workerNodeList = ActiveReadableNodeList();
	workerNodeCount = list_length(workerNodeList);
	taskCount = list_length(job->taskList);
	tasksPerNode = taskCount / ((double) workerNodeCount);

	if (executorType == MULTI_EXECUTOR_REAL_TIME)
	{
		double reasonableConnectionCount = 0;
		int dependedJobCount = 0;

		/* if we need to open too many connections per worker, warn the user */
		if (tasksPerNode >= MaxConnections)
		{
			ereport(WARNING, (errmsg("this query uses more connections than the "
									 "configured max_connections limit"),
							  errhint("Consider increasing max_connections or setting "
									  "citus.task_executor_type to "
									  "\"task-tracker\".")));
		}

		/*
		 * If we need to open too many outgoing connections, warn the user.
		 * The real-time executor caps the number of tasks it starts by the same limit,
		 * but we still issue this warning because it degrades performance.
		 */
		reasonableConnectionCount = MaxMasterConnectionCount();
		if (taskCount >= reasonableConnectionCount)
		{
			ereport(WARNING, (errmsg("this query uses more file descriptors than the "
									 "configured max_files_per_process limit"),
							  errhint("Consider increasing max_files_per_process or "
									  "setting citus.task_executor_type to "
									  "\"task-tracker\".")));
		}

		/* if we have repartition jobs with real time executor and repartition
		 * joins are not enabled, error out. Otherwise, switch to task-tracker
		 */
		dependedJobCount = list_length(job->dependedJobList);
		if (dependedJobCount > 0)
		{
			if (!EnableRepartitionJoins)
			{
				ereport(ERROR, (errmsg(
									"the query contains a join that requires repartitioning"),
								errhint("Set citus.enable_repartition_joins to on "
										"to enable repartitioning")));
			}

			ereport(DEBUG1, (errmsg(
								 "cannot use real time executor with repartition jobs"),
							 errhint("Since you enabled citus.enable_repartition_joins "
									 "Citus chose to use task-tracker.")));
			return MULTI_EXECUTOR_TASK_TRACKER;
		}
	}
	else
	{
		/* if we have more tasks per node than what can be tracked, warn the user */
		if (tasksPerNode >= MaxTrackedTasksPerNode)
		{
			ereport(WARNING, (errmsg("this query assigns more tasks per node than the "
									 "configured max_tracked_tasks_per_node limit")));
		}
	}

	return executorType;
}

static PlannedStmt *FinalizePlan(PlannedStmt *localPlan, 
										DistributedPlan *distributedPlan)
{
	PlannedStmt *finalPlan = NULL;
	CustomScan *customScan = makeNode(CustomScan);
	Node *distributedPlanData = NULL;
	MultiExecutorType executorType = MULTI_EXECUTOR_INVALID_FIRST;

	if (!distributedPlan->planningError)
	{
		executorType = JobExecutorType(distributedPlan);
	}

	switch (executorType)
	{
		case MULTI_EXECUTOR_REAL_TIME:
		{
			customScan->methods = &RealTimeCustomScanMethods;
			break;
		}

		case MULTI_EXECUTOR_TASK_TRACKER:
		{
			customScan->methods = &TaskTrackerCustomScanMethods;
			break;
		}

		case MULTI_EXECUTOR_ROUTER:
		{
			customScan->methods = &RouterCustomScanMethods;
			break;
		}

		case MULTI_EXECUTOR_COORDINATOR_INSERT_SELECT:
		{
			customScan->methods = &CoordinatorInsertSelectCustomScanMethods;
			break;
		}

		default:
		{
			customScan->methods = &DelayedErrorCustomScanMethods;
			break;
		}
	}

	if (IsMultiTaskPlan(distributedPlan))
	{
		/* if it is not a single task executable plan, inform user according to the log level */
		if (MultiTaskQueryLogLevel != MULTI_TASK_QUERY_INFO_OFF)
		{
			ereport(MultiTaskQueryLogLevel, (errmsg(
												 "multi-task query about to be executed"),
											 errhint(
												 "Queries are split to multiple tasks "
												 "if they have to be split into several"
												 " queries on the workers.")));
		}
	}

	distributedPlan->relationIdList = localPlan->relationOids;

	distributedPlanData = (Node *) distributedPlan;

	customScan->custom_private = list_make1(distributedPlanData);
	customScan->flags = CUSTOMPATH_SUPPORT_BACKWARD_SCAN;

	if (distributedPlan->masterQuery)
	{
		finalPlan = FinalizeNonRouterPlan(localPlan, distributedPlan, customScan);
	}
	else
	{
		finalPlan = FinalizeRouterPlan(localPlan, customScan);
	}

	return finalPlan;
}
```



# 2. 执行器调度

```
由 PgSQL 代码完成调度
main::main  --> postgres::PostgresMain  --> postmaster::ServerLoop
--> postmaster::BackendStartup  --> postmaster::BackendRun  --> postgres::PostgresMain
--> postgres::exec_simple_query  --> pquery::PortalRun  --> pquery::PortalRunSelect
--> execMain::ExecutorRun  --> execMain::standard_ExecutorRun  
--> execMain::ExecutePlan  --> executor::ExecProcNode  --> nodeCustom::ExecCustomScan

ExecCustomScan 通过以下流程挂载到 node->ExecProcNode，供executor::ExecProcNode使用
postgres::exec_simple_query  --> pquery::PortalStart  --> execMain::ExecutorStart
--> execMain::standard_ExecutorStart  --> execMain::InitPlan
--> execProcnode::ExecInitNode  --> nodeCustom::ExecInitCustomScan
```



# 3. 执行器流程





## 3.1 Real-Time执行器

Real-Time 执行器

```c++
# Real-Time执行器函数 multi_real_time_executor::RealTimeExecScan
1. LockPartitionsInRelationList    -- 为分片表的每一个分片上锁（AccessShareLock）
2. PrepareMasterJobDirectory       -- 在Coordinator上创建文件夹用于保存任务执行结果，当入口关闭时将自动删除
3. ExecuteSubPlans                 -- 顺序执行分布计划的每一个子计划
4. MultiRealTimeExecute            -- 执行逻辑
5. LoadTuplesIntoTupleStore        -- 将文件中的执行结果读取出来，加载到指定元组
6. ReturnTupleFromTuplestore       -- 逐条返回元组中存储的执行结果
```

### 3.1.1 相关数据结构

#### 3.1.1.1 连接池

##### 3.1.1.1.1 ConnectionPlacementHash

ConnectionPlacementHash 为全局变量，是一个名为"citus connection cache (placementid)"的Hash表。表元素为ConnectionPlacementHashEntry 结构体，Hash表以结构体属性 placementId 为key。 

```c++
static HTAB *ConnectionPlacementHash;

/* hash entry */
typedef struct ConnectionPlacementHashEntry
{
	ConnectionPlacementHashKey key;

	/* did any remote transactions fail? */
	bool failed;

	/* primary connection used to access the placement */
	ConnectionReference *primaryConnection;

	/* are any other connections reading from the placements? */
	bool hasSecondaryConnections;

	/* entry for the set of co-located placements */
	struct ColocatedPlacementsHashEntry *colocatedEntry;

	/* membership in ConnectionShardHashEntry->placementConnections */
	dlist_node shardNode;
} ConnectionPlacementHashEntry;

/* hash key */
typedef struct ConnectionPlacementHashKey
{
	uint64 placementId;
} ConnectionPlacementHashKey;

typedef struct ConnectionReference
{
	/*
	 * The user used to read/modify the placement. We cannot reuse connections
	 * that were performed using a different role, since it would not have the
	 * right permissions.
	 */
	const char *userName;

	/* the connection */
	MultiConnection *connection;

	/*
	 * Information about what the connection is used for. There can only be
	 * one connection executing DDL/DML for a placement to avoid deadlock
	 * issues/read-your-own-writes violations.  The difference between DDL/DML
	 * currently is only used to emit more precise error messages.
	 */
	bool hadDML;
	bool hadDDL;

	/* colocation group of the placement, if any */
	uint32 colocationGroupId;
	uint32 representativeValue;

	/* placementId of the placement, used only for append distributed tables */
	uint64 placementId;

	/* membership in MultiConnection->referencedPlacements */
	dlist_node connectionNode;
} ConnectionReference;
```

ConnectionPlacementHash 表存储了到各个placement的连接以及相关信息，为Cn与Wn节点之间的通信提供连接数据。

![1552880467970](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1552880467970.png)



Citus通过 FindOrCreatePlacementEntry 函数对 ConnectionPlacementHash 表进行插入或刷新数据。在这个查找连接池数据的过程中，不仅会从ConnectionPlacementHash 表中获取连接，还依赖以下几个表。

##### 3.1.1.1.2 ColocatedPlacementsHash

ColocatedPlacementsHash 为全局变量，是一个名为"citus connection cache (colocated placements)"的Hash表。表元素为ColocatedPlacementsHashEntry 结构体，Hash表以结构体属性 nodeName 、nodePort、colocationGroupId、representativeValue为key。 

```c++
static HTAB *ColocatedPlacementsHash;

/* hash entry */
typedef struct ColocatedPlacementsHashEntry
{
	ColocatedPlacementsHashKey key;

	/* primary connection used to access the co-located placements */
	ConnectionReference *primaryConnection;

	/* are any other connections reading from the placements? */
	bool hasSecondaryConnections;
}  ColocatedPlacementsHashEntry;

/* hash key */
typedef struct ColocatedPlacementsHashKey
{
	/* to identify host - database can't differ */
	char nodeName[MAX_NODE_LENGTH];
	uint32 nodePort;

	/* colocation group, or invalid */
	uint32 colocationGroupId;

	/* to represent the value range */
	uint32 representativeValue;
} ColocatedPlacementsHashKey;
```

ColocatedPlacementsHash 中存储着到Wn节点的连接数据，当ConnectionPlacementHash 中找不到placement对应的连接时，将从ColocatedPlacementsHash 查找与placement相关属性一致的co-located placement的连接。

![1552880504960](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1552880504960.png)



##### 3.1.1.1.3 ConnectionHash

ConnectionHash 为全局变量，是一个名为"citus connection cache (host,port,user,database)"的Hash表。表元素为ConnectionHashEntry结构体，Hash表以结构体属性 hostname、port、user、database为key。 

```c++
HTAB *ConnectionHash = NULL;

/* hash entry */
typedef struct ConnectionHashEntry
{
	ConnectionHashKey key;
	dlist_head *connections;
} ConnectionHashEntry;

/* hash key */
typedef struct ConnectionHashKey
{
	char hostname[MAX_NODE_LENGTH];
	int32 port;
	char user[NAMEDATALEN];
	char database[NAMEDATALEN];
} ConnectionHashKey;
```

ConnectionHash 表存储了到各个Wn节点的连接。当ConnectionPlacementHash 以及 ColocatedPlacementsHash 中都找不到placement对应的符合要求的连接时，将从中获取连接。若此处仍然找不到合适的连接，将新建立1条连接，并将这个连接关联到ConnectionPlacementHash 以及 ColocatedPlacementsHash。连接结构体 MultiConnection 定义如下。

```c++
typedef struct MultiConnection
{
	/* connection details, useful for error messages and such. */
	char hostname[MAX_NODE_LENGTH];
	int32 port;
	char user[NAMEDATALEN];
	char database[NAMEDATALEN];

	/* underlying libpq connection */
	struct pg_conn *pgConn;

	/* is the connection intended to be kept after transaction end */
	bool sessionLifespan;

	/* is the connection currently in use, and shouldn't be used by anything else */
	bool claimedExclusively;

	/* time connection establishment was started, for timeout */
	TimestampTz connectionStart;

	/* membership in list of list of connections in ConnectionHashEntry */
	dlist_node connectionNode;

	/* information about the associated remote transaction */
	RemoteTransaction remoteTransaction;

	/* membership in list of in-progress transactions */
	dlist_node transactionNode;

	/* list of all placements referenced by this connection */
	dlist_head referencedPlacements;

	/* number of bytes sent to PQputCopyData() since last flush */
	uint64 copyBytesWrittenSinceLastFlush;
} MultiConnection;
```

![1552883618422](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1552883618422.png)

其中，连接结构体MultiConnection 中属性claimedExclusively，以及结构体ConnectionReference 中的属性hadDDL 以及hadDML 直接决定了连接池连接是否可复用。

##### 3.1.1.1.4 ConnectionShardHash

ConnectionShardHash 为全局变量，是一个名为"citus connection cache (shardid)"的Hash表。表元素为ConnectionShardHashEntry 结构体，Hash表以结构体属性 shardId 为key。

```c++
static HTAB *ConnectionShardHash;

/* hash entry */
typedef struct ConnectionShardHashEntry
{
	ConnectionShardHashKey key;
	dlist_head placementConnections;
} ConnectionShardHashEntry;

/* hash key */
typedef struct ConnectionShardHashKey
{
	uint64 shardId;
} ConnectionShardHashKey;
```

ConnectionShardHash 存储着shard 与 placement 的关联关系数据。通过元素属性 placementConnections 的节点与 ConnectionPlacementHash 表中的元素进行关联。

![1552882217305](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1552882217305.png)



以上几个Hash表数据（ConnectionPlacementHash ColocatedPlacementsHash ConnectionShardHash），仅当ResetPlacementConnectionManagement 函数执行时被清空。触发场景：事务的提交、中断。

##### 3.1.1.1.5 ClientConnectionArray

```c++
// MultiConnection 定义见3.1.1.1.3
static MultiConnection *ClientConnectionArray[MAX_CONNECTION_COUNT];

#define MAX_CONNECTION_COUNT 2048 /* simultaneous client connection count */
```

ClientConnectionArray 为全局变量，以connectionId为索引，用于保存real-time与task-tracker执行器执行过程中创建的连接。

##### 3.1.1.1.6 ClientPollingStatusArray

```c++
static PostgresPollingStatusType ClientPollingStatusArray[MAX_CONNECTION_COUNT];

#define MAX_CONNECTION_COUNT 2048 /* simultaneous client connection count */

typedef enum
{
	PGRES_POLLING_FAILED = 0,
	PGRES_POLLING_READING,		/* These two indicate that one may	  */
	PGRES_POLLING_WRITING,		/* use select before polling again.   */
	PGRES_POLLING_OK,
	PGRES_POLLING_ACTIVE		/* unused; keep for awhile for backwards
								 * compatibility */
} PostgresPollingStatusType;
```

ClientPollingStatusArray 为全局变量，用于保存real-time与task-tracker执行器执行过程中创建的连接的状态。同样以connectionId为索引，与ClientConnectionArray 数据相对应，协同管理连接。

当任务执行完毕或异常失败时，将通过 MultiClientDisconnect 或 MultiClientReleaseConnection 重置ClientConnectionArray 以及ClientPollingStatusArray 对应的元素。

#### 3.1.1.2 文件管理

##### 3.1.1.2.1 RegisteredJobDirectories

```c++
JobDirectoryEntry *RegisteredJobDirectories = NULL;

typedef struct JobDirectoryEntry
{
	ResourceOwner owner;
	uint64 jobId;
} JobDirectoryEntry;
```

RegisteredJobDirectories 为全局变量，以数组存储与job相关的文件夹资源。通过向PgSQL注册释放函数MultiResourceOwnerReleaseCallback，删除释放数组元素。

#### 3.1.1.3 后端运行

##### 3.1.1.3.1 backendManagementShmemData

```c++
static BackendManagementShmemData *backendManagementShmemData = NULL;

/*
 * Each backend's data reside in the shared memory
 * on the BackendManagementShmemData.
 */
typedef struct BackendManagementShmemData
{
	int trancheId;
#if (PG_VERSION_NUM >= 100000)
	NamedLWLockTranche namedLockTranche;
#else
	LWLockTranche lockTranche;
#endif
	LWLock lock;

	/*
	 * We prefer to use an atomic integer over sequences for two
	 * reasons (i) orders of magnitude performance difference
	 * (ii) allowing read-only replicas to be able to generate ids
	 */
	pg_atomic_uint64 nextTransactionNumber;

	BackendData backends[FLEXIBLE_ARRAY_MEMBER];
} BackendManagementShmemData;

/*
 * Each backend's active distributed transaction information is tracked via
 * BackendData in shared memory.
 */
typedef struct BackendData
{
	Oid databaseId;
	slock_t mutex;
	bool cancelledDueToDeadlock;
	DistributedTransactionId transactionId;
} BackendData;

/*
 * Citus identifies a distributed transaction with a triplet consisting of
 *
 *  -  initiatorNodeIdentifier: A unique identifier of the node that initiated
 *     the distributed transaction
 *  -  transactionOriginator: Set to true only for the transactions initialized on
 *     the coordinator. This is only useful for MX in order to distinguish the transaction
 *     that started the distributed transaction on the coordinator where we could
 *     have the same transactions' worker queries on the same node
 *  -  transactionNumber: A locally unique identifier assigned for the distributed
 *     transaction on the node that initiated the distributed transaction
 *  -  timestamp: The current timestamp of distributed transaction initiation
 *
 */
typedef struct DistributedTransactionId
{
	int initiatorNodeIdentifier;
	bool transactionOriginator;
	uint64 transactionNumber;
	TimestampTz timestamp;
} DistributedTransactionId;
```

backendManagementShmemData 为全局变量，结构体。在共享内存占有一段名为"Backend Management Shmem"的空间，用于存储与管理后端程序相关信息。

其中， backends 为 BackendData 结构体的数组，存储所有后端程序信息。以进程id为索引，数组元素通过互斥量mutex管理自己的事务。

Citus加载启动时，会初始化 BackendData 类型的全局变量 MyBackendData，关联到 backends 对应元素。

```c++
static BackendData *MyBackendData = NULL;

void InitializeBackendData(void)
{
	。。。。。。

	MyBackendData = &backendManagementShmemData->backends[MyProc->pgprocno];
	。。。。。。
}
```

创建事务时，将触发nextTransactionNumber递增，同时刷新 transactionId 的数据。事务提交、中断时，也将重置 transactionId 的数据。

Citus通过 RegisterXactCallback 向PgSQL 注册 CoordinatedTransactionCallback 函数。

![1552445314959](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1552445314959.png)



##### 3.1.1.3.2 WorkerNodeHash

```c++
static HTAB *WorkerNodeHash = NULL;

typedef struct WorkerNode
{
	uint32 nodeId;                      /* node's unique id, key of the hash table */
	uint32 workerPort;                  /* node's port */
	char workerName[WORKER_LENGTH];     /* node's name */
	uint32 groupId;                     /* node's groupId; same for the nodes that are in the same group */
	char workerRack[WORKER_LENGTH];     /* node's network location */
	bool hasMetadata;                   /* node gets metadata changes */
	bool isActive;                      /* node's state */
	Oid nodeRole;                       /* the node's role in its group */
	char nodeCluster[NAMEDATALEN];      /* the cluster the node is a part of */
} WorkerNode;
```

WorkerNodeHash 为全局变量，是一个名为"Work Node Hash"的Hash表。表元素为 WorkerNode 结构体，取自数据库pg_dist_node表数据。Hash表以结构体属性 workerName 与 workerPort为key。

```c++
/*
 * InitializeWorkerNodeCache initialize the infrastructure for the worker node cache.
 * The function reads the worker nodes from the metadata table, adds them to the hash and
 * finally registers an invalidation callback.
 */
static void
InitializeWorkerNodeCache(void)
{
	。。。。。。

	/*
	 * Create the hash that holds the worker nodes. The key is the combination of
	 * nodename and nodeport, instead of the unique nodeid because worker nodes are
	 * searched by the nodename and nodeport in every physical plan creation.
	 */
	memset(&info, 0, sizeof(info));
	info.keysize = +sizeof(uint32) + WORKER_LENGTH + sizeof(uint32);
	info.entrysize = sizeof(WorkerNode);
	info.hcxt = CacheMemoryContext;
	info.hash = WorkerNodeHashCode;
	info.match = WorkerNodeCompare;
	hashFlags = HASH_ELEM | HASH_FUNCTION | HASH_CONTEXT | HASH_COMPARE;

	newWorkerNodeHash = hash_create("Worker Node Hash", maxTableSize, &info, hashFlags);

	/* read the list from pg_dist_node */
	workerNodeList = ReadWorkerNodes(includeNodesFromOtherClusters);

	。。。。。。
	
	/* iterate over the worker node list */
	foreach(workerNodeCell, workerNodeList)
	{
		。。。。。。

		/* search for the worker node in the hash, and then insert the values */
		hashKey = (void *) currentNode;
		workerNode = (WorkerNode *) hash_search(newWorkerNodeHash, hashKey,
												HASH_ENTER, &handleFound);

		。。。。。。
	}
	。。。。。。
	
	WorkerNodeHash = newWorkerNodeHash;
}
```

WorkerNodeHash  表用于各种场景下程序获取Worker节点信息，常见于ActivePrimaryNodeList 获取active 的primary节点，ActiveReadableNodeList 获取active 的可读节点。

![1552448678908](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1552448678908.png)

##### 3.1.1.3.3 RemoteFileDestReceiver

```c++
typedef struct RemoteFileDestReceiver
{
	/* public DestReceiver interface */
	DestReceiver pub;

	char *resultId;

	/* descriptor of the tuples that are sent to the worker */
	TupleDesc tupleDescriptor;

	/* EState for per-tuple memory allocation */
	EState *executorState;

	/* MemoryContext for DestReceiver session */
	MemoryContext memoryContext;

	/* worker nodes to send data to */
	List *initialNodeList;
	List *connectionList;

	/* whether to write to a local file */
	bool writeLocalFile;
	File fileDesc;

	/* state on how to copy out data types */
	CopyOutState copyOutState;
	FmgrInfo *columnOutputFunctions;

	/* number of tuples sent */
	uint64 tuplesSent;
} RemoteFileDestReceiver;

/* ----------------
 *		DestReceiver is a base type for destination-specific local state.
 *		In the simplest cases, there is no state info, just the function
 *		pointers that the executor must call.
 *
 * Note: the receiveSlot routine must be passed a slot containing a TupleDesc
 * identical to the one given to the rStartup routine.  It returns bool where
 * a "true" value means "continue processing" and a "false" value means
 * "stop early, just as if we'd reached the end of the scan".
 * ----------------
 */
typedef struct _DestReceiver DestReceiver;

struct _DestReceiver
{
	/* Called for each tuple to be output: */
	bool		(*receiveSlot) (TupleTableSlot *slot,
								DestReceiver *self);
	/* Per-executor-run initialization and shutdown: */
	void		(*rStartup) (DestReceiver *self,
							 int operation,
							 TupleDesc typeinfo);
	void		(*rShutdown) (DestReceiver *self);
	/* Destroy the receiver object itself (if dynamically allocated) */
	void		(*rDestroy) (DestReceiver *self);
	/* CommandDest code for this receiver */
	CommandDest mydest;
	/* Private fields might appear beyond this point... */
};
```

RemoteFileDestReceiver 结构体集成 PgSQL公共结构体 （DestReceiver TupleDesc EState MemoryContext）的同时，还定义了多个属性，用于将结果数据传输给各个Worker节点。

![1552468122941](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1552468122941.png)

Citus通过 CreateRemoteFileDestReceiver 创建一个RemoteFileDestReceiver 变量，挂载DestReceiver 相关函数，支撑分布式计划中的子计划执行（ExecuteSubPlans）。

同时，Citus通过 PG_FUNCTION_INFO_V1 向PgSQL注册函数：broadcast_intermediate_result 与 create_intermediate_result。支持通过SQL语句调用这两个函数，基于函数入参创建RemoteFileDestReceiver 来执行相关指令并完成结果输出。

其中，属性 reaultId ，在ExecuteSubPlans 中为planId与subPlanId拼接而成的字符串"planId_subPlanId"，以标志记录数据的本地文件；在SQL函数中，为入参字符串。

##### 3.1.1.3.4  PortalHashTable

RemoteFileDestReceiver 作为结构体变量，只负责相关数据的存储，其钩子函数的工作需依赖外部的管理调度。

PgSQL的全局变量 PortalHashTable，名为"Portal hash"的Hash表，表元素为PortalData结构体。Hash表以结构体属性 name 为key。 

```c++
static HTAB *PortalHashTable = NULL;

typedef struct PortalData
{
	/* Bookkeeping data */
	const char *name;			/* portal's name */
	const char *prepStmtName;	/* source prepared statement (NULL if none) */
	MemoryContext heap;			/* subsidiary memory for portal */
	ResourceOwner resowner;		/* resources owned by portal */
	void		(*cleanup) (Portal portal); /* cleanup hook */

	/*
	 * State data for remembering which subtransaction(s) the portal was
	 * created or used in.  If the portal is held over from a previous
	 * transaction, both subxids are InvalidSubTransactionId.  Otherwise,
	 * createSubid is the creating subxact and activeSubid is the last subxact
	 * in which we ran the portal.
	 */
	SubTransactionId createSubid;	/* the creating subxact */
	SubTransactionId activeSubid;	/* the last subxact with activity */

	/* The query or queries the portal will execute */
	const char *sourceText;		/* text of query (as of 8.4, never NULL) */
	const char *commandTag;		/* command tag for original query */
	List	   *stmts;			/* list of PlannedStmts */
	CachedPlan *cplan;			/* CachedPlan, if stmts are from one */

	ParamListInfo portalParams; /* params to pass to query */
	QueryEnvironment *queryEnv; /* environment for query */

	/* Features/options */
	PortalStrategy strategy;	/* see above */
	int			cursorOptions;	/* DECLARE CURSOR option bits */
	bool		run_once;		/* portal will only be run once */

	/* Status data */
	PortalStatus status;		/* see above */
	bool		portalPinned;	/* a pinned portal can't be dropped */

	/* If not NULL, Executor is active; call ExecutorEnd eventually: */
	QueryDesc  *queryDesc;		/* info needed for executor invocation */

	/* If portal returns tuples, this is their tupdesc: */
	TupleDesc	tupDesc;		/* descriptor for result tuples */
	/* and these are the format codes to use for the columns: */
	int16	   *formats;		/* a format code for each column */

	/*
	 * Where we store tuples for a held cursor or a PORTAL_ONE_RETURNING or
	 * PORTAL_UTIL_SELECT query.  (A cursor held past the end of its
	 * transaction no longer has any active executor state.)
	 */
	Tuplestorestate *holdStore; /* store for holdable cursors */
	MemoryContext holdContext;	/* memory containing holdStore */

	/*
	 * Snapshot under which tuples in the holdStore were read.  We must keep a
	 * reference to this snapshot if there is any possibility that the tuples
	 * contain TOAST references, because releasing the snapshot could allow
	 * recently-dead rows to be vacuumed away, along with any toast data
	 * belonging to them.  In the case of a held cursor, we avoid needing to
	 * keep such a snapshot by forcibly detoasting the data.
	 */
	Snapshot	holdSnapshot;	/* registered snapshot, or NULL if none */

	/*
	 * atStart, atEnd and portalPos indicate the current cursor position.
	 * portalPos is zero before the first row, N after fetching N'th row of
	 * query.  After we run off the end, portalPos = # of rows in query, and
	 * atEnd is true.  Note that atStart implies portalPos == 0, but not the
	 * reverse: we might have backed up only as far as the first row, not to
	 * the start.  Also note that various code inspects atStart and atEnd, but
	 * only the portal movement routines should touch portalPos.
	 */
	bool		atStart;
	bool		atEnd;
	uint64		portalPos;

	/* Presentation data, primarily used by the pg_cursors system view */
	TimestampTz creation_time;	/* time at which this portal was defined */
	bool		visible;		/* include this portal in pg_cursors? */
}PortalData;


/* ----------------
 *		query descriptor:
 *
 *	a QueryDesc encapsulates everything that the executor
 *	needs to execute the query.
 *
 *	For the convenience of SQL-language functions, we also support QueryDescs
 *	containing utility statements; these must not be passed to the executor
 *	however.
 * ---------------------
 */
typedef struct QueryDesc
{
	/* These fields are provided by CreateQueryDesc */
	CmdType		operation;		/* CMD_SELECT, CMD_UPDATE, etc. */
	PlannedStmt *plannedstmt;	/* planner's output (could be utility, too) */
	const char *sourceText;		/* source text of the query */
	Snapshot	snapshot;		/* snapshot to use for query */
	Snapshot	crosscheck_snapshot;	/* crosscheck for RI update/delete */
	DestReceiver *dest;			/* the destination for tuple output */
	ParamListInfo params;		/* param values being passed in */
	QueryEnvironment *queryEnv; /* query environment passed in */
	int			instrument_options; /* OR of InstrumentOption flags */

	/* These fields are set by ExecutorStart */
	TupleDesc	tupDesc;		/* descriptor for result tuples */
	EState	   *estate;			/* executor's query-wide state */
	PlanState  *planstate;		/* tree of per-plan-node state */

	/* This field is set by ExecutorRun */
	bool		already_executed;	/* true if previously executed */

	/* This is always set NULL by the core system, but plugins can change it */
	struct Instrumentation *totaltime;	/* total time spent in ExecutorRun */
} QueryDesc;
```

Citus 通过 ExecutePlanIntoDestReceiver 调用一系列PG对外声明的Portal接口，借助Portal执行计划，并通过DestReceiver的钩子函数输出数据。

```c++
/*
 * ExecuteIntoDestReceiver plans and executes a query and sends results to the given
 * DestReceiver.
 */
void ExecutePlanIntoDestReceiver(PlannedStmt *queryPlan, ParamListInfo params,
							DestReceiver *dest)
{
	。。。。。。

	/* create a new portal for executing the query */
	//步骤1. 向 PortalHashTable 中插入一个元素
	portal = CreateNewPortal();

	//步骤2. 设置portal在pg_cursors表中不可见
	/* don't display the portal in pg_cursors, it is for internal use only */
	portal->visible = false;
	
	//步骤3. 配置portal的部分属性
	PortalDefineQuery(portal, NULL, "", "SELECT", list_make1(queryPlan), NULL);
	
	//步骤4. 根据queryPlan确定执行策略、进一步配置portal相关属性，并启动portal
	PortalStart(portal, params, eflags, GetActiveSnapshot());
	
	//步骤5. 以RemoteFileDestReceiver为入参，运行portal
#if (PG_VERSION_NUM >= 100000)
	PortalRun(portal, count, false, true, dest, dest, NULL);
#else
	PortalRun(portal, count, false, dest, dest, NULL);
#endif

	//步骤6. 关闭portal，并释放相关资源
	PortalDrop(portal, false);
}
```

由于PortalData结构体过于庞大，相关流程非常复杂，下图只展现部分上述代码流程中相关的关键数据。

name 值为"<unnamed portal x>"，其中x为无符号整型数，随CreateNewPortal调用自增。

![1552467343908](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1552467343908.png)



## 3.1.2 相关函数流程

#### 3.1.2.1 文件生成

- ##### PrepareMasterJobDirectory

```
-- 1. MasterJobDirectoryName
      1.1 基于jobId生成文件夹字符串"base/pgsql_job_cach/master_job_xxxx<jobId>"
   2.CitusCreateDirectory
      2.1 创建文件夹
   3. ResourceOwnerEnlargeJobDirectories
      3.1 通过 RegisterResourceReleaseCallback 注册文件夹释放回调函数 MultiResourceOwnerReleaseCallback
	  3.2 若 RegisteredJobDirectories 为空，则为之申请内存(容量 : 16)。若存在但空间已满，则扩容1倍
   4. ResourceOwnerRememberJobDirectory
      4.1 生成一个存储 owner 与 jobId 的元素，插入 RegisteredJobDirectories 数组(索引递增)
	  
multi_resowner::MultiResourceOwnerReleaseCallback
-- 1. 删除文件夹 "base/pgsql_job_cach/master_job_xxxx"
   2. 删除 RegisteredJobDirectories 中owner与jobId对应的元素(索引递减)
```



```
PgSQL
resowner::RegisterResourceReleaseCallback
-- 1. 生成一个存储 callback 与 arg 的元素，从头插入 ResourceRelease_callbacks 链表

ResourceOwnerReleaseInternal 遍历 ResourceRelease_callbacks，触发 MultiResourceOwnerReleaseCallback 以删除文件夹并释放相关资源
-- PREPARE | COMMIT | ABORT TRANSACTION 都将触发
```



#### 3.1.2.2 子计划执行

##### 3.1.2.2.1 入口函数

```c++
void ExecuteSubPlans(DistributedPlan *distributedPlan)
{
	uint64 planId = distributedPlan->planId;
	List *subPlanList = distributedPlan->subPlanList;
	。。。。。。
	
	//步骤1. 获取 WorkerNodeHash 表中所有可读的节点
	nodeList = ActiveReadableNodeList();
	
	//步骤2. 遍历 distributedPlan->subPlanList，按顺序执行子计划
	foreach(subPlanCell, subPlanList)
	{
		DistributedSubPlan *subPlan = (DistributedSubPlan *) lfirst(subPlanCell);
		PlannedStmt *plannedStmt = subPlan->plan;
		uint32 subPlanId = subPlan->subPlanId;
		DestReceiver *copyDest = NULL;
		ParamListInfo params = NULL;
		EState *estate = NULL;
		
		//2.1 根据planId, subPlanId生成一个 resultID（planId_subPlanId）以标识中间结果
		char *resultId = GenerateResultId(planId, subPlanId);

		SubPlanLevel++;
		
		//2.2 创建一个 EState 节点，并为之分配上下文内存
		estate = CreateExecutorState();
		
		//2.3 创建一个 DestReceiver，配置函数指针以及输出参数，用于将结果输出给相关Worker节点（nodeList)。writeLocalFile为false表示数据不保存到本地文件
		copyDest = (DestReceiver *) CreateRemoteFileDestReceiver(resultId, estate,
																 nodeList,
																 writeLocalFile);
		//2.4 创建一个临时 portal，利用DestReceiver(copyDest)执行计划(plannedStmt)并将结果传输给Worker节点
		ExecutePlanIntoDestReceiver(plannedStmt, params, copyDest);

		SubPlanLevel--;
		
		//2.5 释放EState 节点
		FreeExecutorState(estate);
	}
}
```

##### 3.1.2.2.2 重点函数

- ##### CreateRemoteFileDestReceiver


```c++
DestReceiver *CreateRemoteFileDestReceiver(char *resultId, EState *executorState,
							 List *initialNodeList, bool writeLocalFile)
{
	RemoteFileDestReceiver *resultDest = NULL;

	resultDest = (RemoteFileDestReceiver *) palloc0(sizeof(RemoteFileDestReceiver));

	/* set up the DestReceiver function pointers */
	//步骤1. 设置DestReceiver函数指针
	resultDest->pub.receiveSlot = RemoteFileDestReceiverReceive;
	resultDest->pub.rStartup = RemoteFileDestReceiverStartup;
	resultDest->pub.rShutdown = RemoteFileDestReceiverShutdown;
	resultDest->pub.rDestroy = RemoteFileDestReceiverDestroy;
	
	//步骤2. 设置DestReceiver输出模式：
	resultDest->pub.mydest = DestCopyOut;

	/* set up output parameters */
	//步骤3. 设置RemoteFileDestReceiver相关输出参数
	resultDest->resultId = resultId;
	resultDest->executorState = executorState;
	resultDest->initialNodeList = initialNodeList;
	resultDest->memoryContext = CurrentMemoryContext;
	resultDest->writeLocalFile = writeLocalFile;

	return (DestReceiver *) resultDest;
}
```



- #####  RemoteFileDestReceiverStartup


```c++
static void RemoteFileDestReceiverStartup(DestReceiver *dest, int operation,
							  TupleDesc inputTupleDescriptor)
{
	RemoteFileDestReceiver *resultDest = (RemoteFileDestReceiver *) dest;

	const char *resultId = resultDest->resultId;

	CopyOutState copyOutState = NULL;
	const char *delimiterCharacter = "\t";
	const char *nullPrintCharacter = "\\N";

	List *initialNodeList = resultDest->initialNodeList;
	ListCell *initialNodeCell = NULL;
	List *connectionList = NIL;
	ListCell *connectionCell = NULL;

	resultDest->tupleDescriptor = inputTupleDescriptor;

	/* define how tuples will be serialised */
	//步骤1. 配置元组数据构造规则、输出函数
	copyOutState = (CopyOutState) palloc0(sizeof(CopyOutStateData));
	copyOutState->delim = (char *) delimiterCharacter;
	copyOutState->null_print = (char *) nullPrintCharacter;
	copyOutState->null_print_client = (char *) nullPrintCharacter;
	copyOutState->binary = CanUseBinaryCopyFormat(inputTupleDescriptor);
	copyOutState->fe_msgbuf = makeStringInfo();
	copyOutState->rowcontext = GetPerTupleMemoryContext(resultDest->executorState);
	resultDest->copyOutState = copyOutState;

	resultDest->columnOutputFunctions = ColumnOutputFunctions(inputTupleDescriptor,
															  copyOutState->binary);

	/*
	 * Make sure that this transaction has a distributed transaction ID.
	 *
	 * Intermediate results will be stored in a directory that is derived from
	 * the distributed transaction ID across all workers and on the coordinator
	 * itself. Even if we only store results locally, we still want to assign
	 * a transaction ID in case we later store results on workers.
	 *
	 * When we start using broadcast_intermediate_result from workers, we
	 * need to make sure that we don't override the transaction ID here.
	 */
	 //步骤2. 识别当前进程是否存在coordinated事务(CurrentCoordinatedTransactionState为COORD_TRANS_STARTED)，若无则创建一个分布式事务
	BeginOrContinueCoordinatedTransaction();
	
	//步骤3. 若要求数据写入本地文件，则创建相应文件夹路径及文件
	if (resultDest->writeLocalFile)
	{
		const int fileFlags = (O_APPEND | O_CREAT | O_RDWR | O_TRUNC | PG_BINARY);
		const int fileMode = (S_IRUSR | S_IWUSR);
		const char *fileName = NULL;

		/* make sure the directory exists */
		/*
		3.1 若当前存在的事务为分布式，则创建文件夹如下：
		base/pgsql_job_cache/<userId>_<initiatorNodeIdentifier>_<transactionNumber>
		若为非分布式，则创建文件夹如下：
		base/pgsql_job_cache/<userId>_<processId>
		*/
		CreateIntermediateResultsDirectory();
		
		//3.2 校验resultId有效性，并生成文件绝对路径：步骤3.1文件夹路径/<resultId>.data
		fileName = QueryResultFileName(resultId);

		elog(DEBUG1, "writing to local file \"%s\"", fileName);
		
		//3.3 打开文件
		resultDest->fileDesc = FileOpenForTransmit(fileName, fileFlags, fileMode);
	}

	//步骤4. 遍历initialNodeList所有Worker节点，生成connectionList
	foreach(initialNodeCell, initialNodeList)
	{
		WorkerNode *workerNode = (WorkerNode *) lfirst(initialNodeCell);
		int connectionFlags = 0;
		char *nodeName = workerNode->workerName;
		int nodePort = workerNode->workerPort;
		MultiConnection *connection = NULL;
		
		//4.1 强制建立1条新连接到Worker节点(connectionFlags=0)
		connection = StartNodeConnection(connectionFlags, nodeName, nodePort);
		
		//4.2 标记连接状态：占用(claimedExclusively=true)
		ClaimConnectionExclusively(connection);
		
		//4.3 设置连接容错标记：事务出错则退出(remoteTransaction.transactionCritical=true)
		MarkRemoteTransactionCritical(connection);

		connectionList = lappend(connectionList, connection);
	}
	
	//步骤5. 遍历connectionList所有连接：poll/select等待（超时：200ms）连接建立，直到连接成功/失败/超时（5s）
	FinishConnectionListEstablishment(connectionList);

	/* must open transaction blocks to use intermediate results */
	/*步骤6. 通过connectionList，向连接对端的所有Worker节点发送开启远端事务指令
		6.1 遍历connectionList所有连接：
			1. 生成SQL语句
			BEGIN TRANSACTION ISOLATION LEVEL READ COMMITTED;
			SELECT assign_distributed_transaction_id(initiatorNodeIdentifier, transactionNumber, timestamp);
			-- 若存在子事务，则遍历所有子事务追加以下语句
			SAVEPOINT savepoint_<subId>;
			
			2. 通过PQsendQuery下发SQL语句
			
		6.2 通过IO等待事件阻塞直到所有连接可用
		6.3 遍历connectionList所有连接：通过PQgetResultSQL执行结果
	*/
	RemoteTransactionsBeginIfNecessary(connectionList);
	
	//步骤7. 遍历connectionList所有连接：构造SQL语句并下发到Worker节点
	foreach(connectionCell, connectionList)
	{
		MultiConnection *connection = (MultiConnection *) lfirst(connectionCell);
		StringInfo copyCommand = NULL;
		bool querySent = false;
		
		//7.1 根据resultId构造SQL语句：COPY "<resultId>"  FROM STDIN WITH (format result)
		copyCommand = ConstructCopyResultStatement(resultId);
		
		//7.2 调用PQsendQuery下发SQL语句
		querySent = SendRemoteCommand(connection, copyCommand->data);
		if (!querySent)
		{
			ReportConnectionError(connection, ERROR);
		}
	}
	
	//步骤8. 遍历connectionList所有连接：通过PQgetResult获取各个Worker节点SQL执行结果
	foreach(connectionCell, connectionList)
	{
		MultiConnection *connection = (MultiConnection *) lfirst(connectionCell);
		bool raiseInterrupts = true;

		PGresult *result = GetRemoteCommandResult(connection, raiseInterrupts);
		if (PQresultStatus(result) != PGRES_COPY_IN)
		{
			ReportResultError(connection, result, ERROR);
		}

		PQclear(result);
	}
	
	//步骤9. 若元组数据支持二进制拷贝，需做进一步操作
	if (copyOutState->binary)
	{
		/* send headers when using binary encoding */
		resetStringInfo(copyOutState->fe_msgbuf);
		
		//9.1 构造报文头："PGCOPY\n\377\r\n00\0"		
		AppendCopyBinaryHeaders(copyOutState);
		
		//9.2 遍历所有连接：通过PQputCopyData下发指令
		BroadcastCopyData(copyOutState->fe_msgbuf, connectionList);
		
		//9.3 若要求数据写到本地文件，则将步骤9.1报文写入步骤3.2生成的文件
		if (resultDest->writeLocalFile)
		{
			WriteToLocalFile(copyOutState->fe_msgbuf, resultDest->fileDesc);
		}
	}

	resultDest->connectionList = connectionList;
}
```



- ##### RemoteFileDestReceiverReceive


```c++
static bool RemoteFileDestReceiverReceive(TupleTableSlot *slot, DestReceiver *dest)
{
	RemoteFileDestReceiver *resultDest = (RemoteFileDestReceiver *) dest;

	TupleDesc tupleDescriptor = resultDest->tupleDescriptor;

	List *connectionList = resultDest->connectionList;
	CopyOutState copyOutState = resultDest->copyOutState;
	FmgrInfo *columnOutputFunctions = resultDest->columnOutputFunctions;

	Datum *columnValues = NULL;
	bool *columnNulls = NULL;
	StringInfo copyData = copyOutState->fe_msgbuf;

	EState *executorState = resultDest->executorState;
	MemoryContext executorTupleContext = GetPerTupleMemoryContext(executorState);
	MemoryContext oldContext = MemoryContextSwitchTo(executorTupleContext);

	slot_getallattrs(slot);

	columnValues = slot->tts_values;
	columnNulls = slot->tts_isnull;

	resetStringInfo(copyData);

	/* construct row in COPY format */
	AppendCopyRowData(columnValues, columnNulls, tupleDescriptor,
					  copyOutState, columnOutputFunctions, NULL);

	/* send row to nodes */
	BroadcastCopyData(copyData, connectionList);

	/* write to local file (if applicable) */
	if (resultDest->writeLocalFile)
	{
		WriteToLocalFile(copyOutState->fe_msgbuf, resultDest->fileDesc);
	}

	MemoryContextSwitchTo(oldContext);

	resultDest->tuplesSent++;

	ResetPerTupleExprContext(executorState);

	return true;
}
```



- #####  RemoteFileDestReceiverShutdown


```c++
static void RemoteFileDestReceiverShutdown(DestReceiver *destReceiver)
{
	RemoteFileDestReceiver *resultDest = (RemoteFileDestReceiver *) destReceiver;

	List *connectionList = resultDest->connectionList;
	CopyOutState copyOutState = resultDest->copyOutState;

	if (copyOutState->binary)
	{
		/* send footers when using binary encoding */
		resetStringInfo(copyOutState->fe_msgbuf);
		AppendCopyBinaryFooters(copyOutState);
		BroadcastCopyData(copyOutState->fe_msgbuf, connectionList);

		if (resultDest->writeLocalFile)
		{
			WriteToLocalFile(copyOutState->fe_msgbuf, resultDest->fileDesc);
		}
	}

	/* close the COPY input */
	EndRemoteCopy(0, connectionList, true);

	if (resultDest->writeLocalFile)
	{
		FileClose(resultDest->fileDesc);
	}
}
```

#### 3.1.2.3 执行器逻辑

##### 3.1.2.3.1 入口函数

- ##### MultiRealTimeExecute (multi_real_time_executor.c)

```
void MultiRealTimeExecute(Job *job)
1. 创建1个WaitInfo结构体变量waitInfo，存储此次任务执行使用的连接的pollfd 或 event相关数据  -- MultiClientCreateWaitInfo
2. 创建一个临时Hash表workerHash，名为"Worker node hash"。存储WorkerNodeHash中所有可读的Worker节点。
3. 为当前进程创建一个协调事务（若已存在则不重复创建）  -- BeginOrContinueCoordinatedTransaction
	3.1 判断全局变量CurrentCoordinatedTransactionState，若事务已启动（COORD_TRANS_STARTED）则退出
	3.2 设置全局变量CurrentCoordinatedTransactionState，标志事务启动（COORD_TRANS_STARTED）
	3.3 加自旋锁，刷新全局变量MyBackendData相关属性，释放锁。
	
4. 遍历job->taskList，为每一个任务初始化任务执行结构体，并存储到 taskExecutionList  -- InitTaskExecution
	4.1 创建1个TaskExecution结构体变量，基于 task 初始化相关属性
	4.2 遍历task->taskPlacementList，依次设置TaskExecution的数组元素（与placement一一对应）。其中，task状态为EXEC_TASK_CONNECT_START，transmit状态为EXEC_TRANSMIT_UNASSIGNED，connectionId为INVALID_CONNECTION_ID。

此后进入大循环，循环终止条件： 所有task执行完毕 / 某个执行失败 / 任务执行结果累计长度超阈值
5. MultiClientResetWaitInfo  -- 重置等待结构体waitInfo
6. 遍历job->taskList以及taskExecutionList，开启任务运作。
	6.1 LookupWorkerForTask  -- 在workerHash表中，查找任务关联的节点信息workerNodeState，不存在则报错
	6.2 若当前task准备运行（EXEC_TASK_CONNECT_START），但 Worker节点连接数（openConnectionCount） 或 Coordinator节点连接数（所有openConnectionCount之和） 达到最大值，则跳过，暂不执行任务，等待下一次循环
	6.3 ManageTaskExecution  -- 进入状态机，根据任务状态执行相关操作
	6.4 UpdateConnectionCounter  -- 刷新Worker节点的连接数，步骤6.3建连和断连动作都将触发
	6.5 TaskExecutionFailed  -- 当task出现致命错误（criticalErrorOccurred）或者失败次数（failureCount）超阈值，则记录taskId并终止循环
	6.6 TaskExecutionCompleted  -- 遍历task的状态数组，若某个节点状态为执行完毕（EXEC_TASK_DONE），则认为此task执行完毕，并累加completedTaskCount
	6.7 MultiClientRegisterWait  -- 若步骤6.6确认task未执行完毕，则将task使用的连接设置相关poll或select事件，并加入waitInfo的等待数组中
	
7. CheckIfSizeLimitIsExceeded  -- 检查任务执行结果累计长度是否超过阈值
8. MultiClientWait  -- 若还有未完成的task，则poll或select等待直至waitInfo中的连接被唤醒

退出大循环后，相关处理
9. MultiClientFreeWaitInfo  -- 释放连接等待结构体waitInfo
10. 遍历taskExecutionList，取消所有运行/拷贝状态的任务相关的连接上的请求  -- CancelTaskExecutionIfActive
11. 遍历taskExecutionList，关闭任务使用的连接，并释放内存  -- CleanupTaskExecution
12. ErrorSizeLimitIsExceeded  -- 若任务执行结果累计长度是否超过阈值，则记录错误日志
```

##### 3.1.2.3.2 重点函数

- ##### FindPlacementListConnection (metadata_cache.c)

```c++
static MultiConnection *FindPlacementListConnection(int flags, List *placementAccessList, const char *userName, List **placementEntryList)
{
	bool foundModifyingConnection = false;
	ListCell *placementAccessCell = NULL;
	MultiConnection *chosenConnection = NULL;

	/*
	 * Go through all placement accesses to find a suitable connection.
	 *
	 * If none of the placements have been accessed in this transaction, connection
	 * remains NULL.
	 *
	 * If one or more of the placements have been modified in this transaction, then
	 * use the connection that performed the write. If placements have been written
	 * over multiple connections or the connection is not available, error out.
	 *
	 * If placements have only been read in this transaction, then use the last
	 * suitable connection found for a placement in the placementAccessList.
	 */
	foreach(placementAccessCell, placementAccessList)
	{
		ShardPlacementAccess *placementAccess =
			(ShardPlacementAccess *) lfirst(placementAccessCell);
		ShardPlacement *placement = placementAccess->placement;
		ShardPlacementAccessType accessType = placementAccess->accessType;

		ConnectionPlacementHashEntry *placementEntry = NULL;
		ColocatedPlacementsHashEntry *colocatedEntry = NULL;
		ConnectionReference *placementConnection = NULL;

		if (placement->shardId == INVALID_SHARD_ID)
		{
			/*
			 * When a SELECT prunes down to 0 shard, we use a dummy placement.
			 * In that case, we can fall back to the default connection.
			 *
			 * FIXME: this can be removed if we evaluate empty SELECTs locally.
			 */
			continue;
		}

		placementEntry = FindOrCreatePlacementEntry(placement);
		colocatedEntry = placementEntry->colocatedEntry;
		placementConnection = placementEntry->primaryConnection;

		/* note: the Asserts below are primarily for clarifying the conditions */

		if (placementConnection->connection == NULL)
		{
			/* no connection has been chosen for the placement */
		}
		else if (accessType == PLACEMENT_ACCESS_DDL &&
				 placementEntry->hasSecondaryConnections)
		{
			/*
			 * If a placement has been read over multiple connections (typically as
			 * a result of a reference table join) then a DDL command on the placement
			 * would create a self-deadlock.
			 */

			Assert(placementConnection != NULL);

			ereport(ERROR,
					(errcode(ERRCODE_ACTIVE_SQL_TRANSACTION),
					 errmsg("cannot perform DDL on placement " UINT64_FORMAT
							", which has been read over multiple connections",
							placement->placementId)));
		}
		else if (accessType == PLACEMENT_ACCESS_DDL && colocatedEntry != NULL &&
				 colocatedEntry->hasSecondaryConnections)
		{
			/*
			 * If a placement has been read over multiple (uncommitted) connections
			 * then a DDL command on a co-located placement may create a self-deadlock
			 * if there exist some relationship between the co-located placements
			 * (e.g. foreign key, partitioning).
			 */

			Assert(placementConnection != NULL);

			ereport(ERROR,
					(errcode(ERRCODE_ACTIVE_SQL_TRANSACTION),
					 errmsg("cannot perform DDL on placement " UINT64_FORMAT
							" since a co-located placement has been read over multiple connections",
							placement->placementId)));
		}
		else if (foundModifyingConnection)
		{
			/*
			 * We already found a connection that performed writes on of the placements
			 * and must use it.
			 */

			if ((placementConnection->hadDDL || placementConnection->hadDML) &&
				placementConnection->connection != chosenConnection)
			{
				/*
				 * The current placement may have been modified over a different
				 * connection. Neither connection is guaranteed to see all uncomitted
				 * writes and therefore we cannot proceed.
				 */
				ereport(ERROR,
						(errcode(ERRCODE_ACTIVE_SQL_TRANSACTION),
						 errmsg("cannot perform query with placements that were "
								"modified over multiple connections")));
			}
		}
		else if (CanUseExistingConnection(flags, userName, placementConnection))
		{
			/*
			 * There is an existing connection for the placement and we can use it.
			 */

			Assert(placementConnection != NULL);

			chosenConnection = placementConnection->connection;

			if (placementConnection->hadDDL || placementConnection->hadDML)
			{
				/* this connection performed writes, we must use it */
				foundModifyingConnection = true;
			}
		}
		else if (placementConnection->hadDDL)
		{
			/*
			 * There is an existing connection, but we cannot use it and it executed
			 * DDL. Any subsequent operation needs to be able to see the results of
			 * the DDL command and thus cannot proceed if it cannot use the connection.
			 */

			Assert(placementConnection != NULL);
			Assert(!CanUseExistingConnection(flags, userName, placementConnection));

			ereport(ERROR,
					(errcode(ERRCODE_ACTIVE_SQL_TRANSACTION),
					 errmsg("cannot establish a new connection for "
							"placement " UINT64_FORMAT
							", since DDL has been executed on a connection that is in use",
							placement->placementId)));
		}
		else if (placementConnection->hadDML)
		{
			/*
			 * There is an existing connection, but we cannot use it and it executed
			 * DML. Any subsequent operation needs to be able to see the results of
			 * the DML command and thus cannot proceed if it cannot use the connection.
			 *
			 * Note that this is not meaningfully different from the previous case. We
			 * just produce a different error message based on whether DDL was or only
			 * DML was executed.
			 */

			Assert(placementConnection != NULL);
			Assert(!CanUseExistingConnection(flags, userName, placementConnection));
			Assert(!placementConnection->hadDDL);

			ereport(ERROR,
					(errcode(ERRCODE_ACTIVE_SQL_TRANSACTION),
					 errmsg("cannot establish a new connection for "
							"placement " UINT64_FORMAT
							", since DML has been executed on a connection that is in use",
							placement->placementId)));
		}
		else if (accessType == PLACEMENT_ACCESS_DDL)
		{
			/*
			 * There is an existing connection, but we cannot use it and we want to
			 * execute DDL. The operation on the existing connection might conflict
			 * with the DDL statement.
			 */

			Assert(placementConnection != NULL);
			Assert(!CanUseExistingConnection(flags, userName, placementConnection));
			Assert(!placementConnection->hadDDL);
			Assert(!placementConnection->hadDML);

			ereport(ERROR,
					(errcode(ERRCODE_ACTIVE_SQL_TRANSACTION),
					 errmsg("cannot perform a parallel DDL command because multiple "
							"placements have been accessed over the same connection")));
		}
		else
		{
			/*
			 * The placement has a connection assigned to it, but it cannot be used,
			 * most likely because it has been claimed exclusively. Fortunately, it
			 * has only been used for reads and we're not performing a DDL command.
			 * We can therefore use a different connection for this placement.
			 */

			Assert(placementConnection != NULL);
			Assert(!CanUseExistingConnection(flags, userName, placementConnection));
			Assert(!placementConnection->hadDDL);
			Assert(!placementConnection->hadDML);
			Assert(accessType != PLACEMENT_ACCESS_DDL);
		}

		*placementEntryList = lappend(*placementEntryList, placementEntry);
	}

	return chosenConnection;
}
```



- ##### StartNodeConnection (connection_management.c)

  ```c++
  MultiConnection *StartNodeConnection(uint32 flags, const char *hostname, int32 port)
  {
  	return StartNodeUserDatabaseConnection(flags, hostname, port, NULL, NULL);
  }
  ```

- ##### StartNodeUserDatabaseConnection (connection_management.c)

  ```c++
  MultiConnection *StartNodeUserDatabaseConnection(uint32 flags, const char *hostname, int32 port, const
  								char *user, const char *database)
  {
  	ConnectionHashKey key;
  	ConnectionHashEntry *entry = NULL;
  	MultiConnection *connection;
  	bool found;
  
  	/* do some minimal input checks */
  	strlcpy(key.hostname, hostname, MAX_NODE_LENGTH);
  	if (strlen(hostname) > MAX_NODE_LENGTH)
  	{
  		ereport(ERROR, (errcode(ERRCODE_INVALID_PARAMETER_VALUE),
  						errmsg("hostname exceeds the maximum length of %d",
  							   MAX_NODE_LENGTH)));
  	}
  
  	key.port = port;
  	if (user)
  	{
  		strlcpy(key.user, user, NAMEDATALEN);
  	}
  	else
  	{
  		strlcpy(key.user, CurrentUserName(), NAMEDATALEN);
  	}
  	if (database)
  	{
  		strlcpy(key.database, database, NAMEDATALEN);
  	}
  	else
  	{
  		strlcpy(key.database, get_database_name(MyDatabaseId), NAMEDATALEN);
  	}
  
  	if (CurrentCoordinatedTransactionState == COORD_TRANS_NONE)
  	{
  		CurrentCoordinatedTransactionState = COORD_TRANS_IDLE;
  	}
  
  	/*
  	 * Lookup relevant hash entry. We always enter. If only a cached
  	 * connection is desired, and there's none, we'll simply leave the
  	 * connection list empty.
  	 */
  
  	entry = hash_search(ConnectionHash, &key, HASH_ENTER, &found);
  	if (!found)
  	{
  		entry->connections = MemoryContextAlloc(ConnectionContext,
  												sizeof(dlist_head));
  		dlist_init(entry->connections);
  	}
  
  	/* if desired, check whether there's a usable connection */
  	if (!(flags & FORCE_NEW_CONNECTION))
  	{
  		/* check connection cache for a connection that's not already in use */
  		connection = FindAvailableConnection(entry->connections, flags);
  		if (connection)
  		{
  			if (flags & SESSION_LIFESPAN)
  			{
  				connection->sessionLifespan = true;
  			}
  
  			return connection;
  		}
  	}
  
  	/*
  	 * Either no caching desired, or no pre-established, non-claimed,
  	 * connection present. Initiate connection establishment.
  	 */
  	connection = StartConnectionEstablishment(&key);
  
  	dlist_push_tail(entry->connections, &connection->connectionNode);
  	ResetShardPlacementAssociation(connection);
  
  	if (flags & SESSION_LIFESPAN)
  	{
  		connection->sessionLifespan = true;
  	}
  
  	return connection;
  }
  ```

- ##### ManageTaskExecution (multi_real_time_executor.c)

```c
static ConnectAction ManageTaskExecution(Task *task, TaskExecution *taskExecution,
					TaskExecutionStatus *executionStatus,
					DistributedExecutionStats *executionStats)
		
1. EXEC_TASK_CONNECT_START：
	1.1 遍历task->relationShardList，为每个shard创建1个ShardPlacementAccess，并存储到placementAccessList中  -- BuildPlacementSelectList
		1.1.1 根据placement->groupId(指定Worker节点)与shardId，从全局变量DistShardCacheHash中获取对应的shard及placement信息，初始化1个ShardPlacement结构体变量placement
    	1.1.2 根据placement，初始化1个ShardPlacementAccess结构体变量（accessType为PLACEMENT_ACCESS_SELECT），存储到placementAccessList
    	
	1.2 获取1个连接  -- MultiClientPlacementConnectStart
		1.2.1 StartPlacementListConnection（connFlag为CONNECTION_PER_PLACEMENT）
		1.2.2 ClaimConnectionExclusively  -- 标记连接被占用(claimedExclusively=true)
		1.2.3 将连接保存到全局变量ClientConnectionArray中，对应全局变量ClientPollingStatusArray元素设置为PGRES_POLLING_WRITING。下标为connectionId
	
	1.3 将连接存储到TaskExecution的connectionIdArray，下标为currentNodeIndex。若连接有效，则刷新task状态为EXEC_TASK_CONNECT_POLL；反之，则累加失败计数(failureCount)，并累加currentNodeIndex。也即下次执行此task将切换到下一个placement
		
2. EXEC_TASK_CONNECT_POLL：

3. EXEC_TASK_FAILED:

4. EXEC_BEGIN_START:
	4.1 生成SQL语句：
		BEGIN TRANSACTION ISOLATION LEVEL READ COMMITTED;
		SELECT assign_distributed_transaction_id(initiatorNodeIdentifier,
				transactionNumber, timestamp);
		-- 存在子事务时
		SAVEPOINT savepoint_xxx(subId);
	4.2 下发 SQL语句
	
5. EXEC_BEGIN_RUNNING:
	5.1 通过 PQstatus 与 PQconsumeInput、PQisBusy 获取执行状态
	5.2 读空结果

6. EXEC_COMPUTE_TASK_START:
	6.1 生成SQL语句：COPY (task->queryString) TO STDOUT WITH (FORMAT binary) 
		或 COPY (task->queryString) TO STDOUT
	6.2 下发SQL语句
	
7. EXEC_COMPUTE_TASK_RUNNING:
	7.1 通过 PQstatus 与 PQconsumeInput、PQisBusy 获取执行状态以及结果状态
	7.2 若结果状态可用（CLIENT_QUERY_COPY），则在文件夹"base/pgsql_job_cach/master_job_xxxx<jobId>"下，创建task文件"task_xxxxxx<taskId>"
	
8. EXEC_COMPUTE_TASK_COPYING:
	通过 PQgetCopyData 拷贝数据到task文件中
	
9. EXEC_TASK_DONE: 结束
```

重点关注状态机中关于连接查询、分配的环节 (ManageTaskExecution : EXEC_TASK_CONNECT_START)。

```c++
搜索连接方式：
	1. 根据placementId，在全局ConnectionPlacementHash表中查找这个placement；若存在则到该placement的连接已找到，反之则在表中插入1条新数据。
	2. 若未找到且该placement分区方式为HASH/NONE，则根据该placement对应的Worker节点名、端口号、co-location组Id、representativeValue，在全局ColocatedPlacementsHash表中查找co-located placement；若存在则到该placement的连接为co-located placement的连接，反之则在表中插入1条新数据。
	3. 根据placement对应的shardId，在全局ConnectionShardHash表中查找分片，找不到则在表中插入1条新数据。并将该placement与分片关联。
	4. 若步骤1&2 未发现到placement的连接，或者该连接不满足复用条件1（见下文），则根据Worker节点的hostname、端口号、用户名、数据库名，在全局ConnectionHash表中查找到该placement对应的Worker节点的连接，找不到则在表中插入1条新数据。
			
复用连接场景：
	1. 存在到该placement的connnection，且该连接未被占用（claimedExclusively为false）且属于当前用户
	2. 不满足复用条件1，但存在到该分片所在Worker节点的connnection，用户名&数据库名匹配且未被占用（claimedExclusively为false），此时满足复用基本要求；建立新连接场景5与场景6的条件必须不成立，此连接才能真正被复用
			
报错场景：
	1. 到该placement的connnection不满足复用条件1，且该连接被DDL/DML占用（hadDDL/hadDML为true）
    2. 连接池中存在符合复用条件1的连接，曾/正 执行DDL/DML（hadDDL/hadDML为true）；但不是当前连接，且当前连接曾/正 执行DDL/DML（hadDDL/hadDML为true）

建立新连接场景：
	1. 到该placement的connnection不满足复用条件1，且不存在到该placement所在Worker节点的connnection
	2. 到该placement的connnection不满足复用条件1，且到该placement所在Worker节点的connnection 不满足复用条件2
	3. 不存在到该placement的connnection，且不存在到该placement所在Worker节点的connnection
	4. 不存在到该placement的connnection，到该placement所在Worker节点的connnection不满足复用条件2
	5. 不存在到该placement的connnection，到该placement所在Worker节点的connnection满足复用条件2，但当前placement不是distributed且该连接关联的placementId不是当前placementId
	6. 不存在到该placement的connnection，到该placement所在Worker节点的connnection满足复用条件2，但当前placement是distributed且该连接关联的是非co-located placement
			
连接分配后操作：
	1. 若到该placement的connnection不是分配的connnection，则将placementAccess的hasSecondaryConnections设置为true，且将其co-located placementAccess的hasSecondaryConnections设置为true
	2. 若到该placement的connnection不是分配的connnection，且该placement的placementAccess的accessType不是PLACEMENT_ACCESS_SELECT（也即当前placement涉及DDL/DML操作），则该connnection设置为分配的connection
	3. 若该placement的placementAccess的accessType为PLACEMENT_ACCESS_DDL，则为它分配的连接需设置hadDDL为true，表示该连接正在执行DDL。hadDML设置同理。也即若将在该placement关联的连接上进行DDL/DML操作，需要将对应标志位设为true
```



### 3.1.3 CN与WN交互

#### 3.1.3.1 集群配置

| 节点名 | 角色        | IP                   |
| ------ | ----------- | -------------------- |
| cn     | Coordinator | 192.168.221.130:5432 |
| wn1    | Worker      | 192.168.221.131:5432 |
| wn2    | Worker      | 192.168.221.132:5432 |
| client | 客户端      | 192.168.221.133:5432 |

#### 3.1.3.2 表结构

```shell
# 建表配置
create table test1(id int primary key, name int);
SELECT create_distributed_table('test1', 'id', 'hash');

SELECT master_create_worker_shards('test1', 2, 2);
insert into test1 select generate_series(1,20),1234;

# 表结构
postgres=# select * from pg_dist_shard;
 logicalrelid | shardid | shardstorage | shardminvalue | shardmaxvalue 
--------------+---------+--------------+---------------+---------------
 test1        |  102704 | t            | -2147483648   | -1
 test1        |  102705 | t            | 0             | 2147483647
(2 rows)

postgres=# select * from pg_dist_shard_placement;
 shardid | shardstate | shardlength |    nodename     | nodeport | placementid 
---------+------------+-------------+-----------------+----------+-------------
  102704 |          1 |           0 | 192.168.221.131 |     5432 |         723
  102704 |          1 |           0 | 192.168.221.132 |     5432 |         724
  102705 |          1 |           0 | 192.168.221.132 |     5432 |         725
  102705 |          1 |           0 | 192.168.221.131 |     5432 |         726
(4 rows)
```

#### 3.1.3.3 操作

以全局查询为例，在cn节点抓取报文了解，cn与wn及客户端之间的交互.

##### 3.1.3.3.1 查询计划

```
postgres=# explain analyze verbose select * from test1;
                                                             QUERY PLAN                                                             
------------------------------------------------------------------------------------------------------------------------------------
 Custom Scan (Citus Real-Time)  (cost=0.00..0.00 rows=0 width=0) (actual time=28.710..28.716 rows=20 loops=1)
   Output: remote_scan.id, remote_scan.name
   Task Count: 2
   Tasks Shown: All
   ->  Task
         Node: host=192.168.221.131 port=5432 dbname=postgres
         ->  Seq Scan on public.test1_102704 test1  (cost=0.00..32.60 rows=2260 width=8) (actual time=0.019..0.023 rows=13 loops=1)
               Output: id, name
             Planning time: 1.368 ms
             Execution time: 0.080 ms
   ->  Task
         Node: host=192.168.221.132 port=5432 dbname=postgres
         ->  Seq Scan on public.test1_102705 test1  (cost=0.00..32.60 rows=2260 width=8) (actual time=0.027..0.030 rows=7 loops=1)
               Output: id, name
             Planning time: 1.630 ms
             Execution time: 0.168 ms
 Planning time: 0.674 ms
 Execution time: 28.784 ms
(18 rows)
```

##### 3.1.3.3.2 查询结果

```shell
#cn 查询结果
postgres=# select * from test1;
 id | name 
----+------
  1 | 1234
  3 | 1234
  4 | 1234
  5 | 1234
  7 | 1234
  8 | 1234
 10 | 1234
 14 | 1234
 15 | 1234
 16 | 1234
 17 | 1234
 19 | 1234
 20 | 1234
  2 | 1234
  6 | 1234
  9 | 1234
 11 | 1234
 12 | 1234
 13 | 1234
 18 | 1234
(20 rows)

# wn1分片数据
postgres=# select * from test1_102704;
 id | name 
----+------
  1 | 1234
  3 | 1234
  4 | 1234
  5 | 1234
  7 | 1234
  8 | 1234
 10 | 1234
 14 | 1234
 15 | 1234
 16 | 1234
 17 | 1234
 19 | 1234
 20 | 1234
(13 rows)

# wn2分片数据
postgres=# select * from test1_102705;
 id | name 
----+------
  2 | 1234
  6 | 1234
  9 | 1234
 11 | 1234
 12 | 1234
 13 | 1234
 18 | 1234
(7 rows)
```

##### 3.1.3.3.3 交互报文

![selectall-realtime-pkg-1_1](C:\Users\Administrator\Desktop\citus\执行器\real-time\selectall-realtime-pkg-1\selectall-realtime-pkg-1_1.Jpeg)

![selectall-realtime-pkg-2_1](C:\Users\Administrator\Desktop\citus\执行器\real-time\selectall-realtime-pkg-2\selectall-realtime-pkg-2_1.Jpeg)



![selectall-realtime-pkg-3_1](C:\Users\Administrator\Desktop\citus\执行器\real-time\selectall-realtime-pkg-3\selectall-realtime-pkg-3_1.Jpeg)



![selectall-realtime-pkg-4_1](C:\Users\Administrator\Desktop\citus\执行器\real-time\selectall-realtime-pkg-4\selectall-realtime-pkg-4_1.Jpeg)



![selectall-realtime-pkg-5_1](C:\Users\Administrator\Desktop\citus\执行器\real-time\selectall-realtime-pkg-5\selectall-realtime-pkg-5_1.Jpeg)



![selectall-realtime-pkg-6_1](C:\Users\Administrator\Desktop\citus\执行器\real-time\selectall-realtime-pkg-6\selectall-realtime-pkg-6_1.Jpeg)



![selectall-realtime-pkg-7_1](C:\Users\Administrator\Desktop\citus\执行器\real-time\selectall-realtime-pkg-7\selectall-realtime-pkg-7_1.Jpeg)



## 3.2 Task-Tracker执行器

### 3.2.1 Task Tracker进程

```
1. shared_library_init::_PG_init初始化时，通过TaskTrackerRegister注册TaskTracker进程相关数据。Server启动时，通过调用注册函数TaskTrackerMain启动TaskTracker进程
2. TaskTrackerMain死循环监听Postmaster进程状态、以及相关信号量执行相应动作（终止TaskTracker进程等）。并调用ManageWorkerTasksHash处理相关任务事务后，通过pg_usleep休眠指定时间（以秒为单位，可配置）
3. ManageWorkerTasksHash：管理"Worker Task Hash"表（共享内存）中所有任务的计划安排与执行
	3.1 加锁（LW_SHARED），拷贝"Worker Task Hash"表中所有在执行计划内（状态：TASK_ASSIGNED）的任务生成schedulableTaskList，个数上限为MaxRunningTasksPerNode（可配置；默认16）
	3.2 加锁（LW_EXCLUSIVE），刷新schedulableTaskList表中任务的状态为TASK_SCHEDULED。遍历"Worker Task Hash"表，通过状态机ManageWorkerTask管理任务。若状态机处理后，任务状态为TASK_TO_REMOVE，则将该任务从"Worker Task Hash"表中删除
	
4. ManageWorkerTask：管理Worker任务的执行。状态机根据任务当前状态（taskStatus）执行相关动作
	4.1 TASK_ASSIGNED：退出函数
	4.2 TASK_SCHEDULED：
		4.2.1 CreateJobDirectoryIfNotExists
		4.2.2 以任务的dbName与userName为参数，强制建立1条新连接到本地后端。若建连成功，则将新的连接存储到ClientConnectionArray中；反之，则刷新task状态为TASK_FAILED，累计failureCount -- ConnectToLocalBackend
		4.2.3 通过新连接，发送SQL指令到本地后端（异步）。若发送成功，则刷新task状态为TASK_RUNNING；反之则为TASK_FAILED，累计failureCount、关闭当前连接
		
	4.3 TASK_RUNNING：
		4.3.1 校验异步状态。若异常（CLIENT_RESULT_UNAVAILABLE），则刷新task状态为TASK_FAILED，累计failureCount。若繁忙（CLIENT_RESULT_BUSY），则保持当前状态，等待下一次调度。若正常（CLIENT_RESULT_READY），则获取SQL执行的结果数据并做相关校验。
		4.3.2 若上步SQL结果校验失败（CLIENT_QUERY_FAILED），则刷新task状态为TASK_FAILED，累计failureCount。若校验成功（CLIENT_QUERY_DONE），则刷新task状态为TASK_SUCCEEDED。
		4.3.3 异步状态为异常/正常，都将关闭当前连接
	
	4.4 TASK_FAILED：若failureCount超过阈值，则刷新task状态为TASK_PERMANENTLY_FAILED。反之，则为TASK_ASSIGNED
	4.5 TASK_PERMANENTLY_FAILED 或 TASK_SUCCEEDED：退出函数
	4.6 TASK_CANCEL_REQUESTED：刷新task状态为TASK_CANCELED。若任务ID为JOB_CLEANUP_TASK_ID，则退出函数。反之，则校验异步状态，若状态繁忙（CLIENT_RESULT_BUSY）则通过当前连接发送取消SQL指令
	4.7 TASK_CANCELED：关闭任务所用连接，刷新task状态为TASK_TO_REMOVE
	4.8 TASK_TO_REMOVE：记录错误日志，退出函数
```

```
PG_FUNCTION_INFO_V1(task_tracker_assign_task);
PG_FUNCTION_INFO_V1(task_tracker_task_status);
PG_FUNCTION_INFO_V1(task_tracker_cleanup_job);
-- 挂载自定义函数到PgSQL

# task_tracker_assign_task
1. 校验Task Tracker进程状态；校验PG传递的taskCallString长度
2. 在PG缓存NAMESPACENAME中查找以"pg_merge_job_xxxx(jobId)"命名的schema数据，若不存在，则已当前用户身份创建并插入一条数据
3. 根据入参jobId与taskId，在"Worker Task Hash"表中查找任务。若存在则刷新任务数据，不存在则插入一条新数据

# task_tracker_task_status
1. 校验Task Tracker进程状态
2. 根据入参jobId与taskId，在"Worker Task Hash"表中查询任务状态

# task_tracker_cleanup_job
1. 根据PG传递的jobId，在"Worker Task Hash"表中查找任务。若该任务的连接不可用，则从表中删除该任务；反之则刷新task状态为TASK_CANCEL_REQUESTED
2. 根据PG传递的jobId，删除该任务相关目录"base/pgsql_job_cach/job_xxxx"，NAMESPACENAME中"pg_merge_job_xxxx"命名的schema及schema中的表
```

### 3.2.2 执行器函数

```
# Task-Tracker执行器函数 multi_task_tracker_executor::TaskTrackerExecScan
1. ContainsReadIntermediateResultFunction    -- 本执行器不支持复杂的子查询以及CTE
2. LockPartitionsInRelationList       -- 同2.3.2.2
3. PrepareMasterJobDirectory          -- 同2.3.2.2
4. MultiTaskTrackerExecute            -- 执行逻辑
5. LoadTuplesIntoTupleStore           -- 同2.3.2.2
	5.1 读取文件夹"base/pgsql_job_cach/master_job_xxxx"内容到tuplestorestate
6. ReturnTupleFromTuplestore          -- 同2.3.2.2
	6.1 标准化封装结果，用于返回给客户端
```

#### 3.2.2.1 PrepareMasterJobDirectory

```
Citus
multi_real_time_executor.c::PrepareMasterJobDirectory
-- 1. CitusCreateDirectory
      1.1 创建文件夹 "base/pgsql_job_cach/master_job_xxxx"，路径字符串由 MasterJobDirectoryName 生成
   2. ResourceOwnerEnlargeJobDirectories
      2.1 通过 RegisterResourceReleaseCallback 注册文件夹释放回调函数 MultiResourceOwnerReleaseCallback
	  2.2 若 RegisteredJobDirectories 为空，则为之申请内存(容量 : 16)。若存在但空间已满，则扩容1倍
   3. ResourceOwnerRememberJobDirectory
      3.1 生成一个存储 owner 与 jobId 的元素，插入 RegisteredJobDirectories 数组(索引递增)
	  
multi_resowner::MultiResourceOwnerReleaseCallback
-- 1. 删除文件夹 "base/pgsql_job_cach/master_job_xxxx"
   2. 删除 RegisteredJobDirectories 中owner与jobId对应的元素(索引递减)
   

PgSQL
resowner::RegisterResourceReleaseCallback
-- 1. 生成一个存储 callback 与 arg 的元素，从头插入 ResourceRelease_callbacks 链表

ResourceOwnerReleaseInternal 遍历 ResourceRelease_callbacks，触发 MultiResourceOwnerReleaseCallback 以删除文件夹并释放相关资源
-- PREPARE | COMMIT | ABORT TRANSACTION 都将触发
```



#### 3.2.2.2 执行器执行逻辑

```
# 重点函数：MultiTaskTrackerExecute
1. 获取 Citus插件用户名，限制用户配置属性ReadFromSecondaries不能为USE_SECONDARY_NODES_ALWAYS
2. 基于任务树TaskList，生成一个关联了执行结构体的任务的taskAndExecutionList。创建一个"Task Hash"的Hash表。遍历TaskList的每一个任务，初始化它的任务执行结构体，并将任务插入taskAndExecutionList。在Hash表中查询任务的依赖任务，若不存在则插入，并将此任务的依赖任务刷新为Hash表中的任务。  -- TaskAndExecutionList
3. 统计任务树TaskList中顶级任务（SQL_TASK 类型）的数量
4. 返回一个workerNodeList，存储了WorkerNodeHash表（pg_dist_node表中当前集群所有Worker节点）中角色为Primary或未设置的所有有效节点。  -- ActivePrimaryNodeList
5. 创建一个"Task Tracker Hash"表（nodeName&port为key），存储workerNodeList中的Worker节点的相关信息（节点名、端口号、用户名（空）、状态、连接ID等），并为每一个Worker节点创建一个"Task State Hash"表（jobID&taskID为key）；一个"Transmit Tracker Hash"表，数据与"Task Tracker Hash"相同（除了用户名为Citus用户），以及每一个Worker节点对应的"Task State Hash"表。  -- TrackerHash

6. 通过状态机为所有Tracker完成连接分配。遍历"Task Tracker Hash"表元素，直到所有tracker连接分配成功（TRACKER_CONNECTED）/失败（TRACKER_CONNECTION_FAILED）  -- TrackerHashConnect
	6.1 Tracker状态为TRACKER_CONNECT_START（初始值）：连接标志为FORCE_NEW_CONNECTION。也即强制创建（异步）一条新连接到Worker节点。
	6.1.1 若该连接状态异常，则记录错误日志、关闭连接，Tracker状态刷新为TRACKER_CONNECTION_FAILED。
	6.1.2 若正常，则将它存储到ClientConnectionArray中，并在ClientPollingStatusArray中设置此连接对应的值为PGRES_POLLING_WRITING。刷新Tracker的connectionId为此连接ID，Tracker状态刷新为TRACKER_CONNECT_POLL
	
	6.2 Tracker状态为TRACKER_CONNECT_POLL：通过poll获取该连接状态，并据此刷新Tracker状态，connectPollCount相应累加。
	6.2.1 根据connectionId从ClientPollingStatusArray中获取当前连接状态。若为PGRES_POLLING_OK，则Tracker状态刷新为CONNECTED。若为PGRES_POLLING_FAILED，则记录错误日志、关闭连接，Tracker的connectionId重置、状态刷新为FAILED
	6.2.2 若为其他状态，则根据当前状态设置poll参数，等待poll结果；若结果为连接Ready，则刷新ClientPollingStatusArray中对应连接的状态。若connectPollCount超过阈值，则关闭连接，Tracker的connectionId重置、状态刷新为FAILED；反之则不刷新Tracker状态，等待下一次循环。
	
	6.3 Tracker状态为TRACKER_CONNECTED/TRACKER_CONNECTION_FAILED：connectPollCount归0

7. 遍历"Transmit Tracker Hash"表元素，同步骤6操作

此后进入大循环，循环终止条件： 所有task执行完毕 / 某个执行失败 / 过半Tracker状态异常 / 任务执行结果累计长度超阈值
8. 遍历taskAndExecutionList的元素，管理taskExecution逻辑
	8.1. 在"Task Tracker Hash"表中找到对应的任务，以及该任务的依赖任务  -- ResolveTaskTracker ResolveMapTaskTracker
	8.2. ManageTaskExecution  -- task执行逻辑管理
	8.3. 步骤8.2出错执行出错识别与处理流程
	
9. 遍历taskAndExecutionList中所有顶级任务（SQL_TASK 类型），获取任务执行结果
	9.1 在"Transmit Tracker Hash"表中找到对应的任务  -- ResolveTaskTracker
	9.2 ManageTransmitExecution  -- task执行结果数据获取管理
	9.3 步骤9.2执行出错识别与处理
	9.4 TransmitExecutionCompleted
	
10. 检测任务执行结果累计长度是否超阈值，若是则终止任务  -- CheckIfSizeLimitIsExceeded

11. 遍历"Task Tracker Hash"表元素，管理tracker执行逻辑
	11.1 判断当前Tracker状态。若状态正常，则healthyTrackerCount累加。
	11.2 ManageTaskTracker
	
12. 遍历"Transmit Tracker Hash"表元素，管理transmit执行逻辑
	12.1 ManageTransmitTracker

退出循环，进行扫尾工作：清理释放各种资源。
13. 生成一个存储所有jobId以及job依赖的jobId的链表JobIdList  -- JobIdList
14. TrackerCleanupResources
```

#### 3.2.2.3 ManageTaskExecution

```
# 通过状态机管理taskExecution逻辑
1. Execution状态为EXEC_TASK_UNASSIGNED（初始值）：
	1.1 判断当前Tracker状态。若trackerFailureCount或者connectionFailureCount超过阈值，则刷新状态为TRACKER_FAILED
	1.2 判断此任务的依赖任务是否全部执行完毕（EXEC_TASK_DONE）。若未完成，则不刷新状态，退出状态机
	1.3 反之，若依赖任务全部执行完毕；当前任务类型为MAP_OUTPUT_FETCH_TASK，则根据它自身以及所依赖的任务的相关数据生成一条SELECT语句，刷新为该任务的queryString。根据任务的taskType以及queryString生成一条SELECT语句。从"Task State Hash"表查找当前任务的状态结构体，若不存在则插入。状态结构体的status刷新为TASK_CLIENT_SIDE_QUEUED，taskAssignmentQuery刷新为刚生成的SELECT语句。刷新Execution状态为QUEUED

2. Execution状态为EXEC_TASK_QUEUED：
	2.1 同1.1
	2.2 查询"Task State Hash"表中当前任务的状态，据此刷新Execution状态。若为SUCCEEDED则刷新为DONE。若为TASK_CLIENT_SIDE_ASSIGN_FAILED或TASK_CLIENT_SIDE_STATUS_FAILED则刷新为TRACKER_RETRY。若为TASK_PERMANENTLY_FAILED，若当前任务为MAP_OUTPUT_FETCH_TASK则刷新为SOURCE_TASK_TRACKER_RETRY，其他则刷新为TRACKER_FAILED。其他状态时，则不刷新，等待下一次循环

3. Execution状态为EXEC_TASK_TRACKER_RETRY:
	3.1 若当前tracker已分配连接（TRACKER_CONNECTED），且连接当前状态可用，则累加trackerFailureCount
	3.2 判断当前Tracker状态。若trackerFailureCount或者connectionFailureCount超过阈值，则刷新状态为TRACKER_FAILED。反之，则从"Task State Hash"表获取当前任务的状态，据此刷新Execution状态。若TASK_CLIENT_SIDE_ASSIGN_FAILED，则刷新为UNASSIGNED；反之则刷新为QUEUED

4. Execution状态为EXEC_SOURCE_TASK_TRACKER_RETRY:

5. Execution状态为EXEC_TASK_TRACKER_FAILED/EXEC_SOURCE_TASK_TRACKER_FAILED:
	重置Execution状态为EXEC_TASK_UNASSIGNED
6. Execution状态为EXEC_TASK_DONE:
	无动作
```

#### 3.2.2.4 ManageTransmitExecution

```
# 通过状态机管理transmitExecution逻辑
```

#### 3.2.2.5 ManageTaskTracker

```
# 管理taskTracker逻辑
1. 判断当前Tracker状态。若trackerFailureCount或者connectionFailureCount超过阈值，则退出函数
2. 若当前Tracker未分配连接，或连接状态不可用，则重新连接（异步）
	2.1 根据当前Tracker状态进行一系列操作，若状态为TRACKER_CONNECTED，则获取当前连接的状态。若连接状态异常，则刷新Tracker状态为TRACKER_CONNECTION_FAILED，并关闭连接。
	2.2 若状态为TRACKER_CONNECT_START或TRACKER_CONNECT_POLL，则根据状态重新建立1条连接 或 通过poll刷新当前连接以及Tracker数据
	2.3 若状态为TRACKER_CONNECTION_FAILED，则对Tracker的connectionFailureCount累加，connectPollCount归0，状态重置为TRACKER_CONNECT_START

3. 若当前Tracker连接空闲（connectionBusy为false），则下发SQL语句，将所有获取执行结果成功的"Task State Hash"表元素存储到Tracker的assignedTaskList（指针）
	3.1 遍历"Task State Hash"表，找出所有TASK_CLIENT_SIDE_QUEUED的元素存储到tasksToAssignList，并将所有元素的taskAssignmentQuery拼接成1条语句
	3.2 若tasksToAssignList中有元素，则通过当前Tracker连接，下发步骤3.1生成的语句
	3.3 若步骤3.2下发失败，则将tasksToAssignList表元素状态刷新为TASK_CLIENT_SIDE_ASSIGN_FAILED。
	3.4 若步骤3.2下发成功，则遍历tasksToAssignList表元素，获取执行结果，将所有获取结果成功的元素（状态刷新为TASK_ASSIGNED）存储到assignedTaskList。若获取结果失败，则将后续所有元素状态刷新为TASK_CLIENT_SIDE_ASSIGN_FAILED。
	3.5 再一次获取执行结果，确保结果被完全读取

4. 若当前Tracker连接空闲（connectionBusy为false）
	4.1 遍历Tracker的assignedTaskList表元素，将SQL执行完毕（成功/失败）且排在当前Tracker之后的元素存储到taskStatusBatchList（指针），最多MaxTaskStatusBatchSize个元素
	4.2 若步骤4.1获取的taskStatusBatchList不为空，则根据此表中所有元素数据，拼接成1条SELECT语句；并通过当前Tracker连接，下发该SQL语句
	4.3 若发送成功，则设置Tracker的connectionBusy为true，connectionBusyOnTaskList为步骤4.1获取的taskStatusBatchList。若失败，则将taskStatusBatchList第一个元素状态刷新为TASK_CLIENT_SIDE_STATUS_FAILED，重置Tracker的connectionBusy、connectionBusyOnTaskList

5. 若当前Tracker连接正忙（connectionBusy为true）
	5.1 查询结果状态，若不可用（CLIENT_RESULT_UNAVAILABLE），则将connectionBusyOnTaskList第一个元素状态刷新为TASK_CLIENT_SIDE_STATUS_FAILED。
	5.2 若步骤5.1状态为准备完毕（CLIENT_RESULT_READY），则遍历Tracker的connectionBusyOnTaskList表元素，读取执行结果。根据读取状态刷新元素状态，读取异常则终止遍历
	5.3 若步骤5.1状态为不可用 或 准备完毕，则重置Tracker的connectionBusy、connectionBusyOnTaskList
```

#### 3.2.2.6 ManageTransmitTracker

```
# 管理transmitTracker逻辑
```

#### 3.2.2.7 TrackerCleanupResources

```
# 清除与本次SQL指令相关的本地与远端资源
1. 遍历"Task Tracker Hash"表，等待所有正在执行的请求执行完毕  -- TrackerHashWaitActiveRequest
2. 遍历"Transmit Tracker Hash"表，取消所有正在执行的请求  -- TrackerHashCancelActiveRequest
3. 遍历taskAndExecutionList表，关闭所有连接以及文件描述符，释放表元素  -- CleanupTaskExecution
4. 遍历jobIdList表，为每个job生成一个特殊任务来清理Worker节点上所有与该job相关的资源，并将这个任务发送给"Task Tracker Hash"表中所有Tracker，并等待清理任务执行 完毕 或 超时  -- JobCleanupTask TrackerHashCleanupJob
5. 遍历"Task Tracker Hash"表，关闭所有连接  -- TrackerHashDisconnect
6. 遍历"Transmit Tracker Hash"表，关闭所有连接  -- TrackerHashDisconnect
```



## 3.3 Router执行器

### 3.3.1 查询操作（只涉及1个分片）

```
# Router执行器函数 multi_router_executor::RouterSelectExecScan
1. LockPartitionsInRelationList      -- 同2.3.2.2
2. ExecuteSubPlans                   -- 同2.3.2.2
3. ExecuteSingleSelectTask           -- 执行逻辑
4. ReturnTupleFromTuplestore         -- 同2.3.2.2

# 重点函数：ExecuteSingleSelectTask     -- 遍历任务关联的placement（task->taskPlacementList），直到任务执行成功/所有副本执行任务失败
1. BuildPlacementSelectList  -- 生成1个placementAccessList，存储ShardPlacementAccess结构体（accessType为PLACEMENT_ACCESS_SELECT）。与分片（task->relationShardList）对应taskPlacement->groupId的placement（DistShardCacheHash表） 一一对应

2. GetPlacementListConnection  -- 获取placement的连接
	2.1 StartPlacementListConnection
	2.2 FinishConnectionEstablishment
	
3. RemoteTransactionBeginIfNecessary  -- 封装SQL指令，通知connection对端Worker节点执行远端事务，并等待执行完毕
	3.1 StartRemoteTransactionBegin  -- 遍历connectionList
		3.1.1 生成SQL语句：
		BEGIN TRANSACTION ISOLATION LEVEL READ COMMITTED;
		SELECT assign_distributed_transaction_id(initiatorNodeIdentifier,
				transactionNumber, timestamp);
		-- 存在子事务时
		SAVEPOINT savepoint_xxx(subId);
	
		3.1.2 下发SQL语句
	3.2 WaitForAllConnections
	
	3.3 FinishRemoteTransactionBegin  -- 遍历connectionList
	
4. SendQueryInSingleRowMode  -- 设置结果返回模式为single-row。
	4.1 通过connection下发具体任务的sql指令到Worker节点，若查询语句包含条件，则解析后下发（task->queryString）
	4.2 通过PQsetSingleRowMode设置结果返回模式
	
5. StoreQueryResult  -- 读取并保存执行结果
6. CheckIfSizeLimitIsExceeded
```



### 3.3.2 修改操作（只涉及1个分片）

```
# Router执行器准备函数 multi_router_executor::CitusModifyBeginScan
1. 深度拷贝当前分布式计划
2. 若Worker节点任务依赖主节点，在Coordinate节点，计算任务List涉及的所有查询以及join树的所有子表达式（不包含Vars的）  -- ExecuteMasterEvaluableFunctions
3. RouterInsertTaskList
4. RaiseDeferredError
5. RebuildQueryStrings
6. 为所有相关的分片上锁（ShareLock）  -- AcquireMetadataLocks
7. 为所有相关的分割表上锁（AccessShareLock） -- LockPartitionsInRelationList
8. FirstReplicaAssignTaskList

# Router执行器执行函数 multi_router_executor::RouterSequentialModifyExecScan
1. BeginOrContinueCoordinatedTransaction  -- 同2.3.2.2.1
2. ExecuteSingleModifyTask                -- 执行逻辑
3. ReturnTupleFromTuplestore              -- 同2.3.2.2

# 重点函数：ExecuteSingleModifyTask
1. LoadShardInterval
2. BeginOrContinueCoordinatedTransaction  -- 同2.3.2.2.1
3. CoordinatedTransactionUse2PC
4. 连接标志为SESSION_LIFESPAN|FOR_DML。根据组ID以及任务关联的分片List，找出相关的所有分片副本，为每个副本创建2个通道实体（Access标志分别为PLACEMENT_ACCESS_SELECT，PLACEMENT_ACCESS_DML），插入通道List。对于分片副本的PLACEMENT_ACCESS_DML通道实体，若分配的是被DDL/DML占用的Connection分配则直接报错；反之则设置当前通道被DML占用。  -- GetModifyConnections->StartPlacementListConnection
5. LockPartitionRelations
6. AcquireExecutorShardLock
7. SendQueryInSingleRowMode
8. StoreQueryResult ConsumeQueryResult
9. MarkFailedShardPlacements
```



### 3.3.3 修改操作（涉及多个分片）

```
# 多行插入 或 线性顺序连接（初始值：并行连接，可通过设置citus.multi_shard_modify_mode改变）的删改
# Router执行器准备函数 multi_router_executor::CitusModifyBeginScan
同2.3.4.2

# Router执行器执行函数 multi_router_executor::RouterSequentialModifyExecScan
同2.3.4.2
```



### 3.3.4 修改操作（不符合2.3.2.4.3的场景）

```
# Router执行器函数 multi_router_executor::RouterMultiModifyExecScan
1. BeginOrContinueCoordinatedTransaction
2. ExecuteMultipleTasks
3. ReturnTupleFromTuplestore

# 重点函数 ExecuteMultipleTasks -> ExecuteModifyTasks
1. LoadShardInterval
2. LockPartitionRelations
3. AcquireExecutorMultiShardLocks
4. BeginOrContinueCoordinatedTransaction
5. CoordinatedTransactionUse2PC

6. 遍历所有任务，为涉及的每一个分片placement分配连接。  -- OpenTransactionsForAllTasks
6.1 根据任务类型(是否为DDL)确定连接标志为CONNECTION_PER_PLACEMENT|FOR_DDL 或 CONNECTION_PER_PLACEMENT|FOR_DML。
6.2 创建一个"Shard Connections Hash"的Hash表，用于存储到分片的连接
6.3 从上述Hash表中获取到指定分片的连接。若没有，则找出所有相关分片处于finalized状态的placement(以List形式存储)
6.4 获取副本对应的Worker节点信息，并为副本创建1个通道实体，Access标志为PLACEMENT_ACCESS_DML/PLACEMENT_ACCESS_DML（根据任务类型决定），插入通道List
6.5 根据组ID以及任务关联的分片List，找出相关的所有分片副本，为每个副本创建1个通道实体，Access标志为PLACEMENT_ACCESS_SELECT，插入通道List
6.6 筛选（新建&复用&报错）到分片副本的Connection以及到副本所在Worker节点的Connection的标准同2.3.3.4.2。若复用的连接满足Real-Time重新建联场景V或者VI，则需重新建立一条到Worker节点的新连接并关联当前的副本
6.7 标记当前连接为被占用
6.8 为connection设置标记：远端事务失败将导致整个协调事务失败，并等待连接建立完毕
6.9 若提交协议为1PC/2PC，则封装sql指令，通知connectionList中所有Worker节点执行远端事务

7. 从"Shard Connections Hash"的Hash表中获取到分片的连接，用于下发sql命令。 -- GetShardHashConnections
8. SendQueryInSingleRowMode
9. 从"Shard Connections Hash"的Hash表中获取到分片的连接，用于获取sql执行结果。 -- GetShardHashConnections
10. StoreQueryResult ConsumeQueryResult
11. 将"Shard Connections Hash"的Hash表中所有连接状态设置为未占用。  -- UnclaimAllShardConnections
```



# 4. 执行器流程梳理小结

## 4.1 Real-Time执行器

```
1. 为分片表的每一个分片上锁（AccessShareLock）
2. 在Coordinator上创建文件夹用于保存任务执行结果
3. 按顺序依次执行分布计划的每一个子计划：为子计划相关的每一个Worker节点创建一个新的连接，执行完事务后，关闭连接
4. 具体执行逻辑：循环直到所有任务执行完毕/某任务执行失败。
	4.1. 当Coordinator 或 对应的Worker节点Connection数量达到阈值是，持续等待，直到Coordinator 以及 对应的Worker节点的connection数量降下来；
	4.2. 任务进入状态机，根据所处状态执行相关操作。
		4.2.1. 初始化placement的EntryAccess及相关结构体数据。accessType为PLACEMENT_ACCESS_SELECT，connectionFlag为CONNECTION_PER_PLACEMENT
		4.2.2. 获取连接，并将连接标记为占用（claimedExclusively为true）。
			搜索连接方式：以下涉及的Hash表中数据只在事务commit时清空（PG源码入口很多）
			1. 根据placementId，在ConnectionPlacementHash表中查找这个placement；若存在则到该placement的连接已找到，反之则在表中插入1条新数据。
			2. 若未找到且该placement分区方式为HASH/NONE，则根据该placement对应的Worker节点名、端口号、co-location组Id、representativeValue，在ColocatedPlacementsHash表中查找co-located placement；若存在则到该placement的连接为co-located placement的连接，反之则在表中插入1条新数据。
			3. 根据placement对应的shardId，在ConnectionShardHash表中查找分片，找不到则在表中插入1条新数据。并将该placement与分片关联。
			4. 若步骤1&2 未发现到placement的连接，或者该连接不满足复用条件1（见下文），则根据Worker节点的hostname、端口号、用户名、数据库名，在ConnectionHash表中查找到该placement对应的Worker节点的连接，找不到则在表中插入1条新数据。
			
		   复用连接场景：
			1. 存在到该placement的connnection，且该连接未被占用（claimedExclusively为false）且属于当前用户
			2. 不满足复用条件1，但存在到该分片所在Worker节点的connnection，用户名&数据库名匹配且未被占用（claimedExclusively为false），此时满足复用基本要求；建立新连接场景5与场景6的条件必须不成立，此连接才能真正被复用
			
		   报错场景：
			1. 到该placement的connnection不满足复用条件1，且该连接被DDL/DML占用（hadDDL/hadDML为true）
		    2. 连接池中存在符合复用条件1的连接，曾/正 执行DDL/DML（hadDDL/hadDML为true）；但不是当前连接，且当前连接曾/正 执行DDL/DML（hadDDL/hadDML为true）

		   建立新连接场景：
			1. 到该placement的connnection不满足复用条件1，且不存在到该placement所在Worker节点的connnection
			2. 到该placement的connnection不满足复用条件1，且到该placement所在Worker节点的connnection 不满足复用条件2
			3. 不存在到该placement的connnection，且不存在到该placement所在Worker节点的connnection
			4. 不存在到该placement的connnection，到该placement所在Worker节点的connnection不满足复用条件2
			5. 不存在到该placement的connnection，到该placement所在Worker节点的connnection满足复用条件2，但当前placement不是distributed且该连接关联的placementId不是当前placementId
			6. 不存在到该placement的connnection，到该placement所在Worker节点的connnection满足复用条件2，但当前placement是distributed且该连接关联的是非co-located placement
			
		   连接分配后操作：
			1. 若到该placement的connnection不是分配的connnection，则将placementAccess的hasSecondaryConnections设置为true，且将其co-located placementAccess的hasSecondaryConnections设置为true
			2. 若到该placement的connnection不是分配的connnection，且该placement的placementAccess的accessType不是PLACEMENT_ACCESS_SELECT（也即当前placement涉及DDL/DML操作），则该connnection设置为分配的connection
			3. 若该placement的placementAccess的accessType为PLACEMENT_ACCESS_DDL，则为它分配的连接需设置hadDDL为true，表示该连接正在执行DDL。hadDML设置同理。也即若将在该placement关联的连接上进行DDL/DML操作，需要将对应标志位设为true
			
		4.2.3. 等待连接建立，下发sql指令到Worker节点开启远端事务
		4.2.4. 等待收到Worker节点事务开启的响应，下发具体任务的sql指令到Worker节点
		4.2.5. 等待收到Worker节点执行完毕的响应，从Worker节点拷贝数据到本地文件，完毕后该任务执行完毕
		4.2.6. 关闭连接
5. 将文件中的执行结果读取出来，加载到指定元组
6. 逐条返回元组中存储的执行结果
```



## 4.2 Router执行器

```
整体流程与Real-Time执行器类似，只有具体执行逻辑存在差异，直接影响了连接复用相关。
具体执行逻辑：遍历任务关联的所有placement，直到任务执行成功/所有placement执行任务失败。
1. 初始化placement相关结构体数据，不同操作场景有一定差异。
	a. 单分片查询操作：为每个placement创建1个placementAccess。accessType为PLACEMENT_ACCESS_SELECT，connectionFlag为SESSION_LIFESPAN
	b. 单分片修改操作：为每个placement先后创建2个placementAccess。accessType分别为PLACEMENT_ACCESS_SELECT，PLACEMENT_ACCESS_DML。connectionFlag为SESSION_LIFESPAN|FOR_DML
	c. 多分片删改操作：同操作b
	d. 多分片插入操作：为每个placement先后创建2个placementAccess。accessType分别为PLACEMENT_ACCESS_DML/PLACEMENT_ACCESS_DML（根据任务类型决定），PLACEMENT_ACCESS_SELECT。connectionFlag为CONNECTION_PER_PLACEMENT|FOR_DDL 或 CONNECTION_PER_PLACEMENT|FOR_DML（根据任务类型决定）

2. 获取连接，只有操作场景d会将连接标记为占用（claimedExclusively为true）。
	此步骤中，搜索方式、复用连接、建立新连接（仅操作场景d 需要判断场景5和6）、分配connnection后的操作，均与Real-Time执行器相同，而报错场景更为复杂。
	报错场景：分配connnection前，存在到该placement的connnection。（根据编号顺序，仅当前面的编号不满足才能进入下面的编号场景判断）
		a. 该placementAccess的accessType为PLACEMENT_ACCESS_DDL; 且hasSecondaryConnections为true 或存在co-located placementAccess且其hasSecondaryConnections为true
			也即，希望在当前连接上进行DDL操作，但此连接或它的co-located连接的hasSecondaryConnections标记为true
		b. 连接池中存在符合复用条件1的连接，曾/正 执行DDL/DML（hadDDL/hadDML为true）；但不是当前连接，且当前连接曾/正 执行DDL/DML（hadDDL/hadDML为true）。
			也即，当前连接曾/正 执行DDL/DML，同时还存在其他到该placement的可复用的连接
		c. 不满足复用条件1，且当前连接曾/正 执行DDL/DML（hadDDL/hadDML为true）
		d. 该placementAccess的accessType为PLACEMENT_ACCESS_DDL，但不满足上述3种报错场景

3. placement关联到当前connection，并等待connection建立成功的响应
4. 封装sql指令，通知connection对端Worker节点执行远端事务，并等待执行完毕
5. 设置结果返回模式为single-row。通过connection下发具体任务的sql指令到Worker节点。若查询语句包含条件，则解析后下发
6. 读取并保存执行结果
```

```
Task-Tracker执行器  -- 未完待续
整体流程与Real-Time（非Router）执行器类似，只有具体执行逻辑存在差异，直接影响到连接复用相关
```



## 4.3 DDL/DML占用 或 报错场景

```
# 预制条件
1. 选择的复用连接与当前EntryAccess使用的connection不一致，则设置副本的实体通道以及其co-located实体的hasSecondaryConnections为true（报错场景1&2）
2. 当前placement的连接满足复用条件，若该连接正在执行DDL/DML，则设置foundModifyingConnection为true（报错场景3）
3. 若该placement的EntryAccess的accessType为PLACEMENT_ACCESS_DDL，则为它分配的连接需设置hadDDL为true，标志该连接正在执行DDL。hadDML设置同理。（报错场景3&4）

# 报错场景（根据编号顺序，仅当前面的编号不满足才能进入下面的编号场景判断）
1. 当前副本的实体通道AccessType为PLACEMENT_ACCESS_DDL，且该实体通道的hasSecondaryConnections为true
2. 当前副本的实体通道AccessType为PLACEMENT_ACCESS_DDL，且该实体通道存在co-located实体，且其hasSecondaryConnections为true
3. 已找到可复用的连接，但该连接正在执行DDL/DML，且当前副本的连接也正在执行DDL/DML
4. 当前副本的连接正在执行DDL/DML
5. 当前副本的实体通道的AccessType为PLACEMENT_ACCESS_DDL
```

