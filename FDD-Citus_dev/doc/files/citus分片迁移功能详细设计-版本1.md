---
typora-copy-images-to: 图片
---

# citus在线扩容之分片迁移功能详细设计



## 一、需求分析

随着业务的快速增长，数据容量越来越膨胀，系统不可避免的需要进行扩容，Citus社区版本已经可以实现集群在线扩容，只是在同步复制表数据到新增新节点时，需要全程对复制表加写锁，并且最关键的是无法实现数据的自动重新分布，即扩容前创建的表，不会落到新增节点上，而之后创建的表也不会和之前创建的表具有亲和关系，影响连接复用和表之间的关联操作等。

### 1.1、citus社区版在线扩容现状

citus社区版虽然已经实现了在线扩容，即通过master_add_node函数，把一个新节点加入到集群，为了说明该函数在实际业务上存在的不足，先大体说下该函数的实现流程：

1. 在所有cn中插入该新节点的信息并且激活该节点
2. 在所有CN上对所有复制表和其涉及的外键表加写锁
3. 在新节点上创建对应的表、设置表的owner
4. 将主CN上的所有复制表都通过copy方式复制到新节点
5. 待所有的复制表都同步到新节点后修改所有CN上的对应元数据，
6. 释放步骤2加的锁，函数返回执行成功

注：为了避免出现死锁，步骤2的加锁流程为：1、对所有复制表根据oid进行排序，2、在主CN和其他CN上将排序后的表进行遍历，对每个表，先锁该表所有的外键表，然后锁本表。

从上面的流程中可以看出，虽然实现了将一个节点加入了集群，达到了扩容，可实际上并没有解决扩容的根本问题：单workernode上数据容量的膨胀快达到磁盘上限；因为它只实现了复制表数据的同步，没有实现分布表数据的重分布，也就是对于扩容前创建的表，后续的数据插入，依然不会插入到新增加的节点中。

具体而言，citus社区版在线扩容主要存在以下问题：

1. 没有解决原有表在WorkNode上的膨胀问题，即对于扩容前创建的表，后续的数据插入，依然不会插入到新增加的节点中。
2. 扩容前和扩容后所创建的表无法具有亲和关系，也就无法复用连接以及进行亲和表之间的某些关联等操作。
3. 在扩容过程中，因为同步所有复制表是采用单进程的copy方式，所以整个过程中都会加写锁，若这些复制表很大，那么因为锁表而阻塞业务的时间就会很长。

### 1.2、FDD在线扩容设计目标

在线扩容的最终设计目标是在用户无感知的情况下实现扩容，在扩容过程中，会根据数据均衡算法，自动计算出需要从扩容前的节点中在线迁移哪些分片到新节点；一旦迁移成功后，新节点加入集群，新的集群中各节点间的数据容量大体接近，并且应用可以无感知的从新节点查询或修改迁移过来的数据。

下图为在线扩容时数据的迁移方式：

![1562553432651](https://github.com/liupan126/flyingddb/blob/master/doc/citus%E5%88%86%E7%89%87%E8%BF%81%E7%A7%BB%E5%8A%9F%E8%83%BD%E8%AF%A6%E7%BB%86%E8%AE%BE%E8%AE%A1image/1562553432651.png?raw=true?raw=true)

<center>图1、citus在线扩容和存量数据迁移<center\>

具体而言，在线扩容**最终目标**是：

1、只会在最后路由切换的过程中阻塞集群写操作，而且该过程耗时尽可能少

2、根据设定的规则和预置算法，自动选择迁移分片，保证扩容后，各节点间的数据容量是大体相同的

3、采用并行的分片迁移算法，加快迁移速度，避免串行的方式使得迁移时追增量速度比不上产生增量的速度

4、在对迁移前后的数据进行校验，数据一致后，能够自动删除旧的分片，释放磁盘空间

5、具备一定的容错性，在迁移过程中出故障后，待 故障恢复后，能够尽量从故障处继续后续流程，缩短扩容耗时

6、整个扩容过程中，能够提供观察方式，实时监控和查看当前的扩容进度，遇到 异常能够及时主动告警

## 二、友商分析

目前国内使用citus社区版并且成功投入生产环境的有苏宁，不过他们没有修改citus的源码，而是采用存储过程、脚本等方式，来使其满足业务需求，比如编写了存储过程，实现了分片迁移，但是其实现存在一个问题：当迁移过程中存在在线业务且迁移的分片表很大时，在追增量过程中全程加锁，会长时间阻塞业务；并且每次迁移都需要指定一个分片（该分片的亲和分片会一起迁移），当某个表的分片数很多时，就需要手动执行很多次，才可能达到数据均衡。因为 存在这些不足，所以本文不对该方案多描述。

腾讯TBase(以前称之为PGXZ)，是基于另一个分布式框架PGXL，在其基础上进行二次开发的，且成功投入生产环境，目前已经在微信支付商户系统中运行近5年，管理超过230个节点和400T的数据量（2017年数据），也是全球最大的PostgreSQL分布式集群之一。

TBase解决了和在数据治理过程中面临的数据倾斜、成本优化、数据迁移等问题，并且在原有源码上进行一系列内核优化，包括映射关系表、虚拟节点组、多维分片策略、不停机数据搬迁等功能。虽然基于的框架不同，但是其实现思想值得我们借鉴，所以本章节下面会着重介绍其在线迁移实现思路。

citus企业版，也提供了分片迁移函数，来实现对指定分片（包括其亲和分片）的在线迁移；并且还提供了函数，用于实现在扩容后，对指定表中各分片的自动均衡化，并且可以实时观察到迁移进度。

本章节将简述下友商：TBase和citus企业版的在线扩容实现方式，以便作为设计参考：

### 2.1 、腾讯TBase在线扩容实现思路

当集群规模不足以支撑业务量的增长时，需要增加新的节点，TBase会自动将一部分shard从原来的Datanode无缝迁移到新节点上。或者当节点数据出现倾斜时，系统自动将shard从负载较高的节点迁移到负载较低的节点。

在TBase中，有三种类型的数据迁移：

1、   热数据变冷，迁移到Cold Group；这是跨Group迁移

2、   小账户变大，迁到 Huge Key Group；这也是跨Group迁移

3、   扩容或者因为均衡的原因，在一个Group内部的节点之间进行迁移。 

为了实现上述的三种数据迁移，同时再保证高可用和数据一致性的基础上，不停机就能完成数据的迁移，TBase提出的解决方案是根据迁移目标，设定一系列任务(Shard Moving Task)关键点，并对这些关键点进行拆解分析并加以实现。

一个分布式迁移任务（Shard Moving Task）由一个三元组（源source, 目标target, 分片Shards）来定义；从源节点迁移分片中的数据到目标节点，整个流程一共分成5个大步骤：迁移存量数据、迁移增量数据、数据检验、切换路由、清理（如下图）：

![1562565814778](https://github.com/liupan126/flyingddb/blob/master/doc/citus%E5%88%86%E7%89%87%E8%BF%81%E7%A7%BB%E5%8A%9F%E8%83%BD%E8%AF%A6%E7%BB%86%E8%AE%BE%E8%AE%A1image/1562565814778.png?raw=true?raw=true)

<center>图2、TBase迁移步骤<center\>

**1、迁移存量**

迁移存量：顾名思义，就是将需要搬迁的分片的存量数据从源节点搬迁到目标节点。此时业务依然在写，为保证二者存量数据迁移不会存在重复或遗漏的数据，TBase的方案是将开始导出存量数据和开始记录增量这两个动作使用同一个数据库快照(Snapshot)。需要注意的是，在路由切换之前，这些目标节点中的数据对外不可见。

![1562565828553](https://github.com/liupan126/flyingddb/blob/master/doc/citus%E5%88%86%E7%89%87%E8%BF%81%E7%A7%BB%E5%8A%9F%E8%83%BD%E8%AF%A6%E7%BB%86%E8%AE%BE%E8%AE%A1image/1562565828553.png?raw=true?raw=true)

<center>图3、TBase迁移存量<center\>

**2、追增量**

为确保重做增量数据的同时，新的增量数据写入顺利，TBase采取多轮迭代的方式来追增量数据。每一轮的增量数据会越来越少（搬迁的速度比新增的速度快），因此每一轮迭代的重做时间逐轮收敛，直到收敛到某一个可配置的阈值，就进入下一个步骤数据校验。

不过为了避免串行的追增量太慢，即追增量速度比产生增量速度还慢，所以TBase采用并行追增量的方式（微信支付现网为8个并发），流程如下：

![1562565841942](https://github.com/liupan126/flyingddb/blob/master/doc/citus%E5%88%86%E7%89%87%E8%BF%81%E7%A7%BB%E5%8A%9F%E8%83%BD%E8%AF%A6%E7%BB%86%E8%AE%BE%E8%AE%A1image/1562565841942.png?raw=true?raw=true)

<center>图4、TBase并行追增量<center\>
**3、数据校验**

TBase支持严格的数据校验，要求迁移后，不仅数据条数一要致，而且内容也必须完全一致。但是传统的校验需要花费很多的时间，而且，为了保证源节点数据不再新增，必须有一个加锁（只读）的过程。

TBase的方案是，不是等到源节点统计完成之后才解除阻塞，而是统计校验语句获取快照解除阻塞；因此，所以这个加锁的时间并不长，通常在5ms以内。

**4、再追变更**

如果数据校验的时间较长，这段时间源节点上又会产生较多增量数据，因此流程需要再次追变更，过程与第二步中的追变更完全一样，在某一轮迭代的重做时间达到某个阈值时，开始进入下一步：切换路由。

**5、切换路由**

切换路由需要加锁，也就是阻塞源节点上对这些迁移的分片的写操作，业务在这些分片上的写操作会失败。在路由切换完之后再解除源上的写阻塞。

需要注意的是，在阻塞写的这段时间，切换路由之前，还有最后一轮增量迭代需要在目标节点上重做。根据腾讯现网中经验，这段阻塞部分shard上写的时间绝大部分情况在20ms以内，通常可以做到小于10ms。而且由于扩容时，并非所有节点数据都去做迁移，因此这个影响也有限。

**6、清理**

解锁、停止源节点上的记录增量数据的过程，清理源节点上的重复数据。 

TBase在线迁移实现中有三个关键问题需要关注下：

1、存量/增量临界点：

- 怎么保证存量和增量之间无重复无遗漏：数据库快照

2、数据校验：

- 源/目标节点如何保证完全一样的校验对象：数据库快照

-  支持条数校验和内容校验

3.切路由阻塞写：

- 阻塞时间在20ms

### 2.2、citus企业版实现思路

citus企业版提供了几个UDF函数，可以实现分片迁移（**master_move_shard_placement**）、表中各分片的重新均衡分布（**rebalance_table_shards**）、监视分片迁移进度（**get_rebalance_progress**）功能，本节将着重介绍下这几个函数的功能和使用，以便为我们的设计和开发提供思路。

#### 2.2.1、分片迁移功能

分片迁移（**master_move_shard_placement**）函数：将给定的分片（以及其所有亲和分片）从一个节点移动到另一个节点。该函数通常在表中各分片重新平衡期间间接使用，而不是由数据库管理员直接调用。

该函数提供两种方法移动数据：阻塞写或非阻塞写。阻塞写方法意味着在移动期间暂停对待迁移分片（包括其所有亲和分片）的所有修改操作。而非阻塞写方法，它避免阻止分片写入，但是依赖于Postgres 10逻辑复制。

迁移成功后，源节点中的分片将被删除。如果在迁移期间发生失败，则此函数会抛出错误并使源节点和目标节点保持不变。

**该函数的参数**：

**shard_id：**待移动的分片ID。

**source_node_name：**待迁移分片存放的workernode节点的DNS名称（“源”节点）。

**source_node_port：**数据库服务器正在侦听的源workernode节点上的端口。

**target_node_name：**分片将迁移到目的workernode节点的DNS名称（“目标”节点）。

**target_node_port：**数据库服务器正在侦听的目标workernode节点上的端口。

**shard_transfer_mode :(**可选）指定复制方法，是否使用PostgreSQL逻辑复制或跨workernode节点的COPY命令，有以下三个取值可选：

- auto：如果当前pg版本支持逻辑复制，并且待迁移的表具有副本标识，则自动选择逻辑复制，否则使用COPY方式（例如不支持逻辑复制的低于PostgreSQL 9.6版本）。这是默认值。
- force_logical：即使待迁移的表没有副本标识，也请使用逻辑复制。在迁移期间，对表的任何并发UPDATE/DELETE语句都将失败。
- block_writes：对缺少主键或副本标识的表使用COPY方式（阻塞写入操作）。

**返回值**

N / A

**示例**

SELECT  master_move_shard_placement （12345 ， 'from_host' ， 5432 ， 'to_host' ， 5432 ）;

#### 2.2.2、表中各分片重新均衡分布

表中各分片的重新均衡分布（**rebalance_table_shards**）函数：将移动给定表中的分片，使它们在workernode之间均匀分布。该函数首先计算出需要进行迁移的分片列表，以确保集群在给定阈值内保持平衡。然后，它将待迁移的系列分片从源节点逐个移动到目标节点，并更新相应的分片元数据以便进行路由切换。

**参数**

**table_name：**需要重新平衡其分片所在位置的表的名称。

**threshold :(**可选）介于0.0和1.0之间的浮点数，表示节点利用率与平均利用率的最大差异比率。例如：该值为0.1，则分片重新均衡器将尝试平衡所有节点中的分片数，以保持节点间分片数数量相差±10％以内。具体来说，分片重新均衡器将尝试把所有工作节点的分片数收敛到:（1 - threshold）* average_utilization ...（1 + threshold）* average_utilization范围内。

**max_shard_moves :(**可选）要移动的最大分片数。

**excluded_shard_list :(**可选）在重新平衡操作期间不应移动的分片的标识符。

**shard_transfer_mode :(**可选）指定复制方法，是否使用PostgreSQL逻辑复制或跨workernode节点的COPY命令。可能的取值是：

- auto：如果当前pg版本支持逻辑复制，并且待迁移的表具有副本标识，则自动选择逻辑复制，否则使用COPY方式（例如不支持逻辑复制的低于PostgreSQL 9.6版本）。这是默认值。
- force_logical：即使待迁移的表没有副本标识，也请使用逻辑复制。在迁移期间，对表的任何并发UPDATE/DELETE语句都将失败。
- block_writes：对缺少主键或副本标识的表使用COPY方式（阻塞写入操作）。

**返回值**

N / A

**示例**

eg1：将尝试在默认阈值内，在各workernode节点上重新平衡github_events表的分片位置：

SELECT  rebalance_table_shards （'github_events' ）;

eg2：将尝试重新平衡github_events表，并且不移动ID为1和2的分片：

SELECT  rebalance_table_shards （'github_events' ， excluded_shard_list ：= '{1,2}' ）;

#### 2.2.3、监视分片迁移进度

监视分片迁移进度（**get_rebalance_progress**）函数：一旦某个表开始执行其分片重新均衡，该函数将列出所涉及的每个分片的迁移进度，它监视着rebalance_table_shards()函数的分片移动计划和执行流程。

**参数**

N / A

**返回值**

包含这些列的元组：

- **sessionid**         ：重新平衡监视器的Postgres PID
- **table_name**    ：当前正在移动的分片所属的表
- **shardid**            ：需要移动的分片id
- **shard_size**       ：分片大小（以字节为单位）
- **sourcename**   ：源节点的主机名
- **sourceport**     ：源节点的端口
- **targetname **  ：目标节点的主机名
- **targetport**      ：目标节点的端口
- **progress **           ：迁移进度，其中：0 = 等待移动、1 = 移动、2 = 完成

## 三、在线扩容概要设计

综合借鉴腾讯TBase、亚信AntDB、citus企业版的在线扩容（迁移）功能，结合目前的长沙研发团队人员和知识掌握情况，在遵循由易到难、由部分到整体的设计思路，飞象分布式数据库的在线扩容功能开发，将分为三个阶段来实现，本节将重点介绍下三个阶段的各自实现内容和实现的目标。

### 3.1、在线扩容功能实现的三个阶段

在1.2节的FDD在线扩容设计目标中可以了解到，飞象分布式数据库的在线扩容，最终达到的目标是：集群在新增加workernode节点后，新集群中的数据能够以分片为单位，自动达到均衡化，整个过程不用人来参与，而且只有最终修改分片相关元数据（路由切换）过程中，才会需要对待迁移分片进行阻塞写操作。

要实现该目标，大体需要执行以下几个关键步骤：

1. 找出集群中的所有分布表，按亲和关系进行分组，保存到table_groups
2. 遍历table_groups，对每一类亲和表，按分片位置，将这些表中的分片进行分组并保存到placement_groups
3. 统计出placement_groups中每类亲和分片所占有的空间，然后结合扩容后集群workernode节点数，采用分片均衡算法，计算出需要从扩容前的节点中，迁移哪些亲和分片到新增节点里面。
4. 采用分片迁移函数，对待迁移的亲和分片组，采用并行方式进行分片迁移

从上面的步骤可以看出，我们要实现集群数据均衡化，就需要以亲和表为单位，分批次进行，而在以亲和表为单位进行迁移过程中，又需要以亲和分片为“原子”，逐个分片（包括其亲和分片）的进行迁移，之所以称之为“原子”，是因为该步骤要么成功，要么失败，而不会存在第3种情况；

所以，按照模块化设计思路，将整个在线扩容功能的实现，拆分为三个独立模块，并且按照依赖关系来分阶段完成，即如下三个阶段：

- 第一阶段之分片迁移：对指定的分片和其亲和分片作为一组迁移单元进行迁移操作，这样待迁移的分片必定都在一个节点上，而目的端也是另一个主机，故该操作可以看成一个“原子”，只有都迁移成功才修改元数据。
- 第二阶段之表数据均衡化：对给定的表，迁移其中的某些分片（包括其亲和分片）到目标节点上，并且使得迁移后，该表的分片数在新集群中分布是均衡的（在阈值范围内）。
- 第三阶段之集群数据均衡化：对给定的数据库，找出其中的分布表作为待迁移的表列表，按照亲和类型，将这些表进行分类，而对每类表，根据某种算法（比如oid最小的）选择其中的某个表，采用前面介绍的表数据均衡功能，实现表中的分片均衡化（会把该表的亲和表也同步过去，故只需要选择亲和表组中的一个）。

这三个阶段的具体实现细节和操作流程，将在下面小节中介绍：

### 3.2、第一阶段之分片迁移

分片迁移功能，就是指定一个分片，并且指定源端和目的端，就会自动把该分片以及其亲和分片，从源端迁移到目的端，只有在迁移成功后，经条数校验和内容校验无误后，在切元数据的过程中，才会阻塞相关分片的写操作，其迁移过程如下图所示：

![1562898056711](https://github.com/liupan126/flyingddb/blob/master/doc/citus%E5%88%86%E7%89%87%E8%BF%81%E7%A7%BB%E5%8A%9F%E8%83%BD%E8%AF%A6%E7%BB%86%E8%AE%BE%E8%AE%A1image/1562898056711.png?raw=true?raw=true)

<center>图5、分片迁移<center\>

>
> 注1：tb1和tb3为具有亲和关系的分布表，tb2为另一个分布表
>

#### 3.2.1、功能实现

该功能的实际操作，通过执行一个UDF函数fdd_move_shard_placement（）来完成，该函数的定义如下所示：

```sql
CREATE TYPE citus.shard_transfer_mode AS ENUM (
   'auto',
   'force_logical',
   'block_writes'
);

CREATE TYPE citus.move_lock_mode AS ENUM (
   'auto_shard_mode',
   'auto_table_mode',
   'auto_database_mode',
   'manual_shard_mod',
   'manual_table_mode',
   'manual_database_mode'
);

CREATE OR REPLACE FUNCTION fdd_move_shard_placement(
	shard_id bigint,
	source_node_name text,
	source_node_port integer,
	target_node_name text,
	target_node_port integer,
	shard_transfer_mode citus.shard_transfer_mode default 'auto',
	Is_colocation bool default true,
	Is_del_old_shard bool default true,
    lock_mode citus.move_lock_mode default 'table_mode',
    task_name text default '')
RETURNS void LANGUAGE C STRICT
AS 'MODULE_PATHNAME', $$fdd_move_shard_placement$$;
```

**该函数的参数**：

**shard_id：**待移动的分片ID。

**source_node_name：**待迁移分片所在的源节点的DNS名称

**source_node_port：**数据库服务器正在侦听的源节点上的端口

**target_node_name：**分片将迁移到目标节点的DNS名称

**target_node_port：**数据库服务器正在侦听的目标节点上的端口。

**shard_transfer_mode :(可选）**指定复制方法，是否使用PostgreSQL逻辑复制或跨workernode节点的COPY命令，有以下三个取值可选：

- auto：如果当前pg版本支持逻辑复制，并且待迁移的表具有副本标识，则自动选择逻辑复制，否则使用COPY方式（例如不支持逻辑复制的低于PostgreSQL 9.6版本）。这是默认值。
- force_logical：即使待迁移的表没有副本标识，也请使用逻辑复制。在迁移期间，对表的任何并发UPDATE/DELETE语句都将失败。
- block_writes：对缺少主键或副本标识的表使用COPY方式（阻塞写入操作）。

**Is_colocation :(可选）**指定在迁移该分片时，是否将其所有的亲和分片都一起迁移走，默认为true，即将亲和分片一起迁移；当为false时，需要申请新的亲和组ID，并且同步修改涉及到的几个元数据表。

**Is_del_old_shard :(可选）**指定在迁移分片成功后，是否删除旧的分片表，默认为true，即将旧的分片表（若亲和分片表也迁移了，则也包括旧的亲和分片表）删除；当为false时，需要人手动执行清理旧分片表的命令来回收。

**lock_mode:(可选）**锁库模式，是每个分片（及其亲和分片）复制完成后，就进行锁库切换元数据，还是最后一起锁库切换元数据。可能的取值是：

- auto_shard_mode：当每个分片（及其亲和分片）复制完成后，就进行锁库和路由切换操作，只有当前成功后才会进行后续分片的迁移操作

- manual_shard_mode：当每个分片（及其亲和分片）复制完成后，需手动触发才进行锁库和路由切换操作

**task_name:(可选)** 当前迁移任务的名字，如果为空，则默认为第一个参数shard_id，用于在可重入时指定对哪个迁移操作进行重入，以及唤醒某个操作进行锁库和路由切换操作。

**返回值**

N / A

注意：该函数和citus企业版**master_move_shard_placement**函数的不同之处在于，新增加了四个可选项：**Is_colocation** 、**Is_del_old_shard ** 、 **lock_mode**和**task_name**，增加函数的灵活性。

#### 3.2.2、实现目标

考虑到在线运行环境，可能出现分布表很多，比如100个，而且这些表都具有亲和关系，那么迁移一个分片表时，就需要把其他99个亲和分片表一起迁移，相当于需要同时迁移100个分片表，如何保证该操作是“原子性”，并且整个迁移过程中，只会在最后同步完成，切换元数据过程中，才会有一次阻塞写操作。

同时还有可能迁移过程中串行的迁移量太慢，即数据迁移速度比产生数据速度还慢，这样源端和目的端数据永远没法一致；这些问题都是需要考虑到的，具体而言，分片迁移功能的开发，提出了以下设计目标：

- 目标1：在指定迁移一个分片过程中，不管需要同时迁移其多少个亲和分片表，在整个过程中，必须保证具有“原子性”，即只有待迁移的分片表在源端和目的端数据都一致且修改了所有相应元数据，才算成功，否则迁移失败。
- 目标2：在整个迁移过程中，只会在最后同步完成，切换元数据过程中，才会有一次阻塞写操作，并且阻塞耗时尽可能短。
- 目标3：为了避免单进程的迁移速度太慢，使得源端和目标端数据差异越来越大，需要根据实际场景，决定是否采用多进程方式来实现迁移，并且可以考虑采用分组迁移等方式，来加快迁移速度。
- 目标4：整个迁移过程中，能够观察到每个分片的迁移进度和迁移状态，以及估算出迁移完成的剩余时间。
- 目标5：能够在分片数据 同步到目标节点后，可以手动操作，来触发锁库和切换元数据的操作，以便把对业务的影响降低到最小。
- 目标6：在故障时能够精确识别出故障原因（比如逻辑复制冲突、节点重启等），待故障排除后该操作可重入，并且尽可能从中断出开始继续后面的流程，避免冗余操作。

可重入包括：dba手动停止、异常停止（逻辑复制冲突、目的表存在且不为空）、故障中止（主CN、源端、目的端故障，故障包括网络故障和节点故障）、程序出bug退出等场景。

### 3.3、第二阶段之表数据均衡化

表数据均衡化功能，就是对给定的表，迁移其中的某些分片（包括其亲和分片）到目标节点上，并且使得迁移后，该表的分片数在集群中分布是均衡的（在阈值范围内），该功能主要用于两种场景：

场景1：某些表的分片在workernode上分布不均衡，有可能是之前新增加过节点，但是没有迁移分片；也有可能是现在正在进行在线扩容操作，往集群中新增了若干个节点。

场景2：集群中某些分片中的数据倾斜严重，使得数据集中在少数workernode节点上，所以尝试是否能够在压力大的节点上迁移部分分片（包括其亲和分片）到压力小的节点上。

其迁移过程如下图所示：

![1562898078678](https://github.com/liupan126/flyingddb/blob/master/doc/citus%E5%88%86%E7%89%87%E8%BF%81%E7%A7%BB%E5%8A%9F%E8%83%BD%E8%AF%A6%E7%BB%86%E8%AE%BE%E8%AE%A1image/1562898078678.png?raw=true?raw=true)

<center>图6、表数据均衡化<center\>

>  注1：上图假设的是对tb1表进行数据均衡化，其中tb1和tb3为具有亲和关系的分布表，tb2为另一个分布表。
>
> 注2：tb1_04和tb3_04是一对亲和分片，所以他们两个是作为一组，以“原子”的方式一起完成迁移。

从上图可以看出，待迁移的分片表可能有多个，也有可能是多个分片并行迁移，但是每个具体的分片的迁移过程是“原子性”的，这样该功能的执行，就有可能会出现部分成功、部分失败的场景，但是失败不会影响集群的正常使用，仅仅只是当前操作的某些分片没有成功迁移。

#### 3.3.1、功能实现

该功能的实际操作，通过执行一个UDF函数**fdd_rebalance_table_shards（）**来完成，该函数的定义如下所示：

```sql
CREATE OR REPLACE FUNCTION fdd_rebalance_table_shards(
	table_name text,
    expansion_node text default '',
	threshold  float default 0.1,
	max_shard_moves integer default -1,
	excluded_shard_list text default '',
	shard_transfer_mode citus.shard_transfer_mode default 'auto',
	Is_colocation bool default true,
    Is_del_old_shard bool default true,
	lock_mode citus.move_lock_mode default 'table_mode',
    task_name text default '')
RETURNS void LANGUAGE C STRICT
AS 'MODULE_PATHNAME', $$fdd_rebalance_table_shards$$;
```

**参数**

**table_name：**需要重新平衡其分片所在位置的表的名称。

**expansion_node:（可选）**该字段包含的是在线扩容时新增的节点DNS和端口对，可能一次在线扩容了多个节点，则此处会包含这些新增节点的DNS和端口对，当这些新增节点上存在所指定表的分片时，直接报错，因为该场景不属于在线扩容操作；若不是用于在线扩容，而是解决集群中分片倾斜问题，则此处传入空字符串（默认值）

**threshold :(**可选）介于0.0和1.0之间的浮点数，表示节点利用率与平均利用率的最大差异比率。例如：该值为0.1，则分片重新均衡器将尝试平衡所有节点中的分片数，以保持节点间分片数数量相差±10％以内。具体来说，分片重新均衡器将尝试把所有工作节点的分片数收敛到:（1 - threshold）* average_utilization ...（1 + threshold）* average_utilization范围内。

**max_shard_moves :(可选）**要移动的最大分片数。

**excluded_shard_list :(可选）**在重新平衡操作期间不应移动的分片的标识符。

**shard_transfer_mode :(可选）**指定复制方法，是否使用PostgreSQL逻辑复制或跨workernode节点的COPY命令。可能的取值是：

- auto：如果当前pg版本支持逻辑复制，并且待迁移的表具有副本标识，则自动选择逻辑复制，否则使用COPY方式（例如不支持逻辑复制的低于PostgreSQL 9.6版本）。这是默认值。
- force_logical：即使待迁移的表没有副本标识，也请使用逻辑复制。在迁移期间，对表的任何并发UPDATE/DELETE语句都将失败。
- block_writes：对缺少主键或副本标识的表使用COPY方式（阻塞写入操作）。

**Is_colocation :(可选）**指定在对该表进行均衡化时，是否将其所有的亲和表也进行同样的均衡化，默认为true，即在均衡化过程中，迁移分片时会将其亲和分片一起迁移；当为false时，需要申请新的亲和组ID，并且同步修改涉及到的几个元数据表。

**Is_del_old_shard :(可选）**指定在迁移分片成功后，是否删除旧的分片表，默认为true，即将旧的分片表（若亲和分片表也迁移了，则也包括旧的亲和分片表）删除；当为false时，需要人手动执行清理旧分片表的命令来回收。

**lock_mode:(可选）**锁库模式，是每个分片（及其亲和分片）复制完成后，就进行锁库切换元数据，还是最后一起锁库切换元数据。可能的取值是：

- auto_shard_mode：当每个分片（及其亲和分片）复制完成后，就进行锁库和路由切换操作，只有当前成功后才会进行后续分片的迁移操作

- auto_table_mode：当某个表及其亲和表所涉及的分片都复制完成后，才进行锁库和路由切换操作，只有当前成功后才会进行其他表的迁移操作

- manual_shard_mode：当每个分片（及其亲和分片）复制完成后，需手动触发才进行锁库和路由切换操作
- manual_table_mode：当某个表及其亲和表所涉及的分片都复制完成后，需手动触发才进行锁库和路由切换操作

**task_name:(可选)**当前迁移任务的名字，如果为空，则默认为第一个参数table_name，用于在可重入时指定对哪个迁移操作进行重入，以及唤醒某个操作进行锁库和路由切换操作。

**返回值**

N / A

#### 3.3.2、设计考虑点

因为在对表进行均衡化的过程中，需要对表中的各分片所在位置进行重新分布，而这也涉及到相关联的亲和表，所以选择哪些分片迁移到目的节点，是最关键的问题，具体而言需要在实现过程中需要考虑以下几点：

- 考虑点1：因为当集群workernode个数过多时，一般在线扩容是需要同时新增多个节点的，避免每增加一个节点，就进行一次表中各分片均衡操作，使得分片反复迁移，故需要考虑支持把扩容新增加的节点加入集群后，在统一进行表中各分片的均衡操作，并且保证每个分片至多被迁移一次

- 考虑点2：需要按照入参threshold，来考虑从每个源节点上迁移出多少个分片到目的节点。

- 考虑点3：需要考虑某些场景下可能出现数据倾斜，使得一个表中每个分片（及其亲和分片）的所占空间大小差别很大，所以在选择待迁移分片过程中，需要考虑每个分片表（包括其所有亲和分片表）所占空间的大小泽中选择。

- 考虑点4：因为是以分片为单位进行迁移，而分片迁移又是“原子性”的，当配置Is_finally_Lock_together为false 时，需要考虑在故障场景下，若某些分片表已经迁移成功，那么后续恢复操作，将跳过这些分片表，而只是从出错或未完成的分片表中进行恢复。

#### 3.3.3、实现目标

在兼顾上面提到的几种考虑点，结合该功能的实现需求，提出了表中分片重新均衡化功能的实现目标如下：

- 目标1：能够把指定的表所涉及的分片（包括亲和分片），重新均衡化到多个新增加的目标节点上。


- 目标2：以分片和其亲和分片作为一组来计算所占空间，能够使得表数据均衡化后，该表在每个节点上的分片个数符合阈值，并且在每个节点上所占空间尽可能差不多。


- 目标3：可能待迁移的分片以及它们的亲和分片很大，为了避免单进程追增量的速度比不上数据产生的速度，所以需要根据实际情况，采用多进程的方式进行并行分片迁移，并且要避免出现死锁。


- 目标4：在故障时能够精确识别出故障原因（比如逻辑复制冲突、节点重启等），待故障排除后该操作可重入，并且尽可能从中断出开始继续后面的流程，避免冗余操作。

- 目标5：整个迁移过程中，能够观察到每个表和其中每个分片的迁移进度和迁移状态，并且能够估算出迁移完成的剩余时间。
- 目标6：能够在分片数据 同步到目标节点后，可以手动操作，来触发锁库和切换元数据的操作，以便把对业务的影响降低到最小。

### 3.4、第三阶段之集群数据均衡化

集群数据均衡化功能，就是在对集群扩容后，需要把原有集群中的部分数据迁移到新加入的若干个节点上，并且保证迁移后，新集群中各节点上的分片个数在阈值之内且数据容量差不多，在迁移过程中，只有最后修改元数据的阶段，才有可能会阻塞写操作。其迁移过程如下图所示：

![1562898335342](https://github.com/liupan126/flyingddb/blob/master/doc/citus%E5%88%86%E7%89%87%E8%BF%81%E7%A7%BB%E5%8A%9F%E8%83%BD%E8%AF%A6%E7%BB%86%E8%AE%BE%E8%AE%A1image/1562898335342.png?raw=true?raw=true)

<center>图7、集群数据均衡化<center\>

> 注1：上图假设的是对集群进行数据均衡化，其中tb1和tb3为具有亲和关系的分布表，tb2为另一个分布表。
>
> 注2：tb1_04和tb3_04是一对亲和分片，所以他们两个是作为一组，以“原子”的方式一起完成迁移。

上图是一个成倍扩容时，数据重新分布的例子，从中可以看出可以待新增加的2个节点全部加入集群后，在进行分片迁移操作，从而使得扩容后的集群中数据的均衡性。

#### 3.4.1、功能实现

该功能的实际操作，通过执行一个UDF函数**fdd_rebalance_database（）**来完成，该函数的定义如下所示：

```sql
CREATE OR REPLACE FUNCTION fdd_rebalance_database(
	database_name text,
    expansion_node text default '',
	threshold  float default 0.1,
	max_table_moves integer default -1,
	excluded_table_list text default '',
    included_table_list text default '',
	shard_transfer_mode citus.shard_transfer_mode default 'auto',	
    Is_del_old_shard bool default true,
	lock_mode citus.move_lock_mode default 'database_mode',
    task_name text default '')
RETURNS void LANGUAGE C STRICT
AS 'MODULE_PATHNAME', $$fdd_rebalance_database$$;
```

**参数**

**database_name：**指明需要进行重新数据均衡化的数据库名字，主要起到校验的作用，以免误操作。

**expansion_node:（可选）**该字段包含的是在线扩容时新增的节点DNS和端口对，可能一次在线扩容了多个节点，则此处会包含这些新增节点的DNS和端口对，并且当这些新增节点上存在某个表的分片时，该表及其亲和表都不会被迁移；若不是用于在线扩容，而是解决集群中分片倾斜问题，则此处传入空字符串（默认值）

**threshold :(可选）**介于0.0和1.0之间的浮点数，表示节点利用率与平均利用率的最大差异比率。例如：该值为0.1，则分片重新均衡器将尝试平衡所有节点中的分片数，以保持节点间分片数数量相差±10％以内。具体来说，分片重新均衡器将尝试把所有工作节点的分片数收敛到:（1 - threshold）* average_utilization ...（1 + threshold）* average_utilization范围内。

**max_table_moves:(可选）**要移动的最大表个数，该值不包含每个表的亲和表个数，故实际迁移的表可能远大于该值。

**excluded_table_list :(可选）**若该值不为空，则指明在重新平衡操作期间不应被移动的表（包括其亲和表）

**included_table_list:(可选）**若该值不为空，则指明在重新平衡操作期间只会被移动的表（包括其亲和表），并且这些表在expansion_node所指定的节点上不存在其分片表；注意该字段不能够和excluded_table_list 字段同时为非空值，因为这样没有实际意义。

**shard_transfer_mode :(可选）**指定复制方法，是否使用PostgreSQL逻辑复制或跨workernode节点的COPY命令。可能的取值是：

- auto：如果当前pg版本支持逻辑复制，并且待迁移的表具有副本标识，则自动选择逻辑复制，否则使用COPY方式（例如不支持逻辑复制的低于PostgreSQL 9.6版本）。这是默认值。
- force_logical：即使待迁移的表没有副本标识，也请使用逻辑复制。在迁移期间，对表的任何并发UPDATE/DELETE语句都将失败。
- block_writes：对缺少主键或副本标识的表使用COPY方式（阻塞写入操作）。

**Is_del_old_shard :(可选）**指定在迁移分片成功后，是否删除旧的分片表，默认为true，即将旧的分片表（若亲和分片表也迁移了，则也包括旧的亲和分片表）删除；当为false时，需要人手动执行清理旧分片表的命令来回收。

**lock_mode:(可选）**锁库模式，是每个分片（及其亲和分片）复制完成后，就进行锁库切换元数据，还是最后一起锁库切换元数据。可能的取值是：

- auto_shard_mode：当每个分片（及其亲和分片）复制完成后，就进行锁库和路由切换操作，只有当前成功后才会进行后续分片的迁移操作

- auto_table_mode：当某个表及其亲和表所涉及的分片都复制完成后，才进行锁库和路由切换操作，只有当前成功后才会进行其他表的迁移操作

- auto_database_mode：只有当所有涉及的分片全部同步到目的端后，才统一进行阻塞写和路由切换操作
- manual_shard_mode：当每个分片（及其亲和分片）复制完成后，需手动触发才进行锁库和路由切换操作
- manual_table_mode：当某个表及其亲和表所涉及的分片都复制完成后，需手动触发才进行锁库和路由切换操作
- manual_database_mode：只有当所有涉及的分片全部同步到目的端后，需手动触发才统一进行阻塞写和路由切换操作

**task_name:(可选)**当前迁移任务的名字，如果为空，则默认为第一个参数database_name，用于在可重入时指定对哪个迁移操作进行重入，以及唤醒某个操作进行锁库和路由切换操作。

**返回值**

N / A

#### 3.4.2、设计考虑点

在实际运行中，数据会越积越多，节点个数也越来越多，那么待迁移的数据会异常庞大，受限于逻辑复制的框架，在切元数据过程中，需要阻塞数据库写操作，以便目的端数据追上源端，然后进行数据校验，但该过程耗时与数据存量和业务繁忙程度都成正比，为了避免长时间阻塞业务，所以需要考虑可以分批次进行数据库均衡化。

具体而言，在设计和开发过程中，有以下问题需要着重考虑：

- 考虑点1：考虑到集群中节点个数和数据容量，避免长时间阻塞集群正常业务，可以进行分批次数据库均衡化操作，比如先把几个大表迁移走，缓解数据库膨胀压力，后续在迁移剩下的表

- 考虑点2：在运行中，可能会因为某种问题，导致操作中断，要能够提供可恢复的方式，从中断处寻求合适点，继续后续的流程，避免冗余操作

- 考虑点3：在该过程中，是以表为单位进行均衡化的，即该表和及其亲和表是作为一个整体，进行待迁移分片的计算，哪怕是咋故障重启后，也应该如此。

- 考虑点4：如果**Is_finally_Lock_together**为true，即最后统一锁库，那么得综合评估哪些表先进行迁移操作，因为由逻辑复制框架可知，当某个分片表同步到目的端，后续追增量操作是交给逻辑复制的APP进程来完成，在运行中必定同一个复制槽上会有多个表进行发布，那么由APP单进程来追增量将得考虑其压力。

#### 3.4.3、实现目标

在兼顾上面提到的几种考虑点，结合该功能的实现需求，提出了集群数据重新均衡化功能的实现目标如下：

- 目标1：可分批次的进行集群数据重新均衡化操作，避免长时间的阻塞集群业务。


- 目标2：具有可重入功能，即当运行中出现故障时，待故障排除后，能够自动从中断处寻求合适点，继续后续的流程，避免冗余操作。


- 目标3：能够通过算法自动的对待均衡的表进行排序，通过调整表的迁移顺序，加快逻辑复制的进度。
- 目标4：整个迁移过程中，能够观察到每个表和其中每个分片的迁移进度和迁移状态，并且能够估算出迁移完成的剩余时间。
- 目标5：能够在分片数据 同步到目标节点后，可以手动操作，来触发锁库和切换元数据的操作，以便把对业务的影响降低到最小。

## 四、分片迁移整体框架详细设计

### 4.1、进程框架设计

由于pg的udf函数的执行会自动开启一个事务，这样在该函数中若对表进行修改操作，只有等到该udf函数执行完，事务才会被提交，其他进程才可见该函数中对表的修改；

而本设计目标要实现：实时查询迁移进度、迁移多并发操作且故障重启后，也可以继续进行后续的流程，所以在该udf函数中，会起一个“迁移任务前期处理（临时）进程”，来负责迁移计划的生成、迁移计划的前期执行（包括迁移计划插入元数据表、在目的端创建对应分片表、创建对应的逻辑复制等操作）、以及最后触发“调度进程”，来负责后续迁移任务的顺利进行，在这些操作成功处理完成后，会主动退出。

故在线迁移并发操作时的进程间交互框架如下图所示：

 ![1565611742005](https://github.com/liupan126/flyingddb/blob/master/doc/citus%E5%88%86%E7%89%87%E8%BF%81%E7%A7%BB%E5%8A%9F%E8%83%BD%E8%AF%A6%E7%BB%86%E8%AE%BE%E8%AE%A1image/1565611742005.png?raw=true)

<center>图8、进程框架<center\>

而“调度进程”是整个迁移计划具体执行的大脑，其内部的主要实现流程如下图所示：

![1565611790794](https://github.com/liupan126/flyingddb/blob/master/doc/citus%E5%88%86%E7%89%87%E8%BF%81%E7%A7%BB%E5%8A%9F%E8%83%BD%E8%AF%A6%E7%BB%86%E8%AE%BE%E8%AE%A1image/1565611790794.png?raw=true)

<center>图9、调度进程内部实现流程<center\>

从上图可以看出，分片投递，是以分片及其亲和分片为一组，作为最小投递单元进行投递的。

### 4.2、元数据表设计

因为一个迁移任务，涉及到三个子进程，而进程间的通信都是依靠写入三个元数据表来实现的，涉及的三个元数据表以及和模块间的通信，如下图所示：

![1565611883641](https://github.com/liupan126/flyingddb/blob/master/doc/citus%E5%88%86%E7%89%87%E8%BF%81%E7%A7%BB%E5%8A%9F%E8%83%BD%E8%AF%A6%E7%BB%86%E8%AE%BE%E8%AE%A1image/1565611883641.png?raw=true)

<center>图10、模块和元数据之间的交互关系<center\>

上图中，箭头指向元数据表的，表明会更新该元数据表，而箭头指向模块的，表明该模块会查询该元数据中的内容。而这三个元数据表的介绍，如下章节所示。

> 注1、因为只允许在主CN节点上进行在线迁移相关任务，所以以下新增的三个元数据表和新增的两个序列都只存在于主CN节点，是主CN上的本地表和序列，而非分布式表。

#### 4.2.1、**fdd_dist_move_task**

分布式在线迁移任务表：fdd_dist_move_task，主要是用于记录每个迁移任务的相关信息的，包括迁移 任务的名字、迁移模式、旧分片处理策略、锁库模式、数据校验模式、迁移任务的关键过程信息、迁移状态等相关内容，该表的字段如下所示：

<center>表1、fdd_dist_move_task表字段<center\>

| **列**          | **类型**                 | **描述**                                                     |
| :-------------- | ------------------------ | ------------------------------------------------------------ |
| taskname        | name                     | 分布式在线迁移任务名字                                       |
| taskid          | integer                  | 在线迁移任务id（由**pg_dist_move_taskid_seq**序列生成）      |
| rep_mode        | ENUM                     | 迁移模式，是阻塞写方式还是逻辑复制方式                       |
| drop_method     | ENUM                     | 迁移成功后，旧分片的处理策略                                 |
| lock_mode       | ENUM                     | 锁库模式，共六个模式，决定了每次路由切换的粒度，以及是自动还是手动触发锁库进行路由切换 |
| check_pattern   | ENUM                     | 数据校验模式，是数据条数校验还是内容一致校验，还是两者兼之   |
| partkey         | text                     | 记录当前迁移任务涉及到的关键信息：源节点数、目的节点数、亲和表数、亲和分片数、总表数、总分片数 |
| start_time      | timestamp with time zone | 任务开始创建的时间                                           |
| latest_end_time | timestamp with time zone | 当前状态完成时间                                             |
| datasize        | bigint                   | 当前任务涉及分片的总初始大小（以字节为单位）                 |
| shard_state     | integer                  | 当前迁移任务的迁移状态                                       |

其中：

**taskid**：用于表示每一个任务，其类型为integer，为了保证其唯一性，所以需要新增一个任务递增序列g_dist_move_taskid_seq，每次新建一个迁移任务，则从其中递增获取一个任务id，起始任务序列为1。

**rep_mode**：表示迁移任务采用的迁移方式，是使用PostgreSQL逻辑复制还是跨workernode节点的COPY命令，有以下三个取值可选（ENUM类型）：

- 'auto'：如果当前pg版本支持逻辑复制，并且待迁移的表具有副本标识，则自动选择逻辑复制，否则使用COPY方式（例如不支持逻辑复制的低于PostgreSQL 9.6版本）。这是默认值。
- 'force_logical'：即使待迁移的表没有副本标识，也请使用逻辑复制。在迁移期间，对表的任何并发UPDATE/DELETE语句都将失败。
- 'block_writes'：对缺少主键或副本标识的表使用COPY方式（阻塞写入操作）。

**drop_method**：表示迁移成功后，旧分片表的保留方式，是删除还是保留，有以下两个取值（ENUM类型）：

- 'delete_old'：分片迁移成功后，就删除该分片表和其亲和分片表

- 'reserved'：分片迁移成功后，保留旧分片表，但是当前任务涉及到的迁移信息都会保留，以便后续人工手动删除旧分片

**lock_mode**：锁库模式，是每个分片（及其亲和分片）复制完成后，就进行锁库切换元数据，还是最后一起锁库切换元数据。可能的取值是（ENUM类型）：

- 'auto_shard_mode'：当每个分片（及其亲和分片）复制完成后，就进行锁库和路由切换操作，只有当前成功后才会进行后续分片的迁移操作
- 'auto_table_mode'：当某个表及其亲和表所涉及的分片都复制完成后，才进行锁库和路由切换操作，只有当前成功后才会进行其他表的迁移操作
- 'auto_database_mode'：只有当所有涉及的分片全部同步到目的端后，才统一进行阻塞写和路由切换操作
- 'manual_shard_mode'：当每个分片（及其亲和分片）复制完成后，需手动触发才进行锁库和路由切换操作
- 'manual_table_mode'：当某个表及其亲和表所涉及的分片都复制完成后，需手动触发才进行锁库和路由切换操作
- 'manual_database_mode'：只有当所有涉及的分片全部同步到目的端后，需手动触发才统一进行阻塞写和路由切换操作

**check_pattern**：数据校验模式，有以下三种方式（ENUM类型）

-  'count'：只需要源端和目的端相应表的行数一致，就表明两边数据一致，可以进行切换元数据操作。
- 'md5'：对源端和目的端相应表，分别进行md5计算，若两边计算出的md5一致，则表明内容一致。
- 'all'：同时采用上面两种方法校验，先进行count校验，若不一致，则直接报错。

**shard_state**：当前迁移任务的迁移状态，其也是内部实现的一个分片迁移进展的状态机，用于表明每个分片的迁移阶段包括（integer类型）：

- SHARD_INVALID = 0           ：不合法的阶段（不会记录到表中）

- SHARD_INITIAL                   ：初始化，已经为其分配了复制任务处理进程，但是还不能够进行分片投递

- SHARD_CREATED_TABLE   ：已经在目的节点上创建了该表且成功
- SHARD_WAITE_POST          ：前期准备都已经完成，可以进入投递流程
- SHARD_REP_INIT                 ：该表已经投递到复制任务处理进程，但是还没有被具体的复制流程进行处理
- SHARD_REP_DATASYNC      ：该表正在进行存量同步过程中（COPY）阶段
- SHARD_REP_SYNCDONE    ：该表正在进行追增量更新过程中（逻辑复制独有）
- SHARD_REP_READY             ：该表数据同步完成，可以择机进行锁库
- SHARD_MOVE_END             ：该表复制操作已经结束，且元数据已经切换成功
- SHARD_DELETE_OLD           ：该表已经在源端上删除，只存在于目的节点上 

当迁移过程中出现异常时，该字段的值为：迁移阶段+错误类型（从100开始）拼接而成

- TABLE_EXIST = 100           ：在目标节点创建表时，该表已经存在

- CHECK_INCONSIT = 200  ：在进行条数校验和内容校验时，发现两边数据不一致

- CONFLICT = 300                ：该表当前阶段出现了逻辑复制冲突（逻辑复制独有）

eg：

SHARD_DATASYNC + CONFLICT（300 + 5）  ：该表在存量同步阶段出现逻辑复制冲突（逻辑复制独有）

**partkey**：记录当前迁移任务涉及到的：源节点数（s）、目的节点数（d）、亲和表数（ct）、亲和分片数（cs）、总表数（at）、总分片数（as）

#### 4.2.2、fdd_dist_move_shard

分布式在线迁移分片信息表：fdd_dist_move_shard，用于记录每一个迁移任务涉及到的分片，并且把这些分片的相关信息记录下来，包括复制任务处理槽id、所属的迁移任务id、分片初始大小、当前迁移进度等，以便在集群重启或故障恢复后，依然能够尽快的恢复迁移任务；同时用户也可以实时查询当前任务所涉及分片的迁移进度等信息，该表的字段如下所示：

<center>表2、fdd_dist_move_shard表字段<center\>

| 列               | 类型                     | 描述                                                         |
| ---------------- | ------------------------ | ------------------------------------------------------------ |
| shardid          | integer                  | 待移动的分片ID                                               |
| groupid          | integer                  | 待迁移分片表所处的组id                                       |
| repid            | integer                  | 待迁移分片所分配的复制任务处理槽id                           |
| tablename        | regclass                 | 待迁移分片的主表名（此行对应的分布式表。该值引用pg_class系统目录表中的relfilenode列） |
| taskid           | integer                  | 在线迁移任务id                                               |
| assign_shardid   | integer                  | 是由哪个分片因为亲和关系，触发当前分片表进行迁移的，若当前值为shardid，则该行为分片迁移指定的分片 |
| shard_size       | bigint                   | 分片初始大小（以字节为单位）                                 |
| state_begin_time | timestamp with time zone | 当前状态开始的时间                                           |
| shard_state      | integer                  | 该分片当前处于哪个迁移阶段（和**fdd_dist_move_task**表的shard_state字段取值一样） |

**repid**：其值在未投递时，repid%10一定是0，而最终选择复制任务处理子进程时，是会依据投递时的负载情况变动的，但是不管最终选择哪个，其repid/10的值是一样的，也就是选择源端和目的端相同的复制任务处理子进程，该字段的具体生成方式，参见下面的**pg_dist_move_repid_seq**序列修改描述。

#### 4.2.3、fdd_dist_move_worker

复制任务处理子进程信息表：fdd_dist_move_worker，用于记录迁移操作的具体执行进程的相关信息以及该进程上的迁移完成进度，以及在集群重启或故障恢复时，也能够尽快的恢复迁移进程并且处理未完成的任务，该表所含字段如下所示：

<center>表3、fdd_dist_move_worker表字段<center\>

| 列             | 类型    | 描述                                                         |
| -------------- | ------- | ------------------------------------------------------------ |
| rep_id         | integer | 复制任务处理子进程id                                         |
| pid            | integer | 复制任务处理子进程的进程号，对于逻辑复制，指的是复制任务调度子进程，而阻塞写，则还有可能是copy子进程 |
| sub_name       | text    | 对于逻辑复制，指代逻辑复制订阅端的名字，而阻塞写模式此处为空 |
| pub_name       | text    | 对于逻辑复制，指代逻辑复制发布端的名字，而阻塞写模式此处为空 |
| source_groupid | integer | 源节点所在的组id                                             |
| target_groupid | integer | 目标节点所在的组id                                           |
| rep_state      | integer | 该复制任务子进程的整体迁移进度                               |

**pid**：记录实际迁移所涉及的子进程信息，而为了加快迁移进度，采用多进程并行迁移策略，只是多进程的方式依据复制策略的不同而有差别：

- 逻辑复制：多进程并发迁移主要通过迁移任务调度子进程来创建多个逻辑复制槽来实现，而每个逻辑复制槽（在发布端和订阅端都各有一个进程常驻）对待迁移表的存量复制阶段，也会采用多进程并发操作。

​       此时，该字段值为迁移任务调度进程的pid。 

- 阻塞写：多进程并发迁移通过迁移任务调度子进程fork出多个复制处理进程来实现，每个复制处理子进程，会把其所分配的迁移分片表列表，逐个进行copy操作。

​      此时，该字段值为每个复制迁移处理进程的信息。

**rep_state**：记录当前复制任务子进程上的相关分片表的复制进度，决定了何时投入新分片表以及何时数据已经全部同步完成，可以进行路由切换操作。

- REP_INVALID = 0         ：不合法的阶段（不会记录到表中）

- REP_INITIAL                 ：初始状态，已经给当前操作分配了若干个复制子任务，并且给每个涉及的分片表也分配了对应的复制子进程序号

- REP_CREATED_TABLE  ：当前迁移操作所涉及的所有分片表已经在目的端创建了对应的表（“迁移操作总状态”独有）

- REP_CREATED_REP_PROCESS   ：对于逻辑复制：创建了逻辑复制的发布端和订阅端；对于阻塞写：创建了复制处理子进程

- REP_STARTUP                  ：对于逻辑复制：该条逻辑复制槽正在进行存量更新，没有表进入增量更新；对于阻塞写：当前复制子进程正在copy数据

- REP_PART_CATCHUP      ：该条逻辑复制槽的订阅端中有SYNC进程正在进行追增量操作,但是还有部分进程在追存量或等待追存量（逻辑复制独有）

- REP_CATCHUP                 ：该条逻辑复制槽的订阅端中有SYNC进程正在进行追增量操作，剩下其他表都是由APP追增量（逻辑复制独有）

- REP_PART_STREAMING   ：表示某一个分片已经进入'r'或者's'状态

- REP_STREAMING              ：对于逻辑复制：该逻辑复制槽只存在订阅端APP进程追增量操作；对于阻塞写：当前复制子进程已经把分配的表copy完成

- REP_ROUTE_SWITCH      ：正在进行路由切换操作，需要锁库，当前仅有一个复制处理子进程能够进行该操作

- REP_MOVE_END              ：当前批次的表已经复制完成，可以分配新的复制任务，或者进入结束清理操作

> 注意：
>
> 1、只有该操作所涉及的表，有表进入了逻辑复制阶段（对于阻塞写，则是有表正在进行copy操作），“迁移操作总状态”才会变为Startup。
>
> 2、只有该操作所涉及的表，都进入APP进程追增量操作（对于阻塞写，则是copy已经完成），“迁移操作总状态”才会变为Streaming。

错误类型：由迁移阶段（从100开始）+ 错误类型  拼接而成

- TABLE_EXIST = 100               ：在目标节点创建表时，该表已经存在
- CHECK_INCONSIT = 200      ：在进行条数校验和内容校验时，发现两边数据不一致
- CONFLICT = 300                    ：该表当前阶段出现了逻辑复制冲突（**逻辑复制独有**）
- SRC_CONNECT_FAIL = 400  ：源端连接故障
- DST_CONNECT_FAIL = 500  ：目标端连接故障
- MANUAL_INTERRUPT = 600: 人手动中断
- REP_APP_ERR  = 700            ：app进程非正常退出
- REP_APP_SYNC_ERR = 800 ：app和某些sync进程非正常退出
- REP_FETAL_ERR = 900         ：app和所有sync进程全部非正常退出
- REP_SYNC_ERR = 1000        ：某些sync进程非正常退出

 eg：

REP_DATASYNC + CONFLICT（300 + 5）  ：该表在存量同步阶段出现逻辑复制冲突（**逻辑复制独有**）

### 4.3、序列设计

从4.2.1节可以知道迁移任务ID是要求唯一的，并且为了区分任务的先后顺序，要求该ID是递增的，所以新增一个任务递增序列：**pg_dist_move_taskid_seq**，每次新建一个迁移任务，则从其中递增获取一个任务id，起始任务序列为1。

从4.2.2节可以知道repid为待迁移分片所分配的复制任务处理槽id，也要求具有唯一性，所以新增一个复制任务组id递增序列：**pg_dist_move_repid_seq**，用于从1开始递增获取复制任务处理子进程批次的id。

同时考虑到对于源端和目的端一样的复制处理子进程，他们实现的功能都一样，为了实现分组，提出了如下repid生成方式：

1. 对源端和目的端相同的复制处理子进程，只会被分配一个repid。

1. 若需要在源端和目的端相同的情况下，创建多个复制任务处理子进程，则每个复制任务处理子进程序号为：repid*10+i，其中i是第几个复制任务处理子进程（从1开始）。也就是说对于每一对源端和目的端，最多创建9个复制任务处理子进程

### 4.4、全局缓存的设计

因为新增了三个元数据表，为了进度频繁的读表操作以及降低跨表查询，所以需要把一些关键的元数据保存到缓存中，提升性能，根据模块来划分，提出了两种功能的缓存设计：

#### 4.4.1、分片表相关缓存

**1、迁移任务进度缓存（g_TaskMoveCacheList）**

（顺序队列结构，按照任务先后顺序进行排序，新增任务放在队尾，entry为TaskMoveCacheEntry）

 该缓存保存了每一个迁移任务所涉及的所有分片表的迁移进度信息，而这些分片表只会保存在两个队列（等待投递队列和正在投递队列）中的一个，并且是以分片为单位进行投放，所有具有亲和关系的分片表，只会同时存在于两个队列中的一个。

**2、迁移操作涉及分片表缓存（g_MoveShardCacheHash）**

（Hash结构，hash key为shardid，hash entry为MoveShardCacheEntry）

该缓存保存了集群中所有迁移任务涉及到的所有分片表信息，只有往该缓存中新增项时，才会申请内存空间，其他地方只会引用其空间地址。

如上两个缓存涉及到的结构如下所示：

```c
//分片具体信息 
typedef struct _MoveShardCacheEntry
{
 int   shardid   ;
 int   groupid   ;
 int   repid     ;//待迁移分片所分配的复制任务处理子进程序号
 Oid   tablename ;//待迁移分片的表名
 int   taskid    ;
 int assign_shardid ;
 int64 shard_size   ;//分片初始大小（以字节为单位）
 int   stage_time   ;//当前状态开始的时间
 int   shard_state  ;//该分片当前处于哪个迁移阶段
 
 //以下结构不会存到表中，只存在内存中
 bool isValid   ;//此条目是否收到缓存效通知，是否需要重构缓存
 Oid  relationId;//此分片表在订阅端上的oid
}MoveShardCacheEntry;

//分片表分组投递单元
typedef struct _ShardPostUnitEntry
{
 int   anchorShardId  ;//被指定的进行迁移的分片ID
 int   anchorRepId    ;
 int   anchorTaskId   ;
 Oid   anchorRelid    ;//该anchorShardId表的主表的oid
 shard_state anchorShard_state;
 int  shardIndex;
 int  SortedColShardArrayLength;//SortedColShardArray 数组的长度
 MoveShardCacheEntry **SortedColShardArray ; //保存根据shardid排序后的anchorShardId以及所有其亲和分片，决定了投递顺序
}ShardPostUnitEntry;

//迁移任务进度
typedef struct _TaskMoveCacheEntry
{
 int   taskid ;
 int   drop_method   ;//分片迁移成功后，旧分片的处理策略
 int   lock_mode     ;//锁库模式
 int   check_pattern ;//数据校验模式
 shard_state   shard_state;//该任务当前处于哪个迁移阶段
 
 //以下结构不会存到表中，只存在内存中
 bool isValid ;//此条目是否收到缓存效通知，是否需要重构缓存
 List *MoveShardWaitingList      ;//等待投递队列，根据投递算法决定分片顺序
 int   MoveShardPostingHashLength;//在MoveShardPostingHash中分片的个数
 HTAB *MoveShardPostingHash      ;//正在投递队列(hash key为shardid，hash entry为ShardPostUnitEntry)
}TaskMoveCacheEntry;

//迁移任务进度缓存
//按照任务顺序进行排序，新增任务放在队尾，entry为TaskMoveCacheEntry
List *g_TaskMoveCacheList;


//迁移操作涉及分片表缓存
//hash key为shardid，hash entry为MoveShardCacheEntry
HTAB *g_MoveShardCacheHash;
```

#### 4.4.2、**复制进程相关缓存**

**1、复制处理进程调度缓存（g_MoveScheduleCacheHash）**

（Hash结构，hash key为rep_id，hash entry为MoveScheduleCacheEntry）

该缓存保存了所有迁移任务所创建的复制进程信息，其中可以查到每个迁移进程的pid、源端和目的端节点信息、该进程的迁移进度信息等，该缓存涉及到的结构如下所示：

```c
//复制任务处理子进程信息
typedef struct MoveWorker
{
 int  rep_id;
 int  pid;
 char sub_name[NAME_LEN];
 char pub_name[NAME_LEN];
 int  source_groupid;
 int  target_groupid;
 int  rep_state;
 
 //以下结构不会存到表中，只存在内存中
 bool isValid;//此条目是否收到缓存效通知，是否需要重构缓存
 char source_node_name[NAME_LEN];
 int  source_node_port;
 char target_node_name[NAME_LEN];
 int  target_node_port;
 HTAB *PostedShardsHash;//当前复制进程正在处理的分片表(Hash结构，hash key为relationId（此分片表在订阅端上的oid），hash entry为MoveShardCacheEntry)
}MoveWorker;

//复制任务处理槽信息
typedef struct MoveScheduleCacheEntry
{
 int anchorRepId;
 int source_groupid;
 int dst_groupid;
 int rep_state;
 bool claimedExclusively;
 int WorkerNum;//当前复制槽上处理进程的个数
 MoveWorker *MoveWorkerArray[NUM_PER_REPLICATION];
}MoveScheduleCacheEntry;


//复制任务处理调度缓存
//hash key为rep_id，hash entry为MoveScheduleCacheEntry
HTAB *g_MoveScheduleCacheHash;
```

#### 4.4.3、缓存间的依赖关系

三种缓存间的相互依赖关系如下图所示：

![1566283748128](https://github.com/liupan126/flyingddb/blob/master/doc/citus%E5%88%86%E7%89%87%E8%BF%81%E7%A7%BB%E5%8A%9F%E8%83%BD%E8%AF%A6%E7%BB%86%E8%AE%BE%E8%AE%A1image/1566283748128.png?raw=true)

<center>图11、缓存之间的依赖关系<center\>

从上图可以看出，缓存结构可以按照模块分为两类：分片投递模块和分片表复制进程模块，两个模块相互独立，各司其职，简单而言：

- 分片投递模块，决定了某个迁移任务需要迁移哪些分片以及迁移到哪个节点，确定分片投递优先级，并且在恰当的时间内，把分片顺利的投递出去。
- 分片表复制进程模块，只负责处理分片表准确的从源端复制到目的端，根据任务和配置参数确定并行复制进程个数，同时定时更新每个分片表的迁移进度，以便尽快处理下一个等待的分片表。

## 五、分片迁移之阻塞写方案详细设计







## 六、分片迁移之逻辑复制方案详细设计